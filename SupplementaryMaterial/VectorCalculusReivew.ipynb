{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3642d15e",
   "metadata": {},
   "source": [
    "# Overview of vector calculus    \n",
    "\n",
    "## Introduction   \n",
    "\n",
    "An understanding of **vector calculus** is essential to working with common data science, machine learning, including AI algorithms. Functions of more than one variable are vector valued. Such functions are common in data science and machine learning. In simple terms, vector calculus is **calculus applied to vector valued functions**. .   \n",
    "\n",
    "Many data science and machine lerning problems require minimizing or maximizing a vector valued function. These foundational algorithms are based on the **gradient** of a function. The behavior of the gradient can be understood by looking at the **Hessian** that tells one about the curvature of the gradient. \n",
    "\n",
    "An optimum (maximum or minimum) of a vector is found when the graident is zero. At a maximum, the curvature is negative, and at a minimum. the curvature is positive.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab511542",
   "metadata": {},
   "source": [
    "## A Simple Example   \n",
    "\n",
    "To make these concepts more concrete through a simple 1-dimensional example. This 1-demensional example is based on a function of just a single variable. We will expand on these the concepts presented to multiple dimensions at the end of this notebook.   \n",
    "    \n",
    "For this example, we will with with a simple 1-dimensional function, a parabola:     \n",
    "\n",
    "$$y=x^2$$     \n",
    "\n",
    "The gradient of this function is simply computed as:      \n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{d\\ x^2}{d x} = 2x$$\n",
    "\n",
    "The gradient is $0$ at $x = 0$. To determine if this is a minimum of a maximum we must compute the second derivative of curvature of the function:     \n",
    "$$\\frac{d^2y}{dx^2} = 2$$\n",
    "\n",
    "The second derivative, or the curvature, is a constant. Since the curvature is postive $x = 0$ must be a minimum.   \n",
    "\n",
    "Execute the code in the cell below to plot the function, the gradient and the second derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8351c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-3.0, 3.25, 0.25)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot(x, x**2.0, label='Function');\n",
    "ax.plot(x,2*x, linestyle='dashed', label='Gradient');\n",
    "ax.plot(x, [2.0]*len(x), linestyle='dotted', label='Curvature');\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_title('Properties of the function $y = x^2$')\n",
    "ax.axvline(ymin=-10,ymax=25, c='Red');\n",
    "ax.axhline(xmin=-5,xmax=5, c='Red');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afe210",
   "metadata": {},
   "source": [
    "Examine the plot noting the following.   \n",
    "1. The minimum is at $x = 0$.    \n",
    "2. The gradient is is $0$ at $x = 0$, and becomes positive for $x > 0$ and negative for $x < 0$.    \n",
    "3. The curvature is constant and positive, indicatng there is only a minimum for this function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632dfb1",
   "metadata": {},
   "source": [
    "## Gradient Descent to MInimum\n",
    "\n",
    "**Gradient descent** algorithms are often used to find maximum and minimum of complex functions. The basic idea is to follow the gradient of a function until it approaches zero. Gradient descent take a series of small steps, determined by a **learning rate**, $\\alpha$, in the direction of the gradient at the current point. In mathematical terms we can write the update equation from value $x_{n-1}$ to value $x_n$ as:    \n",
    "\n",
    "$$x_n = x_{n-1} - \\alpha\\ grad(x_{n-1})$$\n",
    "\n",
    "Since gradient descent is an approximate numerical algorithm, it is highly unlikely that the algorithm will even actually find the point where the gradinet is exactly 0. Therefore we use a **stopping criteria** to determine when the approximation is good enough to stop. For example, we can run the algorithm until the gradient is less than some small number.     \n",
    "\n",
    "The `gradient_descent` function in the code cell below implements a simple 1-dimensional version of the algorithm. Notice the following about this code:   \n",
    "1. The `while` loop runs until the error tolerance or stopping condition is reached. Starting at an intial condition, $x_0$, the value of $x$ is updated using the relationship shown above.    \n",
    "2. The gradient for the new value of x is then computed by calling a Python lambda, or anonymous function. \n",
    "3. The values of the parameter $x$ and the gradient are then computed.    \n",
    "\n",
    "Execute the code in the cell below and examine the results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(x_values, grad_values):   \n",
    "    n_steps = len(x_values)\n",
    "    _, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "    ax = ax.flatten()\n",
    "    ax[0].plot(range(n_steps), x_values);\n",
    "    ax[0].axhline(xmin=0,xmax=n_steps, c='Red');\n",
    "    ax[0].set_xlabel('Number of iterations')\n",
    "    ax[0].set_ylabel('x')\n",
    "    ax[0].set_title('x vs. number of iterations')\n",
    "    ax[1].plot(range(n_steps), grad_values);\n",
    "    ax[1].axhline(xmin=0,xmax=n_steps, c='Red');\n",
    "    ax[1].set_xlabel('Number of iterations')\n",
    "    ax[1].set_ylabel('Gradient')\n",
    "    ax[1].set_title('Gradient vs. number of iterations')\n",
    "\n",
    "\n",
    "def gradient_descent(x0, alpha, tollerance = 0.01):\n",
    "    gradient = grad(x0) \n",
    "    x=x0\n",
    "    x_out = []\n",
    "    grad_out = []\n",
    "    while(abs(gradient) > tollerance):         \n",
    "        x = x - alpha * gradient\n",
    "        gradient = grad(x)\n",
    "        x_out.append(x)\n",
    "        grad_out.append(gradient)\n",
    "    return x_out, grad_out  \n",
    "\n",
    "grad = lambda x: 2*x    \n",
    "\n",
    "x0 = -3\n",
    "alpha = 0.001\n",
    "x_values, grad_values = gradient_descent(x0, alpha)\n",
    "\n",
    "plot_trajectory(x_values, grad_values)\n",
    "print('Final x = ' + str(x_values[len(x_values)-1]))\n",
    "print('Gradient at termination = ' + str(x_values[len(grad_values)-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb7f2d",
   "metadata": {},
   "source": [
    "Notice the following properties of the gradient descent algorithm evident in these plots:     \n",
    "1. The algorithm finds a good approximation with $x \\approx 0$.\n",
    "2. Since we have selected a relatively small learning rate was chosen, the algorithm converges slowly.     \n",
    "3. Gradient does indeed approach close to 0 at termination of the algorithm.      \n",
    "\n",
    "In the above experiment we started with an intial conditon of $x_0 = -3$. What if we use a positive initial condition like $x_0 = 3$. To find out, execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972648d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 3\n",
    "x_values, grad_values = gradient_descent(x0, alpha)\n",
    "\n",
    "plot_trajectory(x_values, grad_values)\n",
    "print('Final x = ' + str(x_values[len(x_values)-1]))\n",
    "print('Gradient at termination = ' + str(x_values[len(grad_values)-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5a618",
   "metadata": {},
   "source": [
    "These resutls are nearly idendentical to the first set. Only the sign of the values of $x$ and the gradient has changed. This should not be surprising, since the parabolic function is symmetric about the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116bbb63",
   "metadata": {},
   "source": [
    "## A Multi-dimensional Example      \n",
    "\n",
    "To continue our exploration of vector calculus with a 2-dimenstional example. For this example, we will work with the following function of a vector of 2 variables:     \n",
    "\n",
    "$$\n",
    "z( \\vec{v}) = z(x,y) = -3 x^2 -5 y^2\n",
    "$$\n",
    "\n",
    "We can find the gradient of this vector as follows, by taking all possible derivatives of $z(\\vec{v})$ with respect to the variables $x,y$:      \n",
    "\n",
    "$$\n",
    "grad \\big( z(\\vec{v}) \\big) =  \\nabla_{\\vec{v}} z(\\vec{v}) = \n",
    "\\begin{pmatrix}\n",
    "  \\frac{\\partial z(\\vec{v})}{\\partial x}\\\\\n",
    "   \\frac{\\partial z(\\vec{v})}{\\partial y} \n",
    " \\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "  -6 x\\\\\n",
    "  -10 y \n",
    " \\end{pmatrix} \n",
    " $$\n",
    " \n",
    " How can we understand this result? There are two dimensions that can be interpreted as:   \n",
    " 1. The value of $z$ decreases by 6 units (slope of -6) for every unit increase in the value of $x$.     \n",
    " 1. The value of $z$ decreases by 10 units (slope of -10) for every unit increase in the value of $y$.    \n",
    " \n",
    " The gradient of this function has only one zero at $[x,y] =[0,0]$. The question is if this is a maximum or a minimum and how strongly is the function curved at this point. To find out, we must compute the **Hessian** of the function. The Hessian matrix for this function can be computed by computing all the possible partiqal    \n",
    " \n",
    " $$\n",
    " Hes \\big( z(\\vec{v}) \\big) = \\nabla^2_{\\vec{v}} z(\\vec{v}) = \n",
    " \\begin{pmatrix}\n",
    "  \\frac{\\partial^2 z(\\vec{v})}{\\partial x^2},     \\frac{d z^2(\\vec{v})}{\\partial x \\partial y}\\\\\n",
    "  \\frac{\\partial^2 z(\\vec{v})}{\\partial x \\partial y}, \\frac{\\partial^2 z(\\vec{v})}{\\partial y^2} \n",
    " \\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "  -6,\\ \\ -6 x - 10 y\\\\\n",
    "  -6 x - 10 y,\\ \\ -10 \n",
    " \\end{pmatrix} \n",
    " $$    \n",
    " \n",
    " At $[x,y] =[0,0]$, the Hessian or curvature is:     \n",
    " \n",
    "$$\n",
    "\\nabla^2_{\\vec{v}} z(\\vec{v}) =  \n",
    "\\begin{pmatrix}\n",
    "  -6,\\ \\ 0\\\\\n",
    "  0,\\ \\ -10 \n",
    " \\end{pmatrix} \n",
    "$$  \n",
    " \n",
    " The diagonal elements of the Hessian are both negative, making $[x,y] =[0,0]$ a maximum.   \n",
    " \n",
    " We can easily verify that maximum of $z(\\vec{v})$ is well determined by computing the eigenvalues. Execute the code in the cell below to compute and display these eigenvalues.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hess = np.array([[-6,0],\n",
    "               [0,-10]])\n",
    "eigenvalues, eigenvectors = np.linalg.eig(Hess)\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86eda27",
   "metadata": {},
   "source": [
    "Notice two points here:    \n",
    "1. As expected the absolute values of the eigenvalues are quite large, indicating the maximum is well determined.   \n",
    "2. The eigenvalues are the values of the diagonal Hessian matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c999221",
   "metadata": {},
   "source": [
    "#### Copyright 2023, Stephen F Elston. All rights reserved.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
