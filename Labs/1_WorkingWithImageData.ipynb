{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c631483",
   "metadata": {},
   "source": [
    "# CSCI E-25      \n",
    "## Working with Image Data  \n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf2301-94d0-451c-a53b-478ca5b94685",
   "metadata": {},
   "source": [
    "## Introduction   \n",
    "\n",
    "This lesson will familiarize you with the basic concepts of working with image data and some statistical properties of images. Some key points of this lesson are:     \n",
    "1. Discrete pixel structure of digital images.    \n",
    "2. Representation of color and grayscale images.   \n",
    "3. Intensity distribution of image data.\n",
    "4. Equalizing intensity distributions and improving contrast. \n",
    "5. Resizing images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f191512",
   "metadata": {},
   "source": [
    "## Importing Packages\n",
    "\n",
    "As a first step, you will now import the required Python packages. \n",
    "\n",
    "> **Scikit-Image:** In this lesson, and for much of the rest of this course, we will be using the Scikit-Learn Image package. This package provides many commonly used computer vision algorithms using a consistent Python API. This package follows the API conventions of Scikit-Learn, enabling the application of a rich library of machine learning algorithms. There is excellent documentation available for the [Scikit-Learn Image package](https://scikit-image.org/docs/stable/index.html). You may wish to start by reading through the [User Guide](https://scikit-image.org/docs/stable/user_guide/index.html). Examples of what you can do with Scikit-Learn Image package can be seen in the [Gallery](https://scikit-image.org/docs/stable/auto_examples/index.html).    \n",
    ">\n",
    "> **Scikit-Image and Numpy:** Like all Scikit packages, Scikit-Image is built on [Numpy](https://numpy.org/). If you are not very familiar with Numpy there is an ReviewOfLinearAlgebra Jupyter notebook under the SuplementaryMaterial directory in the course GitHub repository, or you can find tutorials [here](https://numpy.org/numpy-tutorials/). A tutorial on using Numpy with Scikit-learn image data objects can be found [here](https://scikit-image.org/docs/stable/user_guide/numpy_images.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6eb630-1eca-464f-9d6f-a62f83bec557",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To run the code in this notebook you will need to have Scikit-Image installed in your envrionment. If you do not have Scikit-Image installed, you can uncomment and execute the code in the cell below. You can find [full instalation instructions here](https://scikit-image.org/docs/stable/user_guide/install.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68195305-5f02-4452-88e6-5b9756ef82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip insall scikit-image\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20959bc-63d7-4fd3-a731-01adcfc9b30f",
   "metadata": {},
   "source": [
    "To get started with this lesson, execute the code in the cell below to import the packages you will need. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b514ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import skimage \n",
    "import skimage.filters as skfilters\n",
    "\n",
    "from skimage import data\n",
    "from skimage import exposure\n",
    "from skimage.morphology import disk, square\n",
    "from skimage.color import rgb2gray, rgb2ycbcr, ycbcr2rgb, rgb2xyz, xyz2rgb, rgb2yuv, yuv2rgb, rgb2hsv, hsv2rgb\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import resize\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters.rank import equalize, threshold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "import cv2 as cv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a95e4-a68d-419d-9e88-425088a7fead",
   "metadata": {},
   "source": [
    "# Exercise 1-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a4d1f-ac21-4314-b6c0-6c05cc3c6bd5",
   "metadata": {},
   "source": [
    "## Structure of a Color Image Object\n",
    "\n",
    "The code in the cell below loads a color image of a human retina, prints the data types and dimensions of the image object, and displays the image. The image is displayed by [matplotlib.pyplot.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html). Execute the code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_image = data.retina()\n",
    "\n",
    "print('The image object is ' + str(type(retina_image)))\n",
    "print('The pixel values are of type ' + str(type(retina_image[0,0,0])))\n",
    "print('Shape of image object = ' + str(retina_image.shape))\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(6, 6))\n",
    "_=ax.imshow(retina_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3b5b71",
   "metadata": {},
   "source": [
    "The image object has 3-dimensions, the two spatial dimensions and the 3 color channels. Examine this image noticing the wide variation in color and intensity. Notice also that the illumination of the retina does not appear uniform, resulting in a bright spot on the left and a darker region on the right.  \n",
    "\n",
    "> **🚩 Exercise 1-1:** Complete the code for the function in the cell below to display the 3 color channels of the image and the original image in a 2x2 array using the [matplotlib.pyplot.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) function. The color channels are in red, green, blue order and should be displayed as gray scale using the `cmap=plt.get_cmap('gray')` argument. Your function should label the channels and the original image. Execute your function and examine the results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a1e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3_color_channels(img, titles=['Red','Green','Blue','Color'], plot_image=True):\n",
    "    '''Function plots the three color channels of the image along with the complete image'''\n",
    "    \n",
    "    fig, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    ## Complete the code below\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "plot_3_color_channels(retina_image)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f729fd3",
   "metadata": {},
   "source": [
    "> ❓Examine the intensity (brightness) of the color channels and answer these questions:    \n",
    "> 1. Which channel has the greatest intensity, and does this make sense given the image?      \n",
    "> 2. Is it likely that the saturation of the red channel arises as an artifact of the illumination spot on the left of the retina image?      \n",
    "> **End of exercise.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c53a4",
   "metadata": {},
   "source": [
    "> ✅ **Answers:**     \n",
    ">    1.      \n",
    ">    2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea7ec6",
   "metadata": {},
   "source": [
    "When working with digital images it is always important to keep in mind the discrete nature of the samples. To demonstrate the discrete nature of a digital image you can visualize a 100 pixel, or $10 \\times 10$, sample from the larger image by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3_color_channels(retina_image[600:610,300:310,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e915a",
   "metadata": {},
   "source": [
    "Notice the discrete nature in each of the three color channels and the color image. The sum of these discrete color-channel pixel intensities yields the color image.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50786c27-6006-4ff6-9482-045dd2608135",
   "metadata": {},
   "source": [
    "# Exercise 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ff6db",
   "metadata": {},
   "source": [
    "## Statistical Properties of an Image  \n",
    "\n",
    "The next question is, what is the distribution of pixel intensities in the 3 color channels of the image? Histograms and cumulative density functions are used to analyze these distributions. The code in the cell below plots the histograms of the 3 color channels along with their cumulative distributions. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_distributions(img, xlim=(0,255)):\n",
    "    '''Function plots histograms of the three color channels of the image along \n",
    "    with the cumulative distributions'''\n",
    "    \n",
    "    fig, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "    ax = ax.flatten()\n",
    "    titles=['Red','Green','Blue']\n",
    "    \n",
    "    for i in range(3):\n",
    "        \n",
    "        ax[i].hist(\n",
    "            img[:,:,i].flatten(),\n",
    "            bins=50,\n",
    "            density=True,\n",
    "            color=titles[i],\n",
    "            alpha=0.3\n",
    "        )\n",
    "        \n",
    "        ax[i].set_title(titles[i])\n",
    "        ax[i].set_xlabel('Pixel value')\n",
    "        ax[i].set_ylabel('Density')\n",
    "        ax[i].set_xlim(xlim)\n",
    "        \n",
    "        ax[3].hist(\n",
    "            img[:,:,i].flatten(),\n",
    "            bins=50,\n",
    "            density=True,\n",
    "            cumulative=True,\n",
    "            color=titles[i],\n",
    "            histtype='step',\n",
    "            label=titles[i]\n",
    "        )\n",
    "        \n",
    "        ax[3].set_xlim(xlim)\n",
    "        \n",
    "    ax[3].set_title('Cumulative distributions')  \n",
    "    ax[3].set_xlabel('Pixel value')\n",
    "    ax[3].set_ylabel('Cumulative density')  \n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "plot_image_distributions(retina_image)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcba2a3",
   "metadata": {},
   "source": [
    "There are several properties of the distribution of the pixel values which are important:     \n",
    "1. The distribution of the intensity for the red channel has clearly higher values than the other channels.    \n",
    "2. A significant fraction of pixel values have 0 intensity for all 3 color channels. These pixels are primarily the black background around the retina, but may also represent the dark pupil spot in the center of the retina.   \n",
    "3. A few red channel pixels have the maximum value of 255. The red intensity of these pixels is said to be **saturated**.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227cba26",
   "metadata": {},
   "source": [
    "## Improving Contrast     \n",
    "\n",
    "Our next question to address is what is the ideal distribution of the intensity values of an image? A useful, and obvious answer, is that we want the pixel values over the full range of possible values. For unsigned integer values, {0, 255}. Further, the distribution of pixel values should be uniform. For the $n=256$ unsigned integer values the **probability mass function**, or **PMF**, of the $ith$ value is:     \n",
    "\n",
    "$$p(i) = \\frac{1}{n}$$      \n",
    "\n",
    "And the **cumulative density function**, or **CDF**, of the uniform distribution at the $i$th value is, $x_i$:   \n",
    "\n",
    "$$CDF(i) = \\sum_{i=0}^{n-1} \\frac{1}{x_i}$$  \n",
    "\n",
    "We can visualize an example of a gray-scale image of unsigned integers on the range {0,255} with random uniformly distributed pixel values. The code in the cell below forms a gray-scale image randomly sampled uniform-distributed pixel values and displays the result.               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd129376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random image pixel values between 0 and 255\n",
    "random_values = nr.uniform(\n",
    "    low=0.0,\n",
    "    high=255.0,\n",
    "    size=retina_image.shape[0] * retina_image.shape[1]\n",
    ")\n",
    "\n",
    "# Reshape them to match the target 2D image dimensions\n",
    "random_image = (random_values * 255) \\\n",
    "    .reshape(retina_image.shape[0], retina_image.shape[1])\n",
    "\n",
    "# Optimize the data type give pixel value range from 0 to 255\n",
    "random_image = random_image.astype(np.uint8)\n",
    "\n",
    "print(random_image.shape)\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(6, 6))\n",
    "_=ax.imshow(random_image, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e514f5",
   "metadata": {},
   "source": [
    "To view the distribution of the pixel values of this image execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gray_scale_distribution(img):\n",
    "    '''Function plots histograms a gray scale image along \n",
    "    with the cumulative distribution'''\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(10, 4))\n",
    "    ax[0].hist(img.flatten(), bins=50, density=True, alpha=0.3)\n",
    "    ax[0].set_title('Histogram of image')\n",
    "    ax[0].set_xlabel('Pixel value')\n",
    "    ax[0].set_ylabel('Density')\n",
    "    ax[1].hist(img.flatten(), bins=50, density=True, cumulative=True, histtype='step')\n",
    "    ax[1].set_title('Cumulative distribution of image')  \n",
    "    ax[1].set_xlabel('Pixel value')\n",
    "    ax[1].set_ylabel('Cumulative density') \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_gray_scale_distribution(random_image)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99451c",
   "metadata": {},
   "source": [
    "> 🚩 **Exercise 1-2:** To compare the pixel value distribution of the retina image to the ideal values, do the following:   \n",
    "> 1. Create a gray-scale image object named `retina_gray_scale` using the [skimage.color.rgb2gray](https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_rgb_to_gray.html) function. \n",
    "> 2. Print the dimensions of the image object.  \n",
    "> 3. Display the gray-scale image. Make sure the image is large enough to see the details.   \n",
    "> 4. Plot the distribution of the pixel values of the gray-scale image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grayscale(img):\n",
    "    fig, ax = plt.subplots( figsize=(5, 5))\n",
    "    _=ax.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "\n",
    "## Put you code below    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b4b8a5",
   "metadata": {},
   "source": [
    "> ❓ Examine the distribution plots and answer these questions:  \n",
    "> 1. How would you describe these results with respect to the ideal distribution?    \n",
    "> 2. How do you think the range of pixel values limit the contrast of the image?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606daaa5",
   "metadata": {},
   "source": [
    "> ✅ **Answer:** \n",
    "> 1.     \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da8508-5469-44e5-a934-56932b1fdc84",
   "metadata": {},
   "source": [
    "# Exercise 1-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4171bb",
   "metadata": {},
   "source": [
    "## Histogram Equalization\n",
    "\n",
    "**Contrast** of an image is the range between the minimum and maximum pixel values of an image. The larger the range of values the more distinctive the differences in the image will be. To improve the contrast in an image we need to **equalize** the pixel values over the maximum of the range. The goal is to find a transformation that stretches the pixel values into a uniform distribution. This process is known as **histogram equalization**.    \n",
    "\n",
    "Histogram equalization can be performed in a number of ways. The obvious algorithm is global histogram equalization. The pixel values are transformed to equalize the histogram across the entire image. However, if illumination is inconsistent across the image, global equalization will not be optimal. An alternative is to perform local histogram equalization over small areas of the image. This method is known as **adaptive histogram equalization**. Adaptive equalization can compensate for uneven illumination across the image.           \n",
    "\n",
    "> 🚩 **Exercise 1-3:** You will now apply both common types of histogram equalization to the gray-scale retina image. The code in the cell below uses both the [skimage.exposure.equalize_hist](https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html) function and the [skimage.exposure.equalize_adapthist](https://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.equalize_adapthist) function. Additionally, the code tests different clip limits for the adaptive histogram equalization, $[0.01,0.02,0.1]$, and with a patch size of $8 \\times 8$. A clip limit of $0.01$ is the default value. The code in the cell does the following for each function and arguments:    \n",
    "> 1. Executes the two different equalization functions with corresponding parameters passed via `kwargs` to `test_equalize(...)` method. \n",
    "> 2. Displays the equalized gray-scale image using the `plot_grayscale()` function.   \n",
    "> 3. Plots the distribution of the pixel values of the equalized gray-scale image using the `plot_gray_scale_distribution()` function.    \n",
    "> \n",
    "> Execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b424284-233f-40d5-82d5-2ed845b40b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_equalize(img, func, **kwargs): \n",
    "    img_equalized = func(img, **kwargs)\n",
    "    plot_grayscale(img_equalized)\n",
    "    plot_gray_scale_distribution(img_equalized)    \n",
    "    return img_equalized\n",
    "\n",
    "equalize_list = [\n",
    "    exposure.equalize_hist,\n",
    "    exposure.equalize_adapthist,\n",
    "    exposure.equalize_adapthist,\n",
    "    exposure.equalize_adapthist\n",
    "]\n",
    "\n",
    "args_list = [\n",
    "    None,\n",
    "    {'clip_limit': 0.01, 'kernel_size': 12},\n",
    "    {'clip_limit': 0.03, 'kernel_size': 12},\n",
    "    {'clip_limit': 0.1, 'kernel_size': 12},\n",
    "]\n",
    "\n",
    "for func, kwargs in zip(equalize_list, args_list):\n",
    "    print('\\nFor function ' + func.__name__ + ' ' + (json.dumps(kwargs) if kwargs else ''))\n",
    "    \n",
    "    retina_gray_scale_equalized = \\\n",
    "        test_equalize(retina_gray_scale, func, **kwargs) if kwargs \\\n",
    "        else test_equalize(retina_gray_scale, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7d4aa",
   "metadata": {},
   "source": [
    "> ❓ Answer the following questions:  \n",
    "> 1. Compare the unequalized and equalized imagess. What aspects of the of the images are more apparent with the improved contrasted.  \n",
    "> 2. Compare the distributions of pixel values between the unequalized image, the globally histogram equalized image, and adaptively epequalized images (3). What do the differences in pixel distributions tell you about the effect of the equalization algorithms? You can use the distribution of the uniformly distributed random image as a benchmark.\n",
    "> 3. Compare the globally equalized histogram image to the locally equalized images. What is the key difference, and how can you explain this difference given the pixel distributions? Which equalization method provides a better image for extracting features for CV models and why?  \n",
    "> 4. The images created by the three clipping levels are not particularly different. What consistent change can you notice and why does this make senese given the algorithm?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f273b57",
   "metadata": {},
   "source": [
    "> ✅ **Answers:**   \n",
    "> 1.           \n",
    "> 2.          \n",
    "> 3.             \n",
    "> 4.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2ffdf-c121-4e31-bc26-27d6b3f56656",
   "metadata": {},
   "source": [
    "# Exercise 1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6db19",
   "metadata": {},
   "source": [
    "### Equalization for Multi-Channel Images   \n",
    "\n",
    "Contrast improvement, including histogram equalization, cannot be directly applied to the individual color channels of an RGB image. For RGB images the intensity of each color channel is mathematically unconstrained by the other two. However in reality, the brightness or intensity of each pixel depends on the value of all three channels. Therefore, independently applying 2-dimensional equalization to an RGB image causes normalization problems.  \n",
    "\n",
    "A common approach is to transform an RGB image into one of several possible formats that use a 2-dimensional color space map or **chromaticity** map of image intensity. There are a great many such choices, a number of which are supported in the [skimage.color](https://scikit-image.org/docs/stable/api/skimage.color.html) package.  \n",
    "\n",
    "As an example, the [Hue Saturation and Value (HSV)](https://en.wikipedia.org/wiki/HSL_and_HSV) is a cylindrical coordinate map of hue as the angle, staturation as the radius and value as the vertical. The **HSV** color space is shown in the figure below.   \n",
    "\n",
    "\n",
    "<img src=\"../img/hsv_space.png\" alt=\"Drawing\" style=\"width:400px; height:400px\"/>\n",
    "HSV Color Space in Cylindrical Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9110055",
   "metadata": {},
   "source": [
    "> 🚩 **Exercise 1-4:** To apply the [sklearn.exposure.equalize_adapthist](https://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.equalize_adapthist) function to a color image the following steps are used:\n",
    "> 1. The [skimage.color.rgb2hsv](https://scikit-image.org/docs/stable/api/skimage.color.html#skimage.color.rgb2hsv) function is used to convert the *RGB* image to *HSV* format.     \n",
    "> 2. The hue, saturation and value channels of the transformed image are displayed. Make sure to use the correct labels when you call the `plot_3_color_channels` function and set `plot_image=False`.\n",
    "> 3. Equalize only the value channel (last dimension) of the HSV image, setting 'clip_limit=0.05'.    \n",
    "> 4. The equalized image is converted to *RGB* using the [skimage.color.hsv2rgb](https://scikit-image.org/docs/stable/api/skimage.color.html#skimage.color.hsv2rgb) function.\n",
    "> 5. Plot the color channels and the composite image of the equalized image.   \n",
    ">  \n",
    "> Execute the code and examine the results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42706169",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222fdc6",
   "metadata": {},
   "source": [
    "> ❓ Notice that the hue component of the HSV repersentation of the image has saturated pixel values (not to be confused with color staturation of the HSV representation) and nearly constant. There is the expected variation in the saturation and value components of the HSV representation.\n",
    "> \n",
    "> Compare the equalized RGB images and pixel value densities to the images and densities of the original image and answer the answer the following questions:    \n",
    "> 1. Did the histogram equalization achieve the goal of improving the contrast of the image both in the color channels and for the 3-channel color image, and why?   \n",
    "> 2. Given the use of the locally adapted histogram equalization algorithm, does the distribution of the pixel values in the 3 channels of the equalized image make sense, and why?    \n",
    "> 3. What is the evidence of saturation of the red color channel after equalization?    \n",
    "> 4. Does the change in color of the 3-channel color image make sense given the histogram equalization, and why?    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241c515",
   "metadata": {},
   "source": [
    "> ✅ **Answers:**\n",
    "> 1.            \n",
    "> 2.         \n",
    "> 3.            \n",
    "> 4.                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecec7e",
   "metadata": {},
   "source": [
    "### Rank equalization\n",
    "\n",
    "Contrast improvement is such an important data preparation step for computer vision that many algorithms have been proposed. One approach is to use rank statistics over a small region or local region of the image. The [skimage.filters.rank.equalize](https://scikit-image.org/docs/dev/api/skimage.filters.rank.html#skimage.filters.rank.equalize) function implements just such an algorithm. Execute the code in the cell below to see the effect this algorithm has on the gray-scale retina image. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_rank_equalized = equalize(np.multiply(retina_gray_scale, 255).astype(np.uint8), footprint=disk(9))\n",
    "plot_grayscale(retina_rank_equalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209fedcf",
   "metadata": {},
   "source": [
    "This locally equalized image shows considerably more detail than the global histogram equalization or adaptive histogram equalization methods. But, is this what we really want? In some cases yes. If fine details like texture are important to the computer vision solution, this equalization would be preferred. However, too much detail might lead to unnecessary complexity if the goal was to identify major structural elements of the image. In summary, the correct preprocessing for an image depends on the other algorithms one intends to apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96267473-0f4c-4d88-be47-13e86e1ad3d2",
   "metadata": {},
   "source": [
    "# Exercise 1-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e1d06",
   "metadata": {},
   "source": [
    "## Other Contrast Adjustments   \n",
    "\n",
    "Besides histogram equalization, numerous mathematical transformations for improving contrast have been developed. These methods seek to improve contrast by a nonlinear transformation of the pie values. We will examine just of few of the many possibilities:     \n",
    "\n",
    "- **Gamma adjustment** is a power law transformation that shifts the histogram of the pixel values. For input pixel values $x_i$, and power, $\\gamma$, the output pixel values are computed $x'_i = gain * x_i^{\\gamma}$, were gain is an optional scale adjustment. If $\\gamma < 1$ the histogram shifts to the right and for $\\gamma > 1$ the histogram shifts to the left.     \n",
    "- **Logarithmic adjustment** computes a logarithmic compression of the pixel values, $x_i$, $x'_i = gain * log(x_i + 1)$, where gain is an optional scale adjustment.     \n",
    "- **Sigmodal adjustment** is a nonlinear transformation of the pixel values, $x_i$, with a cutoff value, $x'_i = \\frac{1}{1 + exp(gain * (cutoff - x_i))}$, and an optional gain adjustment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e727f33",
   "metadata": {},
   "source": [
    "> 🚩 **Exercise 1-5:** To get a feel for the gamma adjustment method you will now do the following:  \n",
    "> 1. Iterate over gamma values of $[0.5, 2.0]$.     \n",
    "> 2. Apply the gamma adjustment [skimage.exposure.adjust_gamma](https://scikit-image.org/docs/stable/api/skimage.exposure.html#skimage.exposure.adjust_gamma) function to the gray scale retina image.     \n",
    "> 3. Display the adjusted image and the pixel value density. Make sure you include a printed indication of gamma for each case.      \n",
    "> 4. Execute your code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bde22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Place your code below    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13851b24",
   "metadata": {},
   "source": [
    "> ❓ Examine your results for the values of gamma, comparing them to the original gray-scale image. How does the brightness of the image and the distribution of pixel values change with gamma?    \n",
    "> **End of exercise.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f7e89",
   "metadata": {},
   "source": [
    "> ✅ **Answer:**               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75ad9a-7d56-4e25-9a01-b0dd5a7eccec",
   "metadata": {},
   "source": [
    "# Exercise 1-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf2c33",
   "metadata": {},
   "source": [
    "> 🚩 **Exercise 1-6:** To get a feel for the sigmoidal adjustment method you will now do the following:  \n",
    "> 1. Iterate over cutoff values of $[0.3, 0.4, 0.5]$.     \n",
    "> 2. Apply the sigmoidal adjustment [skimage.exposure.adjust_sigmoid](https://scikit-image.org/docs/stable/api/skimage.exposure.html#skimage.exposure.adjust_sigmoid) function to the gray scale retina image.     \n",
    "> 3. Display the adjusted image and the pixel value density. Make sure you print cutoff indication for each case.\n",
    "> 4. Execute your code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05292d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec50ad",
   "metadata": {},
   "source": [
    "> ❓ Examine the images and the pixel value densities for the resulting images and compare these to the original gray-scale image. How does the brightness and densities change with the cutoff value? Pay attention to expansion or compression of the range of pixel values.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a9dfa",
   "metadata": {},
   "source": [
    "> ✅ **Answer:**            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5cf257-5db9-40a7-ab33-f81bc0e887e9",
   "metadata": {},
   "source": [
    "# Exercise 1-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383688df",
   "metadata": {},
   "source": [
    "## Binary Images  \n",
    "\n",
    "Many computer vision algorithms operate on binary images. Primarily these methods are in the category of **morphology**, which we will explore later. A binary image has only two values, $\\{positive, negative \\}$ or $\\{ 1, 0 \\}$.   \n",
    "\n",
    "> ❗️ **Exercise 1-7:** You will complete a function named `transform2binary()` to convert either a 3-channel color image or gray-scale image to an integer binary image, $\\{ 1, 0 \\}$, given a threshold value in the range $0 \\le threshold \\le 1$ as an argument. The function must do the following:    \n",
    "> 1. If the image is multi-channel, convert it to gray-scale.     \n",
    "> 2. Transform the threshold value to the fraction of the range of the pixel values. Print the transformed threshold value.    \n",
    "> 3. Apply the threshold to the gray-scale pixel values and return the binary images.     \n",
    "> 4. Execute your function on the **locally equalized color** retina image, print the dimensions of the binary image, and display the image, using a threshold value of 0.37.  \n",
    "> 5. Execute your function on the **locally equalized gray scale** retina image, print the dimensions of the binary image, and display the image, using a threshold value of 0.37.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658367b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below\n",
    "def transform2binary(img, threshold=0.5):\n",
    "    '''\n",
    "    Function converts a gray scale or color image to binary values. \n",
    "        Args: \n",
    "            img - a color or gray scale image file\n",
    "            threshold = the threshold value on a 0-1 scale. Pixel values >= threshold are set to 1, else 0\n",
    "        Returns:\n",
    "            Binary 2d image as a numpy array\n",
    "    '''\n",
    "    \n",
    "    ## Make sure to use a copy to prevent weird bugs that \n",
    "    ## that are nearly impossible to track down  \n",
    "    img = np.copy(img)\n",
    "    \n",
    "    ## Complete the code below  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for img in [retina_rgb, retina_gray_scale_equalized]: \n",
    "    retina_binary = transform2binary(img, threshold=0.37)\n",
    "    print(retina_binary.shape)\n",
    "    plot_grayscale(retina_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff935d",
   "metadata": {},
   "source": [
    "> ❓ Examine the image and answer the following questions.  \n",
    "> 1. Does the binary image created from the equalized color image capture key aspects of the retina and why?     \n",
    "> 2. Compare the binary images created from the equalized color image and the equalized gray-scale image. Is there any difference, and is this the result you would expect?  \n",
    "> **End of exercise.**      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d2079",
   "metadata": {},
   "source": [
    "> ✅ **Answers:**\n",
    "> 1.           \n",
    "> 2.                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d09c0d-f0fb-4bdc-be3e-27df67724650",
   "metadata": {},
   "source": [
    "# Exercise 1-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95242a55",
   "metadata": {},
   "source": [
    "In the foregoing exercise, the threshold for the decision classifying pixel values as true or false, $\\{ 0, 1 \\}$ was set manually by trial and error. There are numerous algorithms which have been devised for finding thresholds. In general, these algorithms attempt to find an optimal threshold using various measures. Ideally, these algorithms search for a low-frequency point in the pixel value histograms which can be used to divide the values.     \n",
    "\n",
    "🚩️ **Exercise 1-8:** We can create a binary image using one of the many established algorithms to compute a threshold. In this case [Otsu's threshold algorithm](https://scikit-image.org/docs/dev/api/skimage.filters.html?highlight=threshold_otsu#skimage.filters.threshold_otsu). Use this function to find a threshold, apply the threshold to the equalized gray-scale image to compute a binary image and plot the result.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c512ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b426bc",
   "metadata": {},
   "source": [
    "> ❓ How does this binary image compare the to ones computed with the threshold found by trail-and-error, and why?       \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45788ad7",
   "metadata": {},
   "source": [
    "> ✅ **Answer:**         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8635edb-1615-41d9-bf7f-294a3c779e36",
   "metadata": {},
   "source": [
    "# Exercise 1-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff67b1",
   "metadata": {},
   "source": [
    "## Inversion of Images  \n",
    "\n",
    "For some machine vision algorithms, it is easier or more effective to work with the **inverse image** or **negative** of the image. The concept is simple. Pixel values are generally restricted to a range like $\\{ 0 - 255 \\}$ for unsigned integer representation or $\\{ 0.0 - 1.0 \\}$ for a floating point image. The given an intensity $P_{i,j}$ of the $ij$th pixel, the inverted intensity, $I_{i,j}$, is then:\n",
    "\n",
    "$$I_{i,j} = max\\big[ P \\big] - P_{i,j}$$\n",
    "\n",
    "Where, $max\\big[ P \\big]$ is the largest value the representation of the image allows, typically 255 or 1.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc053b9b",
   "metadata": {},
   "source": [
    "> 🚩 **Exercise 1-9:** You will now write a function named `invert_image` that will perform image inversion on both 3-channel and gray-scale images. Make sure you find the correct maximum value for the data type of the image, 255 for `uint8` or 1.0 float.     \n",
    "> \n",
    "> Now, apply your function to the original color retina image and display the image along with the density plot.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e40b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_image(img):\n",
    "    '''\n",
    "    Function to invert an image, or create the negative.  \n",
    "        Args:\n",
    "            img - a 2d gray scale or color image\n",
    "        Returns: \n",
    "            Inverse of the image\n",
    "    '''\n",
    "    ## Make sure to use a copy to prevent weird bugs that \n",
    "    ## that are nearly impossible to track down  \n",
    "    img = np.copy(img)\n",
    "    \n",
    "    ## Put your code below  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "inverted_retina = invert_image(retina_image)\n",
    "print(inverted_retina.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "_=ax.imshow(inverted_retina)\n",
    "plot_image_distributions(inverted_retina)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fa50a",
   "metadata": {},
   "source": [
    "> Next, apply your function to the adaptive histogram equalized gray-scale retina image and display the image along with the distribution plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_retina_grayscale = invert_image(retina_gray_scale_equalized)\n",
    "print(inverted_retina_grayscale.shape)\n",
    "plot_grayscale(inverted_retina_grayscale)\n",
    "plot_gray_scale_distribution(inverted_retina_grayscale) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39582879",
   "metadata": {},
   "source": [
    "> ❓ Answer the following questions:   \n",
    "> 1. Compare the distribution of the pixel values for the three color channels of the inverted image with the distributions for the original image. Do the distributions of the inverted image make sense given the original values and why? \n",
    "> 2. Do you think the color of the 3-channel inverted image is correct and why?   \n",
    "> 3. The difference in pixel value distributions between the inverted gray scale and original adaptive histogram distributions is subtle. What key difference can you identify?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86d3a2",
   "metadata": {},
   "source": [
    "> ✅ **Answers:**\n",
    "> 1.              \n",
    "> 2.           \n",
    "> 3.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f202b8c-059a-4ee3-bcfb-bf4aa8a833bf",
   "metadata": {},
   "source": [
    "# Exercise 1-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969bed5",
   "metadata": {},
   "source": [
    "## Sampling and resizing images      \n",
    "\n",
    "For many computer vision processes the dimensions of an image must be transformed. We have already explored removing the multi-channel color dimension from an image to form the gray-scale image. Now, we will investigate transforming the pixel row and column dimensions of an image. There are two options:       \n",
    "1. **Downsample:** A downsampled image has a reduced number of pixels. If the multiple between the pixel count of the original image and the downsampled image is an even number, sampled pixel values are used. Otherwise, interpolation is required for arbitrary multiples. Inevitably, down-sampling will reduce the resolution of the image, and fine details will be lost.\n",
    "2. **Upsample:** The number of samples can be increased by interpolation between the pixel values. The interpolated values fill in the values of the new pixel. If the pixel count of the upsampled image is not an even multiple of the original image most of the values will be interpolated. While up-sampling can increase the number of the pixels, this process cannot increase the resolution of an image.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a0175",
   "metadata": {},
   "source": [
    "> 🚩 **Exercise 1-10:** You will now resize the adaptive histogram equalized gray-scale image. Using the [skimage.transform.resize](https://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize) function, do the following:      \n",
    "> 1. downsample the image to dimension $(64,64)$. Print the dimensions and display the resulting image.      \n",
    "> 2. Upsample the downsampled image to dimension $(1024,1024)$. Print the dimensions and display the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downsample the image into retina_decimated_64 variable\n",
    "## Put you code below\n",
    "\n",
    "\n",
    "\n",
    "print(retina_decimated_64.shape)\n",
    "plot_grayscale(retina_decimated_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upsample the image into retina_up_sample variable\n",
    "## Put you code below\n",
    "\n",
    "\n",
    "\n",
    "print(retina_up_sample.shape)\n",
    "plot_grayscale(retina_up_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc5892",
   "metadata": {},
   "source": [
    "> ❓ Notice the changes in resolution of the downsampled and upsampled images.  \n",
    "> 1. How is the reduction in resolution of the $(64,64)$ image exhibited? \n",
    "> 2. Does up-sampling to $(1024,1024)$ restore the resolution of the image or simply blur the 'pixelation' visible in the $(64,64)$ image? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b43a6",
   "metadata": {},
   "source": [
    "> ✅ **Answers:**\n",
    "> 1.              \n",
    "> 2.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9758a2-d87e-4844-8a3e-60f3de8a4d7c",
   "metadata": {},
   "source": [
    "# Exercise 1-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987713f-eb2d-47f9-9a76-f9f2441227b3",
   "metadata": {},
   "source": [
    "## Sampling and Aliasing in Images     \n",
    "\n",
    "As should be clear from the foregoing, the digital images we work with for computer vision are discretely sampled in the 2-dimensional plane. The discrete pixel values $v_{\\mathbf{x}}$ are the result of this sampling. This discrete sampling limits the **spatial resolution** which can be captured in the image. If the samples are spaced too far apart, [aliasing](https://en.wikipedia.org/wiki/Aliasing) will occur. For sinusoidal components of the image, according to the [Nyquist–Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) the sampling frequency must be at least twice the frequency of this component. The sampling rate of 2 times the highest frequency component is known as the **Nyquist Frequency**. Sampling below the Nyquist frequency leads to aliasing. The Nyquist–Shannon limit is easy to express mathematically.\n",
    "\n",
    "$$B \\lt \\frac{f_s}{2}$$\n",
    "Where, $B$ is the **band limit** or highest frequency sampled, and $f_s$ is the sampling frequency or rate.   \n",
    "\n",
    "We can illustrate this concept with an example. The code in the cell below plots a sine function with Nyquist rate sample points, shown as orange dots. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7582cd2-daa1-4fd9-b45a-a2f92c0f64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the sine funciton\n",
    "samples_per_cycle = 16\n",
    "cycles = 6\n",
    "x = [x for x in range(cycles*samples_per_cycle)]\n",
    "signal_1d = [math.sin(w*math.pi/samples_per_cycle) for w in x]   \n",
    "\n",
    "## Plot the sine function \n",
    "_ , ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(x, signal_1d);\n",
    "\n",
    "## Sample points at the Nyquist rate   \n",
    "sample_rate = 16\n",
    "ax.scatter(x[0::sample_rate], signal_1d[0::sample_rate], c='red', s=200, alpha=0.6, label='Nyquist rate');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade214ed-4b20-4b9e-9530-c3d5fd08f9f9",
   "metadata": {},
   "source": [
    "Notice that all the Nyquest rate sample points are at the zero-crossing of the sine function. It is clear that this sample is aliased, since the sine funcition cannot be recopnstructed from these samples.     \n",
    "\n",
    "What is the result of increasng the sample rate above the Nyquist rate? To find out, execute the code in the cell below which uses a sampling rate just above the Nyquist rate.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4f618-d522-4c2d-b889-17d0e1747f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the sine function and ample points at the Nyquist rate   \n",
    "\n",
    "_, ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(x, signal_1d);\n",
    "\n",
    "ax.scatter(\n",
    "    x[0::sample_rate],\n",
    "    signal_1d[0::sample_rate],\n",
    "    c='red',\n",
    "    s=200,\n",
    "    alpha=0.6,\n",
    "    label='Nyquist rate'\n",
    ");\n",
    "\n",
    "## Plot sample points just greater than the Nyquist rate   \n",
    "\n",
    "high_sample_rate = 15\n",
    "\n",
    "ax.scatter(\n",
    "    x[0::high_sample_rate],\n",
    "    signal_1d[0::high_sample_rate],\n",
    "    c='orange',\n",
    "    s=200,\n",
    "    label='Higher sample rate'\n",
    ");\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c285789-f908-4191-b6d7-0cdf3959ec8a",
   "metadata": {},
   "source": [
    "Notice that the higher sampling rate appears to be sufficient to represent the sine function signal. A sine function can be fitted to these sample points, perfectly reconstructing the original signal.\n",
    "\n",
    "What happens if we reduce the sample rate by a factor of 2? To explore this, the code below expands on the previous example by performing the following steps:\n",
    "1. Plot the original sine function and sample points with a frequency just above the Nyquist rate.\n",
    "2. Compute a sine function with frequency 1/2 the first sine function.    \n",
    "3. Plot the low frequency sine function and the low rate sample points along with green Xs. The sample rate is one half the higher sample rate. \n",
    "4. Plot a highlight of the low-frequency sampling on the high frequency sine function. These points will show \n",
    "\n",
    "Execute the code and examine the results.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b1012-1a9e-43fa-bac0-0b64ee106b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLot the original sine function and sample points  \n",
    "_ , ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(x, signal_1d);\n",
    "ax.scatter(x[0::high_sample_rate], signal_1d[0::high_sample_rate], c='orange', s=100, label='high rate on high frewuency');\n",
    "\n",
    "## Compute the downsampled rate and low-frequency sine function\n",
    "up_sample_rate = int(2*high_sample_rate)\n",
    "downsample_rate = 2.0 * samples_per_cycle\n",
    "signal_downsampled = [math.sin(w*math.pi/downsample_rate) for w in x] \n",
    "\n",
    "## Plot the low-frequency sine function and sampling points  \n",
    "ax.scatter(x[0::up_sample_rate], signal_downsampled[0::up_sample_rate], c='green', marker='x', s=100, label='Low rate on low frewuency');\n",
    "ax.plot(x, signal_downsampled, c='green', linestyle='dashed', linewidth=1.0);\n",
    "\n",
    "## PLot the low sample rate at the high-frequency sine function  \n",
    "ax.scatter(x[0::up_sample_rate], signal_1d[0::up_sample_rate], c='gray', marker='o', s=300, alpha=0.3, label='Low rate on high frewuency');\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1e149-a4e9-4fcb-9914-f6a13de8e6de",
   "metadata": {},
   "source": [
    "Observe that there are only 5 low frequency sample points required for the low-frequency sine function. In addition notice that each of these low rate sample points are insufficient to represent the higher frequency sine function, meaning that the low sample rate is aliased by the high frequency sine function.     \n",
    "\n",
    "What sample rate is required for a signal that is a composite of the high frequency sine function and the low-frequency sine function? By the Shannon-Nyquist theorem the sample rate required is 2 times the highest frequency component of the composite signal. To demonstrate this principle the code in the cell below plots the composite signal along with the sample points. Execute this code and examine the results.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc933ee-6438-4296-99a0-ac9cc82248c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(signal_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5428e-c751-4ae8-b45b-40ae04767600",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_signal = np.add(signal_downsampled, signal_1d)\n",
    "\n",
    "_ , ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(x, composite_signal);\n",
    "ax.scatter(x[0::high_sample_rate], composite_signal[0::high_sample_rate], c='orange', s=100, label='Sample point');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d584d96-e8f3-4ffd-a169-d1b2f930a922",
   "metadata": {},
   "source": [
    "Observe that the sample points appear sufficient to reproduce the composite signal since there are more than 2 samples per high frequency cycle. \n",
    "\n",
    "We can demonstrate the concept of aliasing for a 2-D image with a simple example. The example is based on an initial image and three downsampled versions:    \n",
    "1. The initial image has diagonal slashes with sinusoidal amplitudes and dimension $(256,256)$.  \n",
    "2. The image is downsampled to dimension $(256, 256)$. \n",
    "3. The image is downsampled to dimension $(128, 128)$. \n",
    "4. The image is downsampled to dimension $(64, 64)$.\n",
    "   \n",
    "Execute the code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80339c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "x = np.arange(0, dim*dim, 1)\n",
    "sin_image = 1.0 - 0.5 * np.sin(np.divide(x, math.pi)).reshape((dim,dim))\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(12, 12))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i,dim in enumerate([256,128,64,32]):\n",
    "    \n",
    "    sampled_image = resize(sin_image, (dim,dim))\n",
    "    _=ax[i].imshow(sampled_image, cmap=plt.get_cmap('gray'))\n",
    "    _=ax[i].set_title('Dimension = (' + str(dim) + ',' + str(dim) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b4a74",
   "metadata": {},
   "source": [
    "Examine the images above and notice the following:  \n",
    "1. The $(128, 128)$ downsampled image retains the characteristics of the initial image. Look carefully, you can see a slight blurring.      \n",
    "2. The $(64, 64)$ downsampled image retains the sinusoidal slash structure. Coarse pixelation is now quite evident, and the sampling is very close to the Nyquist frequency.\n",
    "3. The $(32, 32)$ downsampled image does not resemble the initial image at all, exhibiting significant aliasing. Scan with your eyes to side and up and down across the image. You may see patterns that are not representative of the original image. Such false patterns are common artifacts arising from aliasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89772dbb",
   "metadata": {},
   "source": [
    "How can aliasing be prevented? A filter can remove the high-frequency components of the image which would lead to the aliasing. A common approach is to use a Gaussian filter. This filter removes high frequency components and has the effect of blurring the image.          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50a619",
   "metadata": {},
   "source": [
    "> 🚩 **Exercise 1-11:** You will now investigate how filtering can be applied to prevent aliasing. The [skimage.transform.resize](https://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize) function applies a Gaussian filter to prevent aliasing by default. The standard deviation of the Gaussian filter, or filter span, can be set to adjust the bandwidth of the filter.    \n",
    "> In this exercise you will resample the adaptive histogram equalized gray-scale retina image using the skimage.transform.resize function with the `anti_aliasing=False` argument. You will use the [skimage.filters.gaussian](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.gaussian) to limit the bandwidth of the image. Do the following to compare the results of different filter bandwidths:   \n",
    "> 1. Compute a scale factor, $sf = \\sqrt{\\frac{original\\ dimension}{reduced\\ dimension}}$.\n",
    "> 2. Apply the Gaussian filter with $sigma = multiplier * sf$ for multiplier in $[0,1,2,3,4]$ (`range(5)`) and resize the image to $(64,64)$ pixels.\n",
    "> 3. For each value of sigma display the image with a title indicating the value of sigma. You may find interpretation easier if you plot the images on a $3 \\times 2$ grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc6918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb97eea",
   "metadata": {},
   "source": [
    "> ❓ Answer the following questions.   \n",
    "> 1. How does the aliasing change with increasing sigma, decreasing bandwidth? Is this the behavior you expect and why?  \n",
    "> 2. How does the blurring of the image change with increasing sigma, decreasing bandwidth? Is this the behavior you expect and why?\n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded48df",
   "metadata": {},
   "source": [
    "> ✅ **Answers:**   \n",
    "> 1.                 \n",
    "> 2.                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31cac1f",
   "metadata": {},
   "source": [
    "#### Copyright 2021, 2022, 2023, 2024, 2025, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3849ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
