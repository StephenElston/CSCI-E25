{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CSCI E-25\n",
    "## Introduction to Convolutional Neural Networks\n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Introduction to convolutional neural networks\n",
    "\n",
    "This lesson introduces you to a powerful neural network architecture, known as **convolutional neural networks**. Convolutional neural networks operate by **learning a set of filters**  or **convolution kernels**. Using a process, known as **convolution**, these filters extract a **feature map** from the raw data, an **embedding**. The feature map is a **lower dimensional** map of the raw input features. This map is learned in a supervised machine learning process employing back propagation. You can think of convolutional neural networks as a powerful and flexible **dimensionality reduction** method. \n",
    "\n",
    "Convolutional neural networks are used to build feature maps from any type of data with coherency in one or more dimensions. This includes time series data, and text data as one dimensional cases, and image data as a two dimensional case. \n",
    "\n",
    "Convolutional neural networks have a reasonably long history. The first known commercial application was for automated processing of check images by LeCun et. al. (1998). For unclear reasons, convolutional neural networks were relegated to specialized applications such as hand writing recognition until Krizhevsky et. al. (2012) used them to decisively win an ImageNet object recognition competition. Curiously, other teams had previously used convolutional neural networks to win competitions, but somehow this was not widely recognized. \n",
    "\n",
    "In this lesson you will learn:\n",
    "1. The basics of convolutional neural network architecture.\n",
    "2. How convolutional neural networks learn the filters to create the feature map. \n",
    "3. Feature extraction for creating the feature map. \n",
    "4. Visualization of the feature map and the filters.\n",
    "5. Commonly used convolutional operator design. \n",
    "6. How to use the feature map in supervised learning. \n",
    "7. Application of various regularization methods for improving the behavior of learning. \n",
    "\n",
    "****\n",
    "**Note:** The formulation of convolution filters used in deep learning is somewhat idiosyncratic. If you are familiar with the standard approach as applied in say electrical engineering or control engineering you may find the approach used here a bit different. \n",
    "****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You may need to install some specialized packages required to execute the code in this notebook. Uncomment the code in the cell below and execute to install these packages.\n",
    "> For some environments you may need to install graphviz by following the instructions for your operating system given [here](https://graphviz.gitlab.io/download/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n",
    "#!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to import the required packages to run this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "from tensorflow import convert_to_tensor \n",
    "import keras.utils as ku\n",
    "from keras.utils import plot_model\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras import regularizers, callbacks\n",
    "from keras.layers import Dropout\n",
    "from keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import rankdata\n",
    "import sklearn.metrics as metrics\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Applying 2-d convolution\n",
    "\n",
    "Let's review how we can apply convolution operations in two dimensions. Figure 1.1 illustrates a simple example of applying a convolution operator to a 2-d input array. The 2-d input array can be a gray-scale image, for example. \n",
    "\n",
    "A 3X3 convolutional operator is illustrated in Figure 1.1. This operator is applied to a small image of dimension of 4X4 pixels. Within the 4X4 image the operator is only applied four times, where the complete operator lies on the image. Convolution where the operator is completely contained within the input sample is sometimes called call **valid convolution**. \n",
    "\n",
    "In this case the 4X4 pixel input results in a 2X2 pixel output. You can think of the convolutional operator mapping from the center of the operator input to pixel in the output. Similar to the 1-d case, for a convolution operator with span $s$ in both dimensions the output array will be reduced in each dimension by $\\frac{s + 1}{2}$\n",
    "\n",
    "![](img/2D-Conv.JPG)\n",
    "<center>Figure 1.1. Simple 2-d convolution operation</center>\n",
    "\n",
    "For a convolution with the identical span $s$ in both dimensions we can write the convolution operations as: \n",
    "\n",
    "$$S(i,j) = (I * K)(i,j) = \\sum_{m = {i - a}}^{i + a} \\sum_{n = j - a}^{j + a} I(i, j) K(i-m, j-n)$$\n",
    "\n",
    "Notice that $S$, $I$ and $K$ are all tensors. Given this fact, the above formula is easily extended to higher dimensional problems. \n",
    "\n",
    "The convoluational kernel $K$ and the image $I$ are commutative so there is an alternative representation by applying an operation known as **kernel flipping**. Flipped kernel convolution can be represented as follows:\n",
    "\n",
    "$$s(i,j) = (I * K)(i,j) = \\sum_{m = {i - a}}^{i + a} \\sum_{n = j - a}^{j + a} I(i-m, j-n) K(i, j)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convolution in higher dimensions\n",
    "\n",
    "In deep convolutional neural networks the input typically have several **input channels** with the convolution operation resulting in multiple **output channels**. \n",
    "\n",
    "So far, we have only looked at single channel convolution. Here are some examples of input tensors with different numbers of input channels:\n",
    "- A gray-scale image has a single input channel of pixels. \n",
    "- A color image typically has 3 input channels, {Red, Green, Blue}.\n",
    "- A color movie has a large number of input channels. For $n$ images in the video there are $3 * n$ images counting the Red, Green, Blue channels for each image.  \n",
    "\n",
    "The purpose of the a convolutional neural network is to create a feature map. To create the most general feature map convolution operators need not be restricted to a single input channel. \n",
    "\n",
    "Further, the feature map can can many channels, resulting in multiple output channels. Each channel is a set of features extracted from the input tensor. This concept is illustrated in Figure 1.2 below. In this figure a set of convolution operators transform the input tensors to multiple channels of the feature map. Each convolution operator maps to a specific output channel in the feature map. \n",
    "\n",
    "![](img/MultiChannel.JPG)\n",
    "<center>Figure 1.2. Example of multichannel convolution</center>\n",
    "\n",
    "Let's investigate the case of a 3-d input tensor $V$, a 3-d output tensor $Z$ and 4-d convolution tensor $K$. In this case the convolution equation becomes:\n",
    "\n",
    "$$Z_{i,j,k} = (V * Z)(i,j,k,l) = \\sum_{l} \\sum_{m = -a}^{a}\n",
    "\\sum_{n = -a}^{a}\n",
    "V_{i,j,l} \\cdot K_{i-m,j-n,k,l}$$\n",
    "where,  \n",
    "$i,j$ are the spatial dimensions,  \n",
    "$k$ is the index of the output channel,  \n",
    "$l$ is the index of the input channel,  \n",
    "$K_{i,j,k,l}$ is the kernel connecting the $l$th channel of the input to the $k$th channel of the output for pixel offsets $i$ and $j$    \n",
    "$V_{i,j,l}$ is the $i,j$ input pixel offsets from channel $l$ of the input,  \n",
    "$Z_{i,j,k}$ is the $ i,j$ pixel offsets from channel $k$ of the output,  \n",
    "$a = \\frac{1}{2}(kernel\\_span + 1)$, for an odd length kernel.  \n",
    "\n",
    "Notice that the summation in the equation above is over input channels $l$ as well as the spatial dimensions $m,n$ of the convolutional operator. This operation is applied for each spatial pixel coordinate $i,j$. This generalization allows the convolutional kernel to operate over multiple input channels. This is property is highly desirable since features should be extracted from the entire input tensor. \n",
    "\n",
    "The entire forgoing operation is performed for each output channel $k$. Therefore, you can think of the $k$th dimension of the 4-d kernel tensor as indexing multiple 3-d convolution kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parameter sharing and sparse interactions\n",
    "\n",
    "**Parameter sharing** and **sparse interactions** are two powerful aspects of convolutional neural networks. To understand what these terms mean and why they are important, consider the fully connected neural network in Figure 1.3 below.  \n",
    "![](img/FullyConnected.JPG)\n",
    "<center>Figure 1.3. A fully connected network</center>\n",
    "\n",
    "The fully connected neural network has a large number of weights, $5^2 = 25$ in this case. Compare this network to the convolutional layer with $2 \\times 2$ operator will have only $2*2 = 4$ weights. For a more realistic case, the fully connected network might have millions of weights. The convolutional network using **weight sharing** across the entire layer. For a $5 \\times 5$ CovNet layer there are only $5 * 5 = 25$ weights. Parameter sharing is also referred to as **tied weights**. This is an example of a **sparse interaction**.\n",
    "\n",
    "The shared nature of the parameters in convolutional neural networks is a significant reduction in complexity. Statistically, such a sparse model is said to **share statistical strength**. This statistical strength results in model weights with less uncertainty and a model less likely to be over-fit.  \n",
    "\n",
    "The computational efficiency of convolutional neural networks is not just in training. Convolutional neural networks are also efficient for creating **rich feature maps** which lead to high levels of accuracy in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Pooling and invariance\n",
    "\n",
    "To further reduce dimensionality and to make the feature map robust to small shifts in the input, **pooling** is applied following the nonlinear detection stage. Pooling involves applying an aggregation function to patches of the output from the nonlinear detector of a convolution layer. \n",
    "\n",
    "Researchers have tried a number of pooling schemes. In practice **max pooling** (Zhou and Chellappa, 1988) has proved to be both robust and versatile. In max pooling the output value is just the maximum of the input values in each patch. \n",
    "\n",
    "Unlike convolutional layers, there are no parameters to learn in pooling layers. \n",
    "\n",
    "Max pooling has the nice property of being robust to small shifts in the input values. For example, Figure 1.4 shows the response of max pooling to a right sift of a single pixel for an operator with a span of 3. Notice that the response itself is simply shifted one pixel to the right as well. The pattern of maximum values in the feature map (the output) remains the same. \n",
    "\n",
    "![](img/ShiftOne.JPG)\n",
    "<center>Figure 1.4.   \n",
    "    Example of max pooling responses to one pixel shift right</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Stride and tiling\n",
    "\n",
    "Up until now, we have only moved the convolution operator one pixel at a time. There is no restriction that requires this type of step, however. In fact, a convolutional operator or pooling operator can be moved by several pixels in any direction. This step size is known as the **stride** of the operator. \n",
    "\n",
    "Operators with stride greater than one **decimate** the number of pixels in the output. This can be useful when resizing images of different dimensions so they have constant input dimensions for a neural network. \n",
    "\n",
    "Using an operator with $stride = span$ is a special case known as **tiling**. Tiled operators are laid out without any overlap. Figure 1.5 shows an example of tiling. In this case, an input of dimensions $6X6$ is tiled by operators with dimension $3X3$ with stride of 3. Only 4 tiles fit on the input in this case, resulting in a $4X4$ output array.\n",
    "\n",
    "![](img/Tiled.JPG)\n",
    "<center>**Figure 1.5. Example of a tiled convolution operator**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-1:** You will now undertake an exercise to better understand the effects of max pooling. To simplify this exercise you will work with 1-dimensional data and **edge detection** operators. Do the following:   \n",
    "> 1. Create a function named `max_pool` that returns maximum of the absolute value kernel operator with a stride of 1 over the input series. The kernel span should be an argument to the function, with default value of 3. For simplicity start the operator at the beginning of the series.     \n",
    "> 2. Create two edge detection convolutional kernels, $kernel1 = [-0.5, 1.0, -0.5]$ and $kernel2 = [0.5, -1.0, 0.5]$.   \n",
    "> 3. Apply both of the convolution kernels to the `series1` provided using the [numpy.convolve](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html) function. Assign these two results to separate variables.    \n",
    "> 4. Apply your `max_pool` function to the convolution result series. Assign these two results to different variables.    \n",
    "> 5. Display the results comparing the original series, the convolution series to the max-pooled convolution series using the `plot_pool` and 'plot_series' functions provided.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(series, pool, original_series, x_pool, label, offset, ax):    \n",
    "    x = list(range(series.shape[0]))\n",
    "    x0 = [x + offset for x in range(len(original_series))]\n",
    "    ax.plot(x, series, label = 'Detected series')\n",
    "    ax.plot(x_pool, pool, color = 'red', label = label)\n",
    "    ax.plot(x0, original_series, color = 'green', ls='--', label = 'Original series')\n",
    "    ax.set_title('Series of raw values and max pool values')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Value') \n",
    "    \n",
    "def plot_pool(series1, series2, original_series, pool1, pool2, span=3):\n",
    "    x = list(range(series1.shape[0]))\n",
    "    end = series1.shape[0]\n",
    "    end_pool = pool1.shape[0]\n",
    "    end_remain = end % end_pool\n",
    "    pool_stride = int((end_pool - end_remain)/end_pool) + 1\n",
    "    x_pool = list(range(end_remain, end_pool + end_remain, pool_stride))\n",
    "    print('length of the input series = ' + str(len(x)))\n",
    "    print('length of the max pool series = ' + str(len(x_pool)))\n",
    "\n",
    "    ## Display the results\n",
    "    fig,ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "    ## First channel\n",
    "    plot_series(series1, pool1, original_series, x_pool, 'First channel max pool', (span -1)/2, ax[0])\n",
    "    ## Second channel\n",
    "    plot_series(series2, pool2, original_series, x_pool, 'Second channel max pool', (span -1)/2, ax[1])\n",
    "    \n",
    "\n",
    "nr.seed(12233)\n",
    "series1 = np.concatenate((np.zeros((20,)), np.zeros((5,)) + 1.0, np.zeros((5,)) - 1.0, np.zeros((20,))))\n",
    "\n",
    "def max_pool(series, span=3, stride=1):\n",
    "    '''Performs simple 1d max pooling with\n",
    "    an operator over span and stride 1'''\n",
    "    ## Your code goes below\n",
    "    out =[]\n",
    "    len = series.shape[0]\n",
    "    offset = int((span + 1)/2) #- 1\n",
    "    print('offset = ' + str(offset))\n",
    "    stop = len - 1 \n",
    "    for i in range(offset, stop, stride): \n",
    "        out.append(np.max(np.abs(series[i:i+span])))\n",
    "    return np.array(out)   \n",
    "\n",
    "\n",
    "kernel1 = np.array([-0.5, 1.0, -0.5])\n",
    "kernel2 = np.array([0.5, -1.0, 0.5])\n",
    "result1 = np.convolve(series1, kernel1)\n",
    "result2 = np.convolve(series1, kernel2)\n",
    "    \n",
    "pool1 = max_pool(result1)\n",
    "pool2 = max_pool(result2)\n",
    "plot_pool(result1, result2, series1, pool1, pool2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. Define and apply new 5-point convolution kernels for the edge detection, $[-0.25, - 0.25, 1.0, -0.25, -0.25]$ and $[0.25, 0.25, -1.0, 0.25, 0.25]$. \n",
    "> 7. Apply the max pooling with kernel span of 5 to both of the edge detection series.  \n",
    "> 8. Display the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel1 = np.array([-0.25, - 0.25, 1.0, -0.25, -0.25])\n",
    "kernel2 = np.array([0.25, 0.25, -1.0, 0.25, 0.25])\n",
    "result1 = np.convolve(series1, kernel1)\n",
    "result2 = np.convolve(series1, kernel2)\n",
    "\n",
    "span=3\n",
    "pool1 = max_pool(result1, span = span)\n",
    "pool2 = max_pool(result2, span = span)\n",
    "plot_pool(result1, result2, series1, pool1, pool2, span = span)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions in one or a few sentences:   \n",
    "> 1. How can you explain the invariance of the max-pooling results to the sign of the convolved series?\n",
    "> 2. How does the max-pooling operator alter edges deteced by the convolution output series?     \n",
    "> 3. How can you explain the difference in the edge detection series and the max-pooled values between the two kernel spans in terms of the receptive field of the operator?    \n",
    "> **End of exercise**.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.     \n",
    "> 2.     \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Convolutional neural networks with Keras\n",
    "\n",
    "With the foregoing theory and simple examples, let's try constructing and testing a convolutional neural network using Keras. In this case, we will test classification of of the MNIST dataset. The neural network will have the following layers:\n",
    "1. An input layer for the 28X28 images.\n",
    "2. A multi-layer convolutional neural network to create a feature map.\n",
    "3. A fully-connected hidden layer to perform the classification.\n",
    "4. A output layer to indicate which digit is most likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing the dataset\n",
    "\n",
    "The preparation of this dataset is follows nearly identical steps to the process followed in an earlier lesson. In summary, three preparation steps are performed:\n",
    "1. Load the training and test image data and labels.\n",
    "2. Reshape the image tensors so the neural network views them as a 4-d tensor. For the training images the tensor has a shape of (60000,28,28,1) corresponding to 60000 images of dimension $28 \\times 28$ with a single channel (these are gray-scale images). At the same time the pixel values are converted to `float` type in the range $\\{ 0,1 \\}$.\n",
    "3. The labels are represented by dummy variables using the Keras [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function.\n",
    "\n",
    "> **Note:** Notice that the channel index is the last of the 4 used for the image tensors. TensorFlow uses a **channel last** format for image tensors, in $image \\times height \\times width \\times channel$, or NHWC format. If you have worked with Torch you will know that this platform uses a channel first format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float')/255\n",
    "print(train_images.shape)\n",
    "print(train_images.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_images.shape, test_labels.shape)\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float')/255\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = ku.to_categorical(train_labels)\n",
    "print(train_labels[5:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = ku.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the one-hot encoding of the labels. There is one dummy variable (column) for each label value, $\\{0,...,9\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining the model. \n",
    "\n",
    "It is now time to define the model. \n",
    "\n",
    "> Early stopping regularization is an old idea in machine learning. The concept is simple, stop learning at the point the model starts to over-fit. Early stopping measures learning with a single selected validation metric. The early stopping algorithm can be describe as follows, using a validation metric like validation error rate or validation loss rate.    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Proceedure Early_Stoping\n",
    "    patience = initialPatience\n",
    "    patienceCount = 0\n",
    "    maxSteps = initialMaxSteps\n",
    "    steps = 0\n",
    "    bestMetric = 1.0\n",
    "    model = Instantiate_Model(trainData, validationData, hyperparameters)    \n",
    "    while patience >= patienceCount and steps <= maxSteps:     \n",
    "        model = Train(model)  \n",
    "        metric = Validate(model)  \n",
    "        if metric > bestMetic: \n",
    "            patienceCount =+\n",
    "        else  \n",
    "            bestMetric = metric   \n",
    "            bestModel = model\n",
    "        steps =+    \n",
    "    return bestModel        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, the callbacks required to implement early stopping regularization are defined. The steps to define the [early stopping call backs with Kera](https://keras.io/api/callbacks/early_stopping/) are:    \n",
    "1. Define a path to an HDF5 file used to save the training history. \n",
    "2. The first callback stops the training when the validation loss does not improve within 2 epochs, the patience.  \n",
    "3. The second callback saves the model weights resulting from the epoch with the best validation loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up and call-backs for early stopping\n",
    "patience = 2\n",
    "filepath = 'my_model_file.keras' # define where the model is saved\n",
    "def set_callbacks_list(patience, filepath):\n",
    "    callbacks_temp = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = patience # Stop after patience steps with lower accuracy\n",
    "         ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True # Only save model if it is the best\n",
    "         )\n",
    "        ]\n",
    "    \n",
    "    return callbacks_temp\n",
    "    \n",
    "callbacks_list = set_callbacks_list(patience, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-2:** You will now use Keras to define the CovNet model named `nn`. This model uses both convolutional layers and max pooling layers. You will define this model by completing the function in the code cell below. You can find [documentation on the Keras convolutional layers here](https://keras.io/layers/convolutional/). You can find [documentaton on Keras pooling layers here](https://keras.io/layers/pooling/).\n",
    "> The architecture of the model you will create is:\n",
    "> 1. A 3X3 convolutional layer with 32 output channels using the [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2D/) function, with `activation='relu'`. Recall that this operation creates a feature map with 32 channels from the single gray-scale input channel. This is the input layer of the model, so make sure you define the input shape. \n",
    "> 2. A 2X2 max pooling layer using the [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) function.\n",
    "> 3. A dropout layer is used to improve regularization of the model using the [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) function with argument 0.2. \n",
    "> 3. A 3X3 convolution layer with 64 output channels, with `activation='relu'`. The 32 channels of the input layer are mapped to a 64 channel feature map. \n",
    "> 4. Another 2X2 max pooling layer. \n",
    "> 5. A dropout layer to improve regularization of the model with argument 0.2. \n",
    "> 6. A final 3X3 convolution layer with 64 output channels, with `activation='relu'`. \n",
    "> 7. A dropout layer to improve regularization of the model with argument 0.2. \n",
    "> 8. Flatten the 64 channel feature map to a vector.\n",
    "> 9. A Dense (fully-connected) hidden layer with 64 units is the first classifier layer, with `activation='relu'`. A limited amount of regularization is applied using Keras [layer regularizer l2](https://keras.io/api/layers/regularizers/) as a ` kernel_regularizer`, with hyperparameter 0.1. This layer operates on the flattened feature map. \n",
    "> 10. A dropout layer to improve regularization of the model with argument 0.2. \n",
    "> 11. The output Dense layer has 10 units with `activation='softmax'` to indicate the classification of the digits.\n",
    "> 12. A summary of the model is printed.\n",
    "> 13. The model object is returned. \n",
    "> Execute your code and examine the summary of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(): \n",
    "    ## Your code goes below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return nn\n",
    "\n",
    "nn = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is quite bit of interesting information in the summary of this model. In one or a few sentences answer the following questions:\n",
    "> 1. How does the dimensionality of the feature map change at each convolution and max pooling layer? \n",
    "> 2. Compare the number of parameters of your CovNet model to the fully connected model you created for Exercise 3-4. How different is the overall number of parameters and layers? \n",
    "> 3. What does the difference in the numbers of parameters between the CovNet and the fully connected model tell you about the ability to train these models with a finite amount of data? \n",
    "> 4. How many learnable parameters do the max-pooling layers have, and why?    \n",
    "> **End of exercise**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.            \n",
    "> 2.           \n",
    "> 3.           \n",
    "> 4.              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train and test the model\n",
    "\n",
    "Now, it is time to compile and fit your model. This particular model appears to require only limited regularization. The table below shows the limited heuristic search performed on the hyperparameters.\n",
    "\n",
    "| Final Accuracy  | Dropout  | weight decay | Learning Rate |   \n",
    "| --------------- | -------- | ------------ | ------------- |\n",
    "| 0.9923          | 0.2      | 0.00         | 0.001        |\n",
    "| 0.9923          | 0.2      | 0.001         | 0.001        |\n",
    "| 0.9890          | 0.5      | 0.001         | 0.001        |\n",
    "\n",
    "\n",
    "The code in the cell below compiles the model using the chosen optimizer with learning rate and weight decay hyperparameters selected. The model is then fit and the history is captured. Run the code in the cell below, which may take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "nn.compile(optimizer=RMSprop(learning_rate=0.001, weight_decay=0.001), \n",
    "           loss = 'categorical_crossentropy', metrics = ['accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "## Now fit the model\n",
    "history = nn.fit(train_images, train_labels, \n",
    "                  epochs = 80, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code in the cells below to plot the loss and accuracy history vs. epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    '''Function to plot the loss vs. epoch'''\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(train_loss) + 1))\n",
    "    plt.plot(x, val_loss, label = 'Validation loss')\n",
    "    plt.plot(x, train_loss, color = 'red', label = 'Training losss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracy(history):\n",
    "    x = list(range(1, len(history.history['accuracy']) + 1))\n",
    "    plt.plot(x, history.history['accuracy'], color = 'red', label = 'Training accuracy')\n",
    "    plt.plot(x, history.history['val_accuracy'], label = 'Validation accuracy')\n",
    "    plt.plot(x, history.history['top_k_categorical_accuracy'], label = 'Top 2 accuracy')\n",
    "    plt.plot(x, history.history['val_top_k_categorical_accuracy'], label = 'Top 2 validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Epoch')      \n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(history)    \n",
    "plot_accuracy(history)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-3:** Examine these results and answer the following questions:    \n",
    "> 1. How does the accuracy and error rate of your CovNet model compare to the regularized fully connected model of Exercise 3-10 of the previous assignment?\n",
    "> 2. How can you characterize the convergence or learning rate of the CovNet model after the first 10 epochs.\n",
    "> 3. Notice that after only a few epochs the top-2 accuracy is nearly 1.0. Given the nature of the output layer of the model, how can you explain this behavior vs. the behavior of the top-1 accuracy? \n",
    ">\n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.            \n",
    "> 2.          \n",
    "> 3.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will continue with the evaluation of the model. One can compute several possible accuracy metrics.    \n",
    "\n",
    "First, you will use **top-1** accuracy along with **top-k** accuracy, in this case top-2. Top-1 accuracy measures the fraction of cases where the correct category is predicted. Top-k accuracy measures the fraction of cases where the correct category is in the top k probabilities of categories. Since there are only 10 categories for this problem we are limiting to $k=2$. For cases with many categories it is common to use top-5 accuracy.    \n",
    "\n",
    "You will also compute and display the mean average precision and mean average recall.  \n",
    "\n",
    "Execute this code in the cell below to compute and display these accuracy metrics along with average precision and average recall.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_imges, test_labels):   \n",
    "    ## Compute class predictions\n",
    "    class_probabilities = model.predict(test_images)\n",
    "    class_predictions = class_probabilities.argmax(axis=1)\n",
    "    \n",
    "    ## Compute the top-1 categorical accuacy.  \n",
    "    categorical_accuracy = CategoricalAccuracy()\n",
    "    categorical_accuracy.update_state(test_labels, class_probabilities)\n",
    "\n",
    "    # Compute the top-2 accuracy.  \n",
    "    top_2_accuracy = TopKCategoricalAccuracy(k=2, name='top_2_categorical_accuracy')\n",
    "    top_2_accuracy.update_state(test_labels, class_probabilities)\n",
    "\n",
    "    ## Summarize and print the result.   \n",
    "    print('Categorical accuracy = ' + str(categorical_accuracy.result().numpy()))\n",
    "    print('Top 2 accuracy = ' + str(top_2_accuracy.result().numpy()))\n",
    "\n",
    "    ## Get the class labels, counts and compute the class-specific recall and precision.  \n",
    "    test_labels_vector = test_labels.argmax(axis=1)\n",
    "    unique_labels, label_counts = np.unique(test_labels_vector, return_counts=True)\n",
    "    class_precision = metrics.precision_score(test_labels_vector, class_predictions, labels=unique_labels, average=None)\n",
    "    class_recall = metrics.recall_score(test_labels_vector, class_predictions, labels=unique_labels, average=None)\n",
    "\n",
    "    ## Comptue and print the mean average precision and recall accounting for class frequency weights\n",
    "    sum_label_counts = np.sum(label_counts)\n",
    "    weighted_average = lambda x: round(np.sum(np.divide(x * label_counts, sum_label_counts)), 4)\n",
    "    print('Average precision = ' + str(weighted_average(class_precision)))\n",
    "    print('Average recall = ' + str(weighted_average(class_recall)))\n",
    "\n",
    "    ## Compute, display and plot the confusion matrix.  \n",
    "    confusion_matrix = metrics.confusion_matrix(test_labels_vector, class_predictions)   \n",
    "    print('\\nConfusion matrix')\n",
    "    print(pd.DataFrame(confusion_matrix))\n",
    "    p = plt.imshow(np.log(np.divide(confusion_matrix + 1.0, np.sum(confusion_matrix, axis=1))))\n",
    "    cb = plt.colorbar(p)\n",
    "    _=cb.set_label('Log count')\n",
    "\n",
    "\n",
    "evaluate_model(nn, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-4:** Examine the evaluation metrics of the model. Notice that the accuarcy, mean average precision and mean average recall are all high. Now, answer these questions:    \n",
    "> 1. Consider the differences between the top-1 (categoracal) accuraccy and top-2 accuracy. Is the difference expected for this model and why?   \n",
    "> 2. Why are average precision and average recall values computed expected for this model?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.        \n",
    "> 2.             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 What features does the convolutional neural network learn?\n",
    "\n",
    "You might well ask, what is in the various layers of the feature map? It turns out, we can actually visualize the output from each layer of the convolutional neural network to get a feel for the learned features. This can be done for both convolutional and max pooling layers.\n",
    "\n",
    "As a first step we must pick and image used as the test case. For this example, we arbitrarily choose the 13th image in the training tensor. Execute the code in the cell below to visualize this image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_images[12,:,:,:]\n",
    "print(img.shape)\n",
    "plt.imshow(img.reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample image turns out to be a number 3. \n",
    "\n",
    "The code in cell below does the following:\n",
    "1. Extracts the layers from the model object into a list. \n",
    "2. Creates an activation map for each layer using the original model object and the list of layer outputs. \n",
    "3. Computes the activations for each layer by applying the predict method to the activation map.\n",
    "\n",
    "> **Note:** In some combinations of environment and versions of Keras and TensorFlow, you may need to change the name of the model object attribute, `nn.input` to `nn.inputs`. \n",
    "\n",
    "We will now extract the activations from the trained model by the following steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Extract the layers from the model    \n",
    "\n",
    "The code below creates a list of the output layers we want to visualize. Execute this code to create the list of these layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [layer.name for layer in nn.layers] # create list of layer names\n",
    "layer_indx = [0,1,3,4,6] # indices of layers we want to visualize\n",
    "layer_outputs = [nn.get_layer(layer_names[i]).output for i in layer_indx]\n",
    "layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Create and compute an activation model    \n",
    "\n",
    "With the layers we want to visualize the activations defined, we can now build a model and extract the activations. This temporary model inherits the weights already learned for the full model. The activations are computed by applying the predict method to the model, with a correctly shaped image tensor as input. Execute the code in the cell below.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_model = models.Model(inputs=nn.inputs[0], outputs=layer_outputs)\n",
    "\n",
    "img_tensor = np.expand_dims(img, axis=0)  # shape becomes (28, 28, 1, )\n",
    "activations = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Display the activations\n",
    "\n",
    "Now that the activations have been computed for the test image, it is time to display them. The code in the cell below does the following for each layer of interest:    \n",
    "1. A plot grid is laid out for each channel in the layer.     \n",
    "2. The activations for each channel of the layer is plotted on the grid.\n",
    "\n",
    "Execute the code in the cell and examine the resulting activation plots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each layer's activations\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # Drop the batch dimension: shape (height, width, channels)\n",
    "    activation_maps = layer_activation[0]\n",
    "    num_channels = activation_maps.shape[-1]\n",
    "\n",
    "    # Compute a grid size so all channels fit in a square\n",
    "    grid_size = int(math.ceil(math.sqrt(num_channels)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle(layer_name)\n",
    "\n",
    "    # Plotting each channel\n",
    "    for i in range(num_channels):\n",
    "        axes[i].imshow(activation_maps[..., i], cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    for i in range(num_channels, grid_size * grid_size):\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-5:** Examine the above sets of images. There are 5 sets of images, one for each of the output of the first 5 layers of the model. For each layer, there is an image showing each channel of the feature map. Keeping in mind that CNNs are known to be effective as finding find-grain features, in one or a few sentences, answer the following questions: \n",
    "> 1. The initial single channel $28 \\times 28 \\times 1$ input image tensor becomes a $32 \\times 26 \\times 26 \\times 1$, 32 channel feature map after the first convolution. The second $32 \\times 13 \\times 13 \\times 1$ output is from the max-pooling layer. In general terms, describe how much these feature maps resemble the original image, and why do you think this might be the case given the receptive field of the kernels?    \n",
    "> 2. Examine the feature maps created by the forth (convolution), fifth (max-pooling), and seventh (convolution) layers. How can you best describe the level of abstraction of these layers and the the resemblance to the original image?      \n",
    "> 3. What effect can you observe of using the max-pooling layer other than dimensionlity reduction, and why?\n",
    "> 4. How does the \n",
    "> 5. What general relationship or trend can you identify between the spatial dimensionality of the feature map and the level of abstraction and localization of the features? \n",
    "> **End of exercise**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.           \n",
    "> 2.            \n",
    "> 3.          \n",
    "> 4.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Padding convolution\n",
    "\n",
    "As you have observed, each convolution layer in a neural network reduces the spatial dimensions by $\\frac{span+1}{2}$ for odd span kernels. The coverage of the convolution operator can be expanded by **zero padding** in the spatial dimension. With the zero padding added, the convolution operator covers the entire spatial dimension of the input tensor. This type of convolution is sometime referred to as **same convolution**, since the output tensor has the same spatial dimensions. \n",
    "\n",
    "In some situations, adding zero padding produces a better feature map. Zero padding allows each pixel to be visited the same number of times by the convolution operator. Without padding pixels at the edge, are not full represented in the feature map. More importantly, the larger feature map will have larger model capacity. \n",
    "\n",
    "2-d convolution with zero padding is illustrated in Figure 4.1 below. The original input 2-d input tensor is shown in gray. The span of the kernel is 3X3 so one additional column of zeros is added to each sided along with a row of zeros top and bottom. As illustrated, the kernel can operate on the edge pixels. The result is an output tensor with the same spatial dimension as the input. \n",
    "\n",
    "![](img/Padding.JPG)\n",
    "<center>**Figure 4.1. 2-d convolution with zero padding**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-6:** With Keras, adding zero padding to convolutional layers is easy. The `padding = 'same'` argument adds zero padding. The default is `padding = 'valid'` adds no padding. Create a new model, with a new name 'nns', with the same layers as the foregoing model, and a dropout rate of 0.2, but with the `padding = 'same'` argument for the convolutional layers.  Then print the summary of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes below\n",
    "\n",
    "\n",
    "nns.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the summary of your model and answer these questions:   \n",
    "> 1. The final feature map is $64 \\times 7 \\times 7$, rather than $64 \\times 3 \\times 3$ for the model without padding. This feature map has 49 pixels per channel, compared to 9 pixels per channel previously. Do you expect that this feature map will contain more information and why? \n",
    "> 2. How has the number of parameters changed for the convolutional layer, and is this expected? \n",
    "> 3. Compare the number of learnable parameters for the 64 unit fully connected layer between the models with and without padding. How can you account for this difference?\n",
    ">    \n",
    "> **End of exercise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**   \n",
    "> 1.      \n",
    "> 2.        \n",
    "> 3.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next execute the code in the cell below to compile and fit the model. Notice that we have increased patience from 2 to 5 for the early stopping criteria. This process will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "nns.compile(optimizer=RMSprop(learning_rate=0.001, weight_decay=0.001), \n",
    "           loss = 'categorical_crossentropy', metrics = ['accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "## Now fit the model\n",
    "patience = 5\n",
    "callbacks_list = set_callbacks_list(patience, filepath)\n",
    "history_nns = nns.fit(train_images, train_labels, \n",
    "                  epochs = 80, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cells below to plot the loss vs. epoch and accuracy vs. epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_nns)    \n",
    "plot_accuracy(history_nns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display accuracy netrics\n",
    "evaluate_model(nns, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-7:** Keeping in mind that small (3rd digit) differences in metrics are often simply a result of random sampling do these results appear significantly difference than for the un-padded model? It may well be the case that richer feature map may not be important for this relatively simple problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Efficient CNN\n",
    "\n",
    "As you have likely noticed, training the foregoing CNN models is a rather computationally intensive process. This was even the case got the small networks used for these examples. In large part, the slow training is a result of the large number of parameters that must be learned for 4-D and higher dimensional convolutions. Consider that for a 32 to 32 channel $3 \\times 3$, 4-D convolution operator the network must learn $3 \\times 3 x 32 x 32 = 9,216$ parameters.       \n",
    "\n",
    "There is a more efficient formulation of the CNN layer, the **separable convolution operator**. The separable convolution operator consists of two components, a **depth-wise 2-D convolution operator** and a **point-wise convolution operator**. The depth-wise convolution learns a set of 2-D weights independently for each channel. The point-wise convolution learns the weights of a 1-D convolution operator that 'mixes between the channels'. Separating the convolutional operator like this greatly reduces the number of parameters that must be learned. For a $3 \\times 3$ 32 to 32 channel convolution the separable operator has $3 \\ times 3 + 32 + 32 = 73$ parameters, a large change compared to the 4-D operator.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-8:** To see the the result of applying separable convolution to the digit classification problem you will do the following. You will use a different method to specify a sequential Keras model than you have used previously. The layers are specified in evaluation order within the list which is the argument to the [keras.models.sequntial](https://keras.io/guides/sequential_model/) function. To complete the code in the function below you can copy the layers already used for Exercise 6-6. You only need to modify the code that defines the convolutional layers, not use the `add` method on the model. Make sure you put comments between the layer specifications in the list. To help you get started the specification of the first two layers is provided. When you have completed the layer list, execute the code in the cell below.                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sep_cnn(shape=(28, 28, 1, ), dropout=0.2, padding='same'):\n",
    "    cnnsep = models.Sequential(\n",
    "        [\n",
    "           layers.Input(shape=shape),\n",
    "           layers.SeparableConv2D(32, (3, 3), padding=padding, activation = 'relu'),\n",
    "        \n",
    "           ## Put the layer list here    \n",
    "           \n",
    "\n",
    "\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "    return cnnsep\n",
    "\n",
    "nnsep = build_sep_cnn()\n",
    "nnsep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "nnsep.compile(optimizer=RMSprop(learning_rate=0.001, weight_decay=0.001), \n",
    "           loss = 'categorical_crossentropy', metrics = ['accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "## Now fit the model\n",
    "patience = 5\n",
    "callbacks_list = set_callbacks_list(patience, filepath)\n",
    "history_nnsep = nnsep.fit(train_images, train_labels, \n",
    "                  epochs = 80, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)\n",
    "\n",
    "## Display learning history\n",
    "plot_loss(history_nnsep)    \n",
    "plot_accuracy(history_nnsep) \n",
    "\n",
    "## Display accuracy netrics\n",
    "evaluate_model(nnsep, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer the following questions.      \n",
    "> 1. How do the performance metrics of the model using the separable convolution compare the the model you constructed and evaluated in Exercise 4-7?\n",
    "> 2. Compare the number of model parameters of the separable convolution layers to the full convolution layers in the model you created in Exercise 4-6. What does this difference tell you about separable convolution vs. full convolution?      \n",
    "> 3. Compare the time required per step to train the model with separable convolution to the time per step for the fully convolution model. Given the reduced number of parameters, is the difference in time required consistent?       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.        \n",
    "> 2.         \n",
    "> 3.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Learning Rate     \n",
    "\n",
    "The two foregoing models have been trained using a fixed learning rate. In many cases a fixed learning rate is far from optimal. The problem arises since initial learning can take large steps, but small steps are required toward convergence to ensure a stable termination of the algorithm. A fixed low learning rate will learn too slowing a the beginning of training but converge at the end. On the other hard a high learning rate is effective at the beginning of training, but will lead to poor or unstable convergence.   \n",
    "\n",
    "The solution is to use a variable learning rate. The variable learning rate decreases at the training proceeds. In this way, the learning rate can be optimized to ensure the fastest possible convergence of the learning. The variable learning rate is determined by a schedule. Common schedules include linear decreases and exponential decreases. Keras supports a number of [learning rate schedulers](https://keras.io/api/optimizers/learning_rate_schedules/).      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-9:** You will now investigate using a variable learning rate for training the CNN model. The code in the cell bellow does the following:  \n",
    "> 1. An exponentially decay learning rate schedule is created using [tensorflow.keras.optimizers.schedules.ExponentialDecay](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/). Notice that the initial learning rate has been increased from the previous exercise.  \n",
    "> 2. The `nnv` model is instantiated, using the function you created earlier, and compiled. A learning rate schedule can be incorporated into most Keras optimizers. In this case, the [RMSprop optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/RMSprop) has an `learning_rate` argument set to the learning rate just created, and with `weight_decay=0.001`.    \n",
    "> 3. The model is fit using 80 epochs, or until the stopping criteria set in the call-back function is reached. The larger number of epochs allow the training to run closer to completion.    \n",
    "> \n",
    "> Execute this code to training the model, which will take some time to run.\n",
    "\n",
    "> **Note:** You may be wondering why a new model needs to be instantiated and compiled to run this experiment. The reason is that Keras (and TensorFlow) model objects will retain any weight values from previous learning. This property can be an advantage if one wishes to do additional training. For example, additional training can be performed when additional data is available, without starting over again. However, if one wishes to run an independent experiment, as we do here, it is best to instantiate a new model object.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the learning rate schedule \n",
    "initial_learning_rate = 0.005\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate, \n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "## Compile the model\n",
    "nnv = build_sep_cnn()\n",
    "nnv.compile(optimizer = RMSprop(learning_rate=lr_schedule, weight_decay=0.001),\n",
    "                loss = 'categorical_crossentropy', \n",
    "                metrics = ['accuracy', 'top_k_categorical_accuracy'])\n",
    "    \n",
    "## Now fit the model\n",
    "patience = 5\n",
    "callbacks_list = set_callbacks_list(patience, filepath)\n",
    "history_vnnsep = nnv.fit(train_images, train_labels, \n",
    "                  epochs = 80, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next execute the code in the cell below to evaluate the model just trained.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_vnnsep)    \n",
    "plot_accuracy(history_vnnsep) \n",
    "\n",
    "## Display accuracy metrics\n",
    "evaluate_model(nnsep, test_images, test_labels)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the results and compare them to those created for Exercise 6-3.      \n",
    "> 1. Compare the number of epochs that occur before reaching either the early stopping criteria or the limit with the model trained in Exercise 4-8. What does this difference tell you about the effect of using a decaying learning rate?    \n",
    "> 2. Notice the difference in the gap between the training and validation curves for both loss and top-1 accuracy compared to the model trained in Exercise 4-8. What does this difference tell you about the training and generalization of the respective models?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.           \n",
    "> 2.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Residual Blocks\n",
    "\n",
    "To train very deep models one needs to do more than just stack sequentially evaluated convolutional layers. Simply stacking increasing numbers of convolutional layers results in diminishing gradients during back propagation. In this case, we are working with relatively shallow networks, which should not encounter problems with vanishing gradients. None the less, we can demonstrate the principle.    \n",
    "\n",
    "The code below implements a model with two residual blocks. This code uses another method for specifying Keras models. The output of each layer is set to a named variable. The input of each layer is explicitly specified using these named variables. Using this approach allows us to create non-sequential models. In this case, we must use an addition operation to combine the residual from the convoluton layers with the **identity shortcut** branch.    \n",
    "\n",
    "The residual blocks use separable convolutional layers. The first residual block has 32 channels and the second residual block has 64 channels. A $1 \\times 1$ convolutional layer is used to transform the 32 channel identity shortcut from the first residual block to the required 64 channels for the second residual block. Each residual block is followed by a max pooling layer.    \n",
    "\n",
    "Execute the code in the cell below and examine the model summary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_input = layers.Input(shape=(28, 28, 1, ))\n",
    "u = layers.SeparableConv2D(32, (3, 3), padding = 'same', activation = 'relu')(CNN_input)\n",
    "\n",
    "## A residual block\n",
    "v = layers.SeparableConv2D(32, (3, 3), padding = 'same', activation = 'relu')(u)\n",
    "v = layers.SeparableConv2D(32, (3, 3), padding = 'same', activation = 'relu')(v)\n",
    "v = layers.Add()([v,u]) # Combine the output of the pervious layer with the output of the convolutional layers\n",
    "v = layers.MaxPooling2D((2, 2))(v)\n",
    "\n",
    "## Another residual block\n",
    "r = layers.Conv2D(filters=64,kernel_size=(1,1))(v) # 1x1 convolution layers to trasform form 32 to 64 channels\n",
    "w = layers.SeparableConv2D(64, (3, 3), padding = 'same', activation = 'relu')(v)\n",
    "w = layers.SeparableConv2D(64, (3, 3), padding = 'same', activation = 'relu')(w)\n",
    "w = layers.Add()([w,r]) # Combine the output of the pervious layer with the output of the convolutional layers\n",
    "w = layers.MaxPooling2D((2, 2))(w)\n",
    "\n",
    "## Flatten and classify\n",
    "x = layers.Flatten()(w)\n",
    "x= layers.Dense(64, activation = 'relu',\n",
    "                        kernel_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "CNN_output = layers.Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "CNN_model = models.Model(CNN_input, CNN_output)\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this model has an additional separable convolution layer the total number of parameters that must be learned has increased only slightly compared to the sequential model we have been working with.     \n",
    "\n",
    "Since the model structure is not sequential, it will be useful to visualize how the layers are connected. Display a graph of the model by executing the code in the cell below which uses the [keras.utils.plot_model](https://keras.io/api/utils/model_plotting_utils/) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(CNN_model, \"mini_resnet.png\", show_shapes=True, dpi=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined it is time to train and evaluate it. The same process and hyperparameters are used here. Execute the code below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the learning rate schedule \n",
    "initial_learning_rate = 0.005\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate, \n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "## compile the model\n",
    "CNN_model.compile(optimizer = RMSprop(learning_rate=lr_schedule, weight_decay=0.01),\n",
    "                loss = 'categorical_crossentropy', \n",
    "                metrics = ['accuracy', 'top_k_categorical_accuracy'])\n",
    "    \n",
    "## Now fit the model\n",
    "patience = 5\n",
    "callbacks_list = set_callbacks_list(patience, filepath)\n",
    "history_CNN_model = CNN_model.fit(train_images, train_labels, \n",
    "                  epochs = 80, batch_size = 128,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)\n",
    "\n",
    "plot_loss(history_CNN_model)    \n",
    "plot_accuracy(history_CNN_model) \n",
    "\n",
    "## Display accuracy netrics\n",
    "evaluate_model(CNN_model, test_images, test_labels)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-10:** Answer the following questions based on the results shown above.    \n",
    "> 1. Are the accuracy results of the model with residual blocks comparable to the sequential model evaluated in Exercise 6-9 and why should this result be expected.       \n",
    "> 2. Is there a significant difference in the number of training epochs required between the sequential model used in Exercise 6-9 and the model with residual blocks and why would this behavior be expected?    \n",
    "> 3. Why is a $1 \\times 1$ convolution layer required in the identity loop of the second residual block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**:\n",
    "> 1.        \n",
    "> 2.          \n",
    "> 3.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In these exercises you have applied convolutional neural networks (CNNs) to an image calassification problem. Some key points of this lesson are:     \n",
    "1. CNNs learn powerful features from images.\n",
    "2. Convolutional feature maps have multiple channels with a spatial dimensions.      \n",
    "3. Multi-layer CNNs create highly absract features at deeper layers. Deeper architectures can learn more powerful features.\n",
    "4. The convolutional feature map is used as an input to a classifier.\n",
    "5. Evaluation of multi-class classifiers can be evaluated by both top-1 and top-k accuacy metrics.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright  2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
