{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f104067e",
   "metadata": {},
   "source": [
    "# CSCI E-25    \n",
    "## Introduction to Machine Learning and Linear Models\n",
    "\n",
    "### Steve Elston   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79ae42",
   "metadata": {},
   "source": [
    "## Introduction to Linear Models \n",
    "\n",
    "The concept of the linear model is the basis of many statistical and machine learning models. Further, an understanding of linear models is a good basis for understand many other types of statistical and machine learning models.   \n",
    "\n",
    "In this lesson we will focus on linear classification models, but the lessons drawn from this discussion can be applied to many other types of ML models. By developing an understanding of linear models, you are building a foundation to understand many other machine learning models. Nearly all machine learning methods suffer from the same problems, including over-fitting and mathematically unstable fitting methods. Understanding these problems in the linear regression context will help you work with other machine learning models.     \n",
    "\n",
    "The method of regression is one of the oldest and most widely used analytics methods. The goal of regression is to produce a model that represents the **best fit** to some observed data. Typically the model is a function describing some type of curve (lines, parabolas, etc.) that is determined by a set of parameters (e.g., slope and intercept). *Best fit* means that there is an optimal set of parameters which minimize an error criteria we choose.     \n",
    "\n",
    "Many machine learning models, including some of the latest deep learning methods, are a form of regression. **Linear regression** is the foundational form of regression. Linear regression minimizes squared error of the predictions of the dependent variable using the values of the independent variables. This approach is know as the **method of least squares**.   \n",
    "\n",
    "Linear classifiers are an extension of linear regression. The linear output is transformed to a categorical probability distribution. The category with the highest probability is typically selected as the final result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1958c12",
   "metadata": {},
   "source": [
    "## Load and Prepare MNIST image Data    \n",
    "\n",
    "We will now work through an example of using a linear classifier for image classification. For this example we will use the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) image data set. MNIST is a commonly used benchmark standard data set used for image classification research. The dataset is comprised of $28 \\times 28$ images of hand written digits in the set $[0-9$. There are 60,000 training images and labels and 10,000 test images and labels.            \n",
    "\n",
    "To get started, execute the code in the cell below to import the packages you will require for this example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db8a2c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T04:14:42.958214Z",
     "start_time": "2024-02-20T04:14:28.623274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tt/z2dnsthj301301nmslkrspm40000gp/T/ipykernel_41363/2627722997.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnr\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mnist\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp_utils\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mku\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/__init__.py:8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"DO NOT EDIT.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03msince your modifications would be overwritten.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _tf_keras\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/_tf_keras/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tf_keras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/_tf_keras/keras/__init__.py:8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"DO NOT EDIT.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03msince your modifications would be overwritten.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m callbacks\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/activations/__init__.py:8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"DO NOT EDIT.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03msince your modifications would be overwritten.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deserialize\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialize\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/src/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/src/activations/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtypes\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m elu\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m exponential\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gelu\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/src/activations/activations.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi_export\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras_export\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/src/backend/__init__.py:33\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Import backend functions.\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m backend() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorflow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 33\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m backend() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjax\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribution_lib\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m image\n",
      "File \u001B[0;32m~/Documents/workspace/CSCI-E25/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtypes\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompiler\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtf2xla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxla\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dynamic_update_slice\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KerasVariable\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import keras.utils.np_utils as ku\n",
    "import sklearn.linear_model as sklm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import skimage.feature as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dfdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef587d6",
   "metadata": {},
   "source": [
    "The MNIST dataset is built into Keras with the training and test subsets of images and labels returned in lists. Execute the code in the cell below to load these subsets. \n",
    "\n",
    "> **Note:** The MNIST data contain simple images of hand written digits. These images are properly cropped and have nearly binary light (digit) and dark (background region) areas. No significant adjustment or transformation of these images is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_train_images, train_labels), (raw_test_images, test_labels) = mnist.load_data()\n",
    "print(raw_train_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5d197",
   "metadata": {},
   "source": [
    "> **Exercise 4-1:** It is useful to get a feeling for what this image data really looks like. On a $5 \\times 5$ grid display the first 25 gray-scale training images. Give each image display a title with the label for that image or case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89055a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(15,15))\n",
    "ax = ax.flatten()\n",
    "\n",
    "## Your code goes below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf47bb",
   "metadata": {},
   "source": [
    "> Examine the images and the labels. What problems can you foresee when a machine learning algorithm attempts to learn to classify the digits shown in these images? Your answer need only be 1 to 3 well chosen sentences.             \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c34ec",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee68685",
   "metadata": {},
   "source": [
    "In order to perform machine learning with image data you must transform the data to a structure with all the **features** for each image in one row of a **model matrix**. The linear model can then be written:      \n",
    "\n",
    "$$X b = y$$\n",
    "\n",
    "where:    \n",
    "$X$ is the model matrix with the features values for each image in the rows.    \n",
    "$b$ is the **coefficient vector**, with one coefficient per feature.   \n",
    "$y$ is the vector of the **labels** which encode the categories of the objects in the images.\n",
    "\n",
    "The goal is to estimate the vector of **coefficients**, **parameters** or **weights** to **minimize errors** in the the prediction of the label, $y$, given the model matrix, $X$. In machine learning terminology, we say that the model **learns the weights**, $b$, to minimize the errors.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7aad72",
   "metadata": {},
   "source": [
    "For this example, we will use the values of the pixels as our feature values. This requires the 2-dimensional images be **flattened** into feature vectors. This concept is illustrated in the figure below.   \n",
    "\n",
    "<img src=\"img/FlatteningImge.JPG\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>Flattening an image to a feature vector</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9349d",
   "metadata": {},
   "source": [
    "> **Exercise 4-2:** You will now flatten the the $28 \\times 28$ images to feature vectors. Do the following:    \n",
    "> 1. Print the shape of the training image array, noticing that each image is a 2-dimensional sub-array.   \n",
    "> 2. Flatten the images to an array of 60,000 rows using [numpy.reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html). The image arrays are 8 bit integers. Convert them to floating point in the range $[0.0 - 1.0]$, which will normalize the feature values.    \n",
    "> 3. Print the shape of the flattened image array.   \n",
    "> 4. Apply the same transformations to the test image array and print the shape of the resulting array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d27364",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726561b5",
   "metadata": {},
   "source": [
    "> Examine the dimensions of the flattened arrays. How many features will your model have? Your answer should show a simple numeric calculation.    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272c93b",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648ed82",
   "metadata": {},
   "source": [
    "It can be instructive to look at the feature matrix. Execute the code in the cell below and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_sample(img, nrows=512):\n",
    "    fig, ax = plt.subplots(figsize=(15,9))\n",
    "    p = ax.imshow(img[:nrows,:])\n",
    "    plt.ylabel('Flattened image number')\n",
    "    plt.xlabel('Features')\n",
    "    plt.title('Flattened images')\n",
    "    cb = plt.colorbar(p)\n",
    "    _=cb.set_label('Intensity')\n",
    "\n",
    "print('train_images.shape = ' + str(train_images.shape))\n",
    "plot_feature_sample(train_images)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d690f",
   "metadata": {},
   "source": [
    "> **Exercise 4-3:** Examine the feature columns in the array displayed above. Notice the differing ranges of values in the features. What does this tell you about the scale differences and the problems this might create in model training? Your answer need only be 1 to 3 well chosen sentences.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8efaef",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe6e3f",
   "metadata": {},
   "source": [
    "The significant difference in scale or value range of the features or pixel locations can lead to problems with model generalization. The coefficients computed will be affected by scale which will affect model performance and generalization. For example, consider that a bit of noise in some images presented to the classifier in production can lead to a high probability of erroneous classifications only as a result of scale differences. \n",
    "\n",
    "> **Note:** The behavior of low variance (small scale) features seen in this example may arise from the way the digit images are cropped. There are several approaches we could try here. \n",
    "> 1. Simply filter features with low variance (or small scale). The results may be a feature set with higher information content per feature.         \n",
    "> 2. Transform the features so that they are all at the same scale.      \n",
    "> 3. Once one of the above methods are applied to the features, dimensionality reduction might be useful. For linear models principle compomonent analysis (PCA) will typically be used.   \n",
    "> **In other words, you must expolore and understand the nature of the data before applying any preprocessing step!**        \n",
    "\n",
    "There are a number of methods we could use to deal with the scale problem. In this case, we will try the simplest possible approach, mean-variance scaling. Mean-variance scalling centers, or zero means, each feature. The centered feature values are then devided by the standard deviation, $\\sigma$. For each feature $x$ we can express mean-variance scaling as follows: \n",
    "\n",
    "$$x_{scaled} = \\frac{x - \\bar{x}}{\\sigma(x)}$$\n",
    "\n",
    "Once the mean-varaince scaling is applied all features have zero mean and unit variance.   \n",
    "\n",
    "The code in the cell below follows the usual process for mean-variance scaling:   \n",
    "1. The [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) is instantiated and fit to the traning data.    \n",
    "2. The scaler is apply to the training data.     \n",
    "3. The same scaler is apply to the test data. Note that it is important to use only the scalar learned from the training data to avoid bias.     \n",
    "Execute the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedeafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler().fit(train_images)\n",
    "train_images = scalar.transform(train_images)\n",
    "test_images = scalar.transform(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58389f",
   "metadata": {},
   "source": [
    "## A Linear Model        \n",
    "\n",
    "With the feature arrays prepared, it is time to construct the machine learning model. The code in the cell below defines the linear **logistic regression** model object and fits it to the training data. Here we use the Scikit-Learn [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) function. The model definition has several key aspects:   \n",
    "- L2 regularization with a hyperparameter $C = 10.0$.  \n",
    "- Since there are 10 categories of digits, the multinomial probability distribution is used.    \n",
    "- An efficient solver for the system of linear equations is selected.    \n",
    "\n",
    "The `fit` method with the arguments of the model (feature) matrix and the label vector.  \n",
    "\n",
    "If you are just learning to use Scikit-Learn it is useful to know that the general approach is used for all machine learning models available in the package. A model object is defined including the values of hyperparameters. A fit method is used to compute model parameters or weights using the model matrix and labels as the arguments. The model matrix has the features in the columns, and the cases, in this case flattened images, in the rows. You can find a [Getting Started Guide](https://scikit-learn.org/stable/getting_started.html) in the Scikit-Learn documentation.   \n",
    "\n",
    "The code in the row below constructs the model object and fits a model. You can see that some `l2` regularization is applied with an hyperparameter of 0.1. Since this is a mutli-class classification problem, the `multi_class='multinomial'` argument is used. Now, execute the code in the cell below to create a model object.  \n",
    "\n",
    "> **Note:** Depending on your environment, you may need to change the solver to achieve convergence of the linear model. If you encounter this problem, uncomment the line of code below, and comment out the other line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=0.1\n",
    "LinearClassifier = sklm.LogisticRegression(penalty='l2', C=C, multi_class='multinomial', solver ='newton-cg').fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56e0d9",
   "metadata": {},
   "source": [
    "> **Exercise 4-4:** The logistic regression model computes probabilities each of the categories for each case. You will now investigate an example by the following steps:    \n",
    "> 1. Apply the `predict_proba` method with the test images to the trained classifier method. \n",
    "> 2. Display the first 10 rows of the resulting array of probabilities.    \n",
    "> 3. Sum the probabilities in the array across the classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1edba41",
   "metadata": {},
   "source": [
    "> Answer the following questions in 1 to 3 well chosen sentences:  \n",
    "> 1. Examine the probabilities of the categories for each of the 10 cases. Is there generally a category with the highest probability? Are there cases where another cases has a reasonably high probability?   \n",
    "> 2. Given your answers to the foregoing question, do you expect this classifier to make errors in identifying the categories?   \n",
    "> 3. Are these proper probability distributions, in the sense that they sum to 1.0 for each case?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b3e09",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.    \n",
    "> 2.     \n",
    "> 3.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f72db",
   "metadata": {},
   "source": [
    "While it is useful to understand how the linear model algorithm computes probabilities for the categories of digits, for most applications we really only want to know the most probable category. The algorithm is quite simple; pick the category with the highest probability. Scikit-Learn provides the `predict` method that computes the probabilities are returns the category with the highest probability. Execute the code in the cell below to see an example.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861307a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predictions = LinearClassifier.predict(test_images)\n",
    "class_predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b60bf8",
   "metadata": {},
   "source": [
    "## Evaluate the Model       \n",
    "\n",
    "Now that you have a model and made predictions it is time to evaluate the model. The [Scikit-Learn metrics package](https://scikit-learn.org/stable/modules/model_evaluation.html) contains numerous functions for evaluating different types machine learning models. \n",
    "\n",
    "A widely used metric for evaluating classifiers; the number of correctly classified cases divided by all cases:   \n",
    "\n",
    "$$Accuracy = \\frac{TP}{TP + FP + TN + FN}$$    \n",
    "\n",
    "Where, $TP$ are the true positives, $FP$ are the false positives, and $FN$ are the false negatives. In the **multi-class** case, we count all elements on the diagonal of the confusion matrix as true positive (TP). All cases on the diagonal are corrected classified in one category or another. Thus for computing average accuracy for the multi-class case there are only true positive cases in the numerator of the formula. You can find more details in [this review paper](https://arxiv.org/abs/2008.05756).    \n",
    "\n",
    "In this case we will focus on evaluation of the multi-class classifier. The [sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) computes the accuracy given the actual labels and the predicted values. Execute the code in the cell below and examine the result.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(test_labels, class_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c39f7",
   "metadata": {},
   "source": [
    "The overall accuracy of the model seems reasonably good. However, one must be extremely careful when evaluating any machine learning model. Any single metric can be quite misleading. It is good practice to look at several views of model performance.   \n",
    "\n",
    "The **confusion matrix** can be a powerful tool for evaluating classifiers. The confusion matrix is a 2-dimensional array with the label values on vertical axis and the predicted values on the horizontal axis. The count of correctly classified cases for each category are along the diagonal. Counts or incorrectly classified cases are found off the diagonal. \n",
    "\n",
    "The confusion matrix can be computed and displayed numerically. For large numbers of categories a visualization of the confusion matrix can be useful. By studying the confusion matrix one can identify many problems which would not be apparent from one to two simple metrics. For example, accuracy of a classifier might seem quite high, but it could be misclassifying all members of some category, while doing well with other categories. Only by examination of the confusion matrix can such problems be discovered.     \n",
    "\n",
    "The code in the cell does the following:  \n",
    "1. Computes the multi-class confusion matrix using the [sklearn.metrics.multilabel_confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html?highlight=multilabel_confusion_matrix) function. \n",
    "2. displays the confusion matrix as a Pandas data frame to improve formatting.   \n",
    "2. Displays the log values of the confusion matrix as an image or heat map. The logarithm is used in this case since the off-diagonal terms are quite small compared to the diagonal terms. A 1 is added to all terms to allow computation of the logarithm, adding a small but negligible bias.     \n",
    "\n",
    "Execute the code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c9d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_labels, class_predictions)   \n",
    "print(pd.DataFrame(confusion_matrix))\n",
    "p = plt.imshow(np.log(np.divide(confusion_matrix + 1.0, np.sum(confusion_matrix, axis=1))))\n",
    "cb = plt.colorbar(p)\n",
    "_=cb.set_label('Log count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d1f49",
   "metadata": {},
   "source": [
    "Some other commonly used [performance metrics for classifiers](https://en.wikipedia.org/wiki/Precision_and_recall) are **Precision** and **Recall**. For binary classifiers these metrics are defined:   \n",
    "\n",
    "\\begin{align}\n",
    "Recall &= \\frac{TP}{TP + FN}\\\\\n",
    "Precision &=  \\frac{TP}{TP + FP}\n",
    "\\end{align} \n",
    "\n",
    "You can think of recall as the fraction of positive cases correctly classified, also know as the **sensitivity** of the classifier. The precision or **positive predictive value** is the probability that a positive case can be correctly classified.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee3f3d",
   "metadata": {},
   "source": [
    "The above formulas cannot be directly applied to the multi-class classification case one typically encounters in CV applications. For this situation we compute class-specific precision and recall from the multi-class confusion matrix using [sklearn.metrics.precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) and [sklearn.metrics.recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score). \n",
    "\n",
    "The [class specific precision and recall are calculated from the confusion matrix](https://medium.com/data-science-in-your-pocket/calculating-precision-recall-for-multi-class-classification-9055931ee229#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImEyOWFiYzE5YmUyN2ZiNDE1MWFhNDMxZTk0ZmEzNjgwYWU0NThkYTUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2NzM0ODI2NzQsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwMjAwNzU5NTcyMzgwNjA4Nzk4OSIsImVtYWlsIjoic3RlcGhlbi5lbHN0b25AZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJTdGVwaGVuIEVsc3RvbiIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BRWRGVHA0YnJDbXJFVFpldW9LeGhqWS01WEFVR2hqU3gxRDdQWmpJVE9RaD1zOTYtYyIsImdpdmVuX25hbWUiOiJTdGVwaGVuIiwiZmFtaWx5X25hbWUiOiJFbHN0b24iLCJpYXQiOjE2NzM0ODI5NzQsImV4cCI6MTY3MzQ4NjU3NCwianRpIjoiMzc1ZjNiMTdmMDA4Mjg4NWQ4ZjgyODlhMzNlYmIyMTVkMjg0NmZjNCJ9.RpejRCenuDVzVPk5hAweRUqR4Pc-pJFRigIIW27mHf7Q0HqiO9CvkLJBu5vSvBoERBw2mdnaHpzpBNtJK3lRT8oPn-KgqX-Utol6lSrWLVGuuefsx4cHdz5CR0q6BOvy4CXk4dgeTpca6Psl51UNeuaV73hs-8bvROXzPoJACRxI0ykCNsf5nzcVysSiv5GOjckj0IfovXeh58N0nj5w4msaRvLUvThGDR7Id35M9JZmfKZimYwFSCC7dNn2Feq6KmkfdxU1U77jGRk9ZZfYSJk0kqIAWTOIh4O2flAwjzpX6ywwgLsBoIrIf1CxAG1d5F79zCapLfDCyOTB4JOVKQ), $C$. These metrics are weighted averages over the correctly and incorrectly classified cases for each label category.       \n",
    "- The **class specific precision** for the $i$th class is the number of cases correctly classified divided by the sum of the $i$th row:         \n",
    "$$precision_i = \\frac{C_{i,i}}{\\sum_j C_{i,j}} = \\frac{TP_i}{\\sum_j C_{i,j}}$$\n",
    "- The **class specific recall** for the $j$th class is the number of cases correctly classified divided by the sum of the $j$th column:         \n",
    "$$recall_j = \\frac{C_{j,j}}{\\sum_i C_{i,j}} = \\frac{TP_j}{\\sum_i C_{i,j}}$$\n",
    "\n",
    "The code in the cell below computes and displays the class specific precision and recall for the classifier just constructed. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dfa2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, label_counts = np.unique(test_labels, return_counts=True)\n",
    "class_precision = metrics.precision_score(test_labels, class_predictions, labels=unique_labels, average=None)\n",
    "class_recall = metrics.recall_score(test_labels, class_predictions, labels=unique_labels, average=None)\n",
    "\n",
    "pd.DataFrame({'Class-specific precision':class_precision, 'Class-specific recall':class_recall})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e46765",
   "metadata": {},
   "source": [
    "Given the above class-specific precision and recall values the average values of these metrics can be computed as a weighted sum. The frequency of the test labels $n_i$ need not be the same or balanced. The weight, $w_i = n_i/\\sum_j n_j$ is the decimal fraction of the \n",
    "\n",
    "\\begin{align}   \n",
    "average\\ precision &= \\sum_i w_i * precision_i\\\\ \n",
    "average\\ recall &= \\sum_i w_i * recall_i\\\\ \n",
    "w_i &= \\frac{n_i}{\\sum_j n_j}\n",
    "\\end{align} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469614db",
   "metadata": {},
   "source": [
    "> **Exercise 4-5:** Now you will compute compute and display the average precision and recall scores for the model by creating and executing code in the cell below. \n",
    "> 1. Display the frequency of the unique labels, in the `label_counts` variable computed in the previous cell.   \n",
    "> 2. Compute and display the weighted average of the precision and recall metrics.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376c42d",
   "metadata": {},
   "source": [
    "> Answer the following questions in 1 to 3 well chosen sentences:  \n",
    "> 1. Referring to the class-specific precision and recall computed above, what does the variation in the precision and recall scores of the different digit classes tell you about the performance of the classifier?   \n",
    "> 2. Average precision and recall scores are often used to summarize the performance of multi-class classifiers. In this case, do you think the summary is reasonable, or does it loose too much information? \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605330e",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.     \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a814b",
   "metadata": {},
   "source": [
    "> **Exercise 4-6:** To better understand the source of errors from the classifier model it is useful to examine some details of the erroneously classified cases. You will now do the following:    \n",
    "> 1. Create an index vector of the erroneously classified cases.     \n",
    "> 2. Create a Pandas data frame containing the class probabilities for the first 25 misclassified cases encountered. Hint, it will improve the readability of your display if you round to 3 decimal places.    \n",
    "> 3. Append a column of the corresponding test labels to the data frame.  \n",
    "> 4. Append a column showing the predicted class to the data frame. \n",
    "> 5. Display the data frame.    \n",
    "> 6. Create a a $5 \\times 5$ grid display the first 25 gray-scale erroneously classified images. Give each image display a title with the label for that image and the predicted class. *Hint,* use a large display area.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f509bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes below                                                                                                                                                                                                                     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(15,15))\n",
    "ax = ax.flatten()\n",
    "\n",
    "## Your code goes below\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae41ef",
   "metadata": {},
   "source": [
    "> Examine and compare the images of the misclassified digits to the probabilities of class assignment in the printed table and answer these questions:   \n",
    "> 1. Notice that some of the digits shown are very poorly formed. Find a few such examples and then examine the category probabilities.    \n",
    "> 2. Given the image and the probabilities describe why you think the error might have occurred?     \n",
    "> **End of exercise.**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8057854",
   "metadata": {},
   "source": [
    "> **Answers:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268ed6a",
   "metadata": {},
   "source": [
    "> **Exercise 4-7:** You may well wonder how one should choose the regularization hyperparameter for the model. The process of finding the best model for an application is know as **model selection.** In this case, answer is to perform a **hyperparameter search**. Now, do the following:   \n",
    "> 1. Define a list of hyperparameter candidates, $[0.003, 0.01,0.03,0.1,0.3,1.0]$, the argument C.\n",
    "> 2. Define 2 empty lists for accumulating train and test accuracy values.    \n",
    "> 3. Iterate over the hyperparameter values. For each value fit a Logistic Regression model, compute the class predictions using both train and test data, and then append the train and test accuracy of the model to the associated list.   \n",
    "> 4. Outside the loop compute lists (or arrays) of the train and test error rates for your model as $1.0 - accuracy$.\n",
    "> 5. Display the accuracy and test error rate for each value of C.  \n",
    "> 6. Execute your code, which may take some time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7890)\n",
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313f728",
   "metadata": {},
   "source": [
    "> 6. On a reasonable size plot area, display a graph of the hyperparameter values on the horizontal axis and the train and test errors on the vertical axis. Include a legend on your plot so you can tell which line is the test and which is the train error. *Hint:* Your plot will be easier to interpret if you use a log scale on the horizontal axis using `ax.set_xscale('log')`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41f750",
   "metadata": {},
   "source": [
    "> Examine the graph and answer the following questions:\n",
    "> 1. Given the results, why do you think the hyperparameter used was selected for the model?   \n",
    "> 2. What does divergence between test and training error with increasing hyperparameter value tell you about improving the generalizeability of the model?   \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45c32d",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.         \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004b658",
   "metadata": {},
   "source": [
    "With the hyperparameter search completed we have completed the important process of **model selection**. We can now construct and test an optimal model. The code in the cell below constructs and performs evaluation on a mulit-class logistic regression model using this optimal L2 regularization hyperparameter value. Execute this code and examine the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e39d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.03\n",
    "LinearClassifier = sklm.LogisticRegression(penalty='l2', C=C, multi_class='multinomial', solver ='newton-cg').fit(train_images, train_labels)\n",
    "\n",
    "class_predictions = LinearClassifier.predict(test_images)\n",
    "print('Overall accuracy = ' + str(metrics.accuracy_score(test_labels, class_predictions)))\n",
    "print('Average precision = ' + str(weighted_average(class_precision)))\n",
    "print('Average recall = ' + str(weighted_average(class_recall)))\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(test_labels, class_predictions)   \n",
    "print(pd.DataFrame(confusion_matrix))\n",
    "p = plt.imshow(np.log(np.divide(confusion_matrix + 1.0, np.sum(confusion_matrix, axis=1))))\n",
    "cb = plt.colorbar(p)\n",
    "_=cb.set_label('Log count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e1374",
   "metadata": {},
   "source": [
    "In this case, the results are only slightly different from the model constructed with our initial regularization hyperparameter choice. This is not an unusal situation. One should not expect dramatic improvements solely from hyperparameter optimization.  However, optimizing model hyperparameters can lead to small, but useful, improvements in ML model performance.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe61595",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have investigated some key points of applying ML to CV problems in this lesson: \n",
    "1. The formulation of linear machine learning models. We will apply this formulation to CV problems beyond classification in subsequent lessons.     \n",
    "2. A basic machine learning workflow with the following steps,   \n",
    "   - Data preparation,    \n",
    "   - Data repreresentation for the model,     \n",
    "   - Model construction,    \n",
    "   - model evaluation,    \n",
    "   - and model selection.   \n",
    "3. Formulation of CV features for machine learning. In this case, we used a basic approach of flattening the gray scale pixels of each image into a vector.    \n",
    "4. The relationship between bias, variance and model capacity was explored through the use of regularization. Model selection was done by hyperparameter searching.  \n",
    "5. Theory of binary classifiers was explored briefly.  \n",
    "6. Theory of multi-class classifiers was applied to the digit classification problem. The results are probabilities for each class, with the largest probability selected as the result.    \n",
    "7. Multi-class classifier evaluation was performed.     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2d471",
   "metadata": {},
   "source": [
    "#### Copyright  2018, 2019, 2020, 2021, 2022, 2023, 2024, Stephen F Elston. All rights reserved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700df3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
