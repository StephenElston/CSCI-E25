{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95325261",
   "metadata": {},
   "source": [
    "# CSCI E-25      \n",
    "## Transfer Learning and Data Augmentation  \n",
    "### Steve Elston\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Transfer learning** and **data augmentation** are important approaches to paractical use of deep neural network models for computer vision. You will work with examples of each of these methods.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21297feb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Transfer learning   \n",
    "Transfer learning employes deep neural network models which have been previously trained on large datasets. This pretraining can require enourmous computing resources, as well as massive datasets. These resources are often not available in practice. The dataset available for a particular prblem may be too small, or the computing resources may not be available for the project.  \n",
    "\n",
    "Most deep learning platforms have a module of pre-trained models. You can find an [extensice list of optipns for Keras](https://keras.io/api/applications/). \n",
    "\n",
    "Instead, we can use a model with weights trained previously on other datasets. We stay that we **transfer** the learning from one task **learned** with some traning data to another task with different image characteristics. This process is known as **transfer learning**. \n",
    "\n",
    "In most cases of transfer learning there are two major components of the model, a **backbone network** and a **head**. The pre-trained **backbone** network produces a feature map. A head, placed on the backbone, performs the task-specific learning. Examples of task-specific learning include:   \n",
    "- **Object classification:** Our goal for the exercises in this lesson.  \n",
    "- **Object detection:** Fine the objects in an image.  \n",
    "- **Semantic segmentation:** Detect and label the types of 'things' in an image.  \n",
    "- **Object tracking:** Track how the bjects in an series of images (a video) are moving.   \n",
    "\n",
    "There are several approaches to task-specific traning with transfer learning:     \n",
    "\n",
    "**Frozen backbone network:** The weights of the backbone network are frozen and the resulting feature map is used directly. The weights of the task-specific head are learned using case-specific data. This approach minimizes the amound of traning data requried, since only the weights of the head need be trained. This approach leads to methods known as **few shot training** for a specific case of a task. Accuracy may be sacrificed since the feature map is not optimized for the task.         \n",
    "\n",
    "**Fine tune traning of the backbone network:** Weights of the task specific head are learned using the case-speific data. At the same time, the weights of the backbone network are **fine-tuned** for the task. In many cases, only a few epochs are required for fine tuning. This second approaceh often provides better performance, since the feature map produced by the backbone is more likely to have features specific for the task. However, fine tuning of the backbone network may fail if there is insufficient data to effectively train the additional weights.     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a5007",
   "metadata": {},
   "source": [
    "### Data augmentation     \n",
    "\n",
    "Even with good pre-training, sufficient task-specific data may not be available even to learn the required weights using transfer learning. In such cases one can apply **data augmentation**.  The process of data augmenation creats new traning samples from existing training images. The new image samples are created by **randomly** appling one or more **transformations** to the original image. The label of the transformed image is the same as the original image. Yet, given the randomly chosen transformations applied, the new image will have different characteristics. Further, since the transformations are random in nature, several new samples can be created from the same original image. Thus, augmented data will help tranin models to have better generalization. \n",
    "\n",
    "Deep learning platforms include packages for standard random data augmentation. For example, in the Keras documentation you can find [examples of applying random transformations](https://www.tensorflow.org/tutorials/images/data_augmentation) to augment image data. \n",
    "\n",
    "Examples of random transformations which can be used to augment image traning data include:   \n",
    "1. Random rotation.     \n",
    "2. Random translation along either or both axes.  \n",
    "3. Random cropping of the image followed by resizing to the original size. \n",
    "4. Flipping the transformed image to create a mirror image.   \n",
    "5. Randomly adjusting the contrast of the image.   \n",
    "6. Adding Gaussian or other noise to the image.  \n",
    "7. Independently applying random brightness adjsutments to the histograms of the color channels. \n",
    "8. Randomly down sampling followed by resizing.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704553d2",
   "metadata": {},
   "source": [
    "## Example Notebook   \n",
    "\n",
    "For these exercises you will use the **[Image classification via fine-tuning with Efficientnet](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/)** Keras example notebook. This notebook uses the pre-trained **EfficientnetB0** model, the smallest model in the EfficientNet family introduced by [Tan and Le, 2019](https://arxiv.org/abs/1905.11946). This version of the model is faster to train and faster at inference, but at a sacrifice in accuracy.   \n",
    "\n",
    "To run the notebook you will need a [Google Colabratory account](https://colab.research.google.com/) if you do not already have one. Log into your google account. Start on the [Image classification via fine-tuning with Efficientnet](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/) page. Then, click the `View in Colab` tab to start the notebook in Colab (but do not execute yet!). Alternatively, you may want to run this notebook locally, if you have the resources. Either way, expect long run-times for model training.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b146af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The Dataset   \n",
    "\n",
    "The [Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html) dataset contains over 20,000 images of 120 dog breeds. The goal for a classifier is to learn to classify dog breeds correctly.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b5f90-38aa-439e-85e5-61f30c3591d7",
   "metadata": {},
   "source": [
    "### Modifications to the Notebook   \n",
    "\n",
    "Before you attempt to run this notebook in Colab you must make several changes. These changes will help with model training and evaluation.     \n",
    "\n",
    "1. **Verify the version of TensorFlow.** The note on the Jupyter notebook indicates it should be run on TensorFlow 3. Yet, this does not seems to be necessary. To verify you are using a suitable version of TensorFlow execute the code shown below just after executing the imports. The version of TensorFlow version 2.15 or higher.        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb01dff1",
   "metadata": {},
   "source": [
    "print('TensorFlow version = ' + str(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad44240-9ac4-4ab1-8f88-6fd40875bfcc",
   "metadata": {},
   "source": [
    "2. **Adding a metric for training the model from scratch.** For this notebook we will use multiple metrics to evaluate the models. To start, we will add `top_k_categorical_accuracy` to the compile method. We are using the default of $k=5$. \n",
    "\n",
    "Replance the following line of code."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe7486da-ef69-4cff-9c26-e53b0e385874",
   "metadata": {},
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca556dce-6477-48e3-b0f2-51ce32ffde4e",
   "metadata": {},
   "source": [
    "With the following code. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "24bc28de-dd4f-4ef4-8f47-0c98012c73c4",
   "metadata": {},
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\",  \"top_k_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffbd81-724c-48a2-bd9a-4461a55d4df6",
   "metadata": {},
   "source": [
    "Make sure to print a model summary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9dc4a-207d-4b15-a21d-9cdc8893d1f9",
   "metadata": {},
   "source": [
    "3. **Split cell of of training model from scratch.** You may find it convenient to split the code cell in the Training Model from Scratch section so that the following lines are in a seperate executable cell. By doing this split, you can restart the notebook and instantiate the model without having to wait for the long training time. You will also reduce the number of epochs to 30 to reduce the lengthy learning time. The code in the split cell is shown below. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "aac0e451-fbeb-4203-bdd6-e04254bd89dd",
   "metadata": {},
   "source": [
    "epochs = 30 \n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb7ade-87a3-4264-97fc-5522fdbd4123",
   "metadata": {},
   "source": [
    "> **Note:** Expect training this model with unforzen weights to take significant time. Using a T4 GPU with Google Colab Pro+ this training took over 1 1/2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff811dce",
   "metadata": {},
   "source": [
    "4. **Improving traing plots.** To obtain a better understanding of the learning of the model substitute the code shown below for the code in the cell following the `Training the Model from Scratch` code cell. This code will display both the loss, accuracy and top 5 accuracy for training and evaluation as the model training evolves.         "
   ]
  },
  {
   "cell_type": "raw",
   "id": "59e47c5e",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist(hist):\n",
    "    _,ax = plt.subplots(1,2, figsize = (12,6))\n",
    "    ax[0].plot(hist.history[\"accuracy\"])\n",
    "    ax[0].plot(hist.history[\"val_accuracy\"])\n",
    "    ax[0].plot(hist.history['top_k_categorical_accuracy'])\n",
    "    ax[0].plot(hist.history['val_top_k_categorical_accuracy'])\n",
    "    ax[0].set_title(\"model accuracy\")\n",
    "    ax[0].set_ylabel(\"accuracy\")\n",
    "    ax[0].set_xlabel(\"epoch\")\n",
    "    ax[0].legend([\"train\", \"validation\", \"Top 5 train accuracy\", \"Top 5 validation accuracy\"], loc=\"upper left\")\n",
    "    ax[1].plot(hist.history[\"loss\"])\n",
    "    ax[1].plot(hist.history[\"val_loss\"])\n",
    "    ax[1].set_title(\"model loss\")\n",
    "    ax[1].set_ylabel(\"loss\")\n",
    "    ax[1].set_xlabel(\"epoch\")\n",
    "    ax[1].legend([\"train\", \"validation\"], loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec3fcd-dc6e-4da9-b64e-de6ed48d31ee",
   "metadata": {},
   "source": [
    "5. **Update Transfer learning.** Dispite the claim in the notebook that the training of the model using transfer learning can be trained with an agressive training rate, this does not seem to be the case. Experimentation shows that performance of the model can be iomproved by a slower learning rate and use of weight decay for regularization. The experiments and results shown in the table below were run only on the first 10 epochs to limit experimentation time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb3b5f-48fd-4a31-9792-2fd1b21ae2fd",
   "metadata": {},
   "source": [
    "| Accuracy  | Learning Rate | Weight Decay |    \n",
    "| --------  | ------------- | ------------ |     \n",
    "| .7898     | $10^{-2}$       | None       |      \n",
    "| .7853     | $10^{-2}$      | 0.001       |    \n",
    "| .7931     | $10^{-2}$      | 0.01        |\n",
    "| .8180     | $10^{-3}$      | 0.01        |\n",
    "| .8108     | $10^{-4}$      | 0.01        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de7698-aa14-4f91-a9d0-7ff9a576f88e",
   "metadata": {},
   "source": [
    "To improve the model training, update the line of code defining the optimizer to add weight decay. Then, update the metrics for the compile method for the transfer learning model to include `top_k_categorical_accuracy`.      \n",
    "\n",
    "Replace the lines of code shown below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cff575b-b3a0-49ab-99fd-bb0ef824aeb1",
   "metadata": {},
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
    "model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d806bd-f307-4243-be51-f9b38debb912",
   "metadata": {},
   "source": [
    "Use the code shown below.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "faa331e5-c132-4f03-8b0c-60e43f933b9b",
   "metadata": {},
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4, weight_decay=0.01)\n",
    "model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\",  \"top_k_categorical_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4261fc-68cd-4ca5-b3df-8f7feca7c890",
   "metadata": {},
   "source": [
    "6. **Fitting the transfer learning model.** To reduce the running time the number of epochs is set to 20. Replace the code shown in the cell below.   "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a88b9219-906e-446d-9c53-9bca3dd4b835",
   "metadata": {},
   "source": [
    "model = build_model(num_classes=NUM_CLASSES)\n",
    "\n",
    "epochs = 25  # @param {type: \"slider\", min:8, max:80}\n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e00394-bf5e-48d7-9ae4-6a7194be697a",
   "metadata": {},
   "source": [
    "Use the code shown below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "60e66324-e8ae-418c-8820-1571417cdce8",
   "metadata": {},
   "source": [
    "model = build_model(num_classes=NUM_CLASSES)\n",
    "\n",
    "epochs = 20 \n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc465b84-f99f-4fa4-ac46-b8f86398295b",
   "metadata": {},
   "source": [
    "7. **Evaluate the trained model.** Add another code cell with the following code which preforms these evaluation functions:     \n",
    "- Compute and print the accuracy.\n",
    "- Compute and print the top 5 accuracy.    \n",
    "- Compute the class-specific precision and recall measure.        \n",
    "- Compute and print the weighted mean precision and recall measures.\n",
    "\n",
    "> **Note:** For this code the metrics package from Scikit Learn is used since it is well documented and stable. Keras has only full metric support for binary classifiers. TensorFlow has some support, but the availability and detials are highly version dependent.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca001674-a927-4b26-b1ac-28df2dad1d3b",
   "metadata": {},
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def print_model_performance(test_labels, test_model):\n",
    "    ## Compute predicted labels\n",
    "    predictions = test_model.predict(ds_test, batch_size=1)\n",
    "    predicted = predictions.argmax(axis=1)\n",
    "\n",
    "    k = 5\n",
    "    print('Overall accuracy = ' + str(round(metrics.accuracy_score(test_labels, predicted), 4)))\n",
    "    print('Top 5 accuracy = ' + str(round(metrics.top_k_accuracy_score(test_labels, predictions, k=k),4)))\n",
    "\n",
    "    unique_labels, label_counts = np.unique(test_labels, return_counts=True)\n",
    "    class_precision = metrics.precision_score(test_labels, predicted, labels=unique_labels, average=None)\n",
    "    class_recall = metrics.recall_score(test_labels, predicted, labels=unique_labels, average=None)\n",
    "\n",
    "    sum_label_counts = np.sum(label_counts)\n",
    "    weighted_average = lambda x: round(np.sum(np.divide(x * label_counts, sum_label_counts)), 4)\n",
    "    print('Average precision = ' + str(weighted_average(class_precision)))\n",
    "    print('Average recall = ' + str(weighted_average(class_recall)))\n",
    "    return predicted\n",
    "\n",
    "one_hot_labels = [x for _,x in  ds_test.unbatch().as_numpy_iterator()]\n",
    "test_labels = np.argmax(one_hot_labels, axis=1)\n",
    "predicted = print_model_performance(test_labels, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542f6ce-7369-4a28-b7c4-9f24344e061c",
   "metadata": {},
   "source": [
    "Finally, you will add a cell with the following code to create and display the confusion matrix. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b46d05bf-7081-4bc5-b8bb-8ea932e58608",
   "metadata": {},
   "source": [
    "def plot_confusion_matrix(test_labels, predicted):\n",
    "    confusion_matrix = metrics.confusion_matrix(test_labels, predicted)\n",
    "\n",
    "    plt.figure(figsize = (12,9))\n",
    "    p = plt.imshow(np.log(np.divide(confusion_matrix + 1.0, np.sum(confusion_matrix, axis=1))))\n",
    "    cb = plt.colorbar(p)\n",
    "    _=cb.set_label('Log count')\n",
    "\n",
    "plot_confusion_matrix(test_labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9ad36-b1ef-429d-aeb1-9fbf2279f7a0",
   "metadata": {},
   "source": [
    "8. **Update for the unfrozen weight model fine tuning.** It will make only minimal difference in the model training but a limited improvement can be had by updating the weight decay for training this model. You will replace the code shown below.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c8499-219e-4bfe-b496-df3291ccf296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model):\n",
    "    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "\n",
    "unfreeze_model(model)\n",
    "\n",
    "epochs = 4  # @param {type: \"slider\", min:4, max:10}\n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7906185-916c-43ae-a3f0-b07c14ca6ffc",
   "metadata": {},
   "source": [
    "The replacement code is shown below. There are three changes to notice.   \n",
    "- A `weight_decay` argument is added to the optimizer.\n",
    "- The compile method is updated by adding `top_k_categorical_accuracy`.\n",
    "- The number of epochs is increased to 10.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "382e2605-518b-492c-8798-d3e9842dce47",
   "metadata": {},
   "source": [
    "def unfreeze_model(model):\n",
    "    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-5, weight_decay=0.01)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\",  \"top_k_categorical_accuracy\"]\n",
    "    )\n",
    "\n",
    "\n",
    "unfreeze_model(model)\n",
    "\n",
    "epochs = 10  \n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8aec7-bd3b-446f-84e2-8370e0a03b74",
   "metadata": {},
   "source": [
    "9. **Evaluate the fine tuned model** Add a code cell and paste in the code shown below to better evaluate the results of fine tuning. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "012d7721-cca5-46c4-aad4-ae0a1f87b62f",
   "metadata": {},
   "source": [
    "predicted = print_model_performance(test_labels, model)\n",
    "plot_confusion_matrix(test_labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68757a34",
   "metadata": {},
   "source": [
    "## Steps to submit notebook   \n",
    "\n",
    "For this assignment, you will **submit two notebooks** to canvas. \n",
    "1. This notebook including your answers to the questions below. \n",
    "2. Once you have successfully exectued your notebook in colab you must use `Save a copy in Drive` to save any of your work in your [GoogleDrive account](https://support.google.com/drive/answer/2424384?hl=en&co=GENIE.Platform%3DDesktop). You can then download the notebook from your GoogleDrive account and upload it into Canvas for submission.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962f8e7",
   "metadata": {},
   "source": [
    "## Understanding the Code   \n",
    "\n",
    "Before proceeding, it is worthwhile to understand some key aspects of the code.\n",
    "\n",
    "### Importing the pre-trained model  \n",
    "\n",
    "Under the heading, *Keras implementation of EfficientNet*, code to import and configure [EfficientNetB0](https://keras.io/api/applications/efficientnet/) is described. Examine the code:  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a973446f",
   "metadata": {},
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "model = EfficientNetB0(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9ad4d",
   "metadata": {},
   "source": [
    "In transfer learning the feature map generating convolutional layers are pre-trained. These pre-trained layers constitued a **convolutional backbone**. A new head is added on top of the backbone and trained for the specific task. The argument `include_top=False` creates a model object without the head.     \n",
    "\n",
    "The `weights='imagenet'` argument creates a model object with weights learned by supervised learning with the large [ImageNet dataset](https://www.image-net.org/). These weights for the convolutional backbone layers for the creation of a rich feature maps without additional traning.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f482f3",
   "metadata": {},
   "source": [
    "### Load the dataset and resize the images. \n",
    "\n",
    "The next several code cells import the Stanford Dogs dataset, and resizes the images to $224 \\times 224$ pixels, as required by EfficientNetB0. The Stanford Dogs dataset comes with independently sampled train and test sets.\n",
    "\n",
    "The code in the cell of the *Visualizing Data* subsection displays a sample of the dog images. Notice about the following about these images:   \n",
    "1. The are different crops and angles for the dog in each image.    \n",
    "2. The dog in the images have a variety of scales.  \n",
    "3. Some images have other objects.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77efbb5a",
   "metadata": {},
   "source": [
    "### Data augmentation    \n",
    "\n",
    "Very often only limited task-specific training data is available. In such cases, **augmentting** the training data can be an effictive preprocessing step.  The code in the *Data augmentation* section performs a series of random data augmenation steps. The code in the second cell of this section displays a sample of agmentated verisons of one image.Examine these results.   \n",
    "\n",
    "> **Exercise 7-1:** Answer these questions:   \n",
    "> 1. With about 20000 traning images for 120 classes do you think there is adequate data to train a complex model, keeping in mind the numebr parameters in the model? Is there a need for data augmentation?      \n",
    "> 2. Which augmentation methods are applied to the images?    \n",
    "> 3. Describe some of the ways you can observed the multiple-transformation agumentation manifest in the displayed samples?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fe5be",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.         \n",
    "> 2.         \n",
    "> 3.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a20841",
   "metadata": {},
   "source": [
    "### Training the model     \n",
    "\n",
    "The remainder of the notebook contains code for traning the model by three different approaches. Examine the code and notice the three different traning approaches:    \n",
    "1. **Training from scratch:** In this case no pre-trained weights are used. The model weights are learned from scratch using the training data.     \n",
    "2. **Trainng the head with frozen backbone weight:** In this case, the weights of the classification head of the model are trained, with the backbone weights frozen. In other words, the feature map is created from the pretrained weights. The feature map is used by the classifier head. The classifier uses weights learned from the training data. Notice in the code for the *Transfer learning from pre-trained weights* section of the notebook* the model object is created with the  following line of code, excluding the head.            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "173ffe61",
   "metadata": {},
   "source": [
    "model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04547bd0",
   "metadata": {},
   "source": [
    "3. **Fine tuning weights:** In some caes, it is possible to improve model performance by **fine tuning** the weights of the backbone. The fine tuning can result in a feature map better suited to the feature specific to images used for the particular case. Once the model in the notebook is trained with the fronzen backbone weights, the weights of the trainable layers are made `trainable`, or unfrozen.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "37cb8169",
   "metadata": {},
   "source": [
    "for layer in model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a3cac",
   "metadata": {},
   "source": [
    "## Executing the Code and Examining the Results      \n",
    "\n",
    "You are now ready to execute the code in the notebook. Do so, and allow the code to run to completion.       \n",
    "\n",
    "> **Warning!! Expect slow execution!** It appears that the model is intended to be run using **Tensor Processing Units (TPUs)** rather than GPUs. Using T4 procesing units, each training epoch takes over 2 minutes. High RAM does not appear to be required. You may need a Google cloud storage account as well as a Google Drive account to run this code.  \n",
    "\n",
    "The entire notebook can be executed from the menu at the top from `Runtime -> Run all`. As noted above, execution will take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11cf30",
   "metadata": {},
   "source": [
    "> **Exercise 7-2:** Examine the results of traning from scratch and answer these quesitons:   \n",
    "> 1. Based on the training loss and error rate, is the model continuing to learn over the epochs.     \n",
    "> 2. Based on the training and validation losses and error rates, does the model show generalizatation as the the training epochs progress.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df11871",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.        \n",
    "> 2.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14e869",
   "metadata": {},
   "source": [
    "> **Exercise 7-3:** Next, examine the results from the pretraned backbone (fronzen weight) model trained in the *Transfer learning from pre-trained weights* section of the notebook. Keeping in mind that we have limited the number of training epochs, answer the following questions.     \n",
    "> 1. In terms of traning and test loss and accuracy, describe the progress of the training of the head using the frozen backbone?    \n",
    "> 2. What evidence is there that the model will generalize?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969f4e0",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.        \n",
    "> 2.          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46566204-f15b-4656-9f7a-01bbd19600b8",
   "metadata": {},
   "source": [
    "> **Exercise 7-4:** Examine the accuracies, average precision and average recall computed from the model. What do this figures tell you about the consistncy of the classification performance of the classifier model across the different classes?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd845d30-d8d1-4dbd-b984-c5f41e03a8b5",
   "metadata": {},
   "source": [
    "> **Answer:**          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cdbc8-6c4d-4304-b73b-d0bef099b67f",
   "metadata": {},
   "source": [
    "> **Exercise 7-5:** Examine the heat map of the log confusion matrix. What does the small number of off-diagonal weakly bright spots on this plot tell you about the errors for this classifier model?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a68f3-f521-4b23-8b3d-4825150238f9",
   "metadata": {},
   "source": [
    "> **Answer:**        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b001da51",
   "metadata": {},
   "source": [
    "> **Exercise 7-6:** Examine the results of the fine tuning withe the unfrozen backbone weights and answer these questions:     \n",
    "> 1. Comparing the validation loss and accuracy of the last epochs of the model with frozen backbone weights and the fine tuning with the unfrozen weights. Has the fine tuning improved the overall model performance and why?          \n",
    "> 2. Is there any additional fine-tune learning after the first epoch and why?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443d793",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.                    \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a1261",
   "metadata": {},
   "source": [
    "#### Copyright 2023, 2024, 2025 Stephen F Elston. All rights reserved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea4cba-1d2c-467b-a227-1864acacc6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
