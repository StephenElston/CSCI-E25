{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d7c4bb-d7e4-48e7-b0ea-278b5acb2111",
   "metadata": {},
   "source": [
    "# Introduction to Semantic Segmentation     \n",
    "## Steve Elston     \n",
    "## Introduciton   \n",
    "**Semantic segmentation** is one of three types of image segmentation tasks. The goal of semantic segmentation is to label each of the pixels in an image by category of the 'thing' associated with each pixel. In other words, the labeling associates the semantics of the things in the image with pixel labels.     \n",
    "\n",
    "You will use a convolution neural network with an encoder-decoder atchitecture known as [**DeepLab3+**](https://arxiv.org/abs/1802.02611v3) to perform multi-class semantic segmentation. For this exercise you will use the [**Crowd Instance-level Human Parsing CHIP Dataset**](https://github.com/nikhilroxtomar/Multiclass-Segmentation-on-Crowd-Instance-level-Human-Parsing-CHIP-Dataset-using-UNET) data set to segment images with multiple people in to up to 20 semmantic categories. The categories include face, hair, arms and hands, legs, feet, etc. The dataset contains pairs of an $512 \\times 512 \\times 3$ RGB image and an $512 \\times 512 \\times 1$ segmentation mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfddba-3498-490e-9c46-0c5c2af6787c",
   "metadata": {},
   "source": [
    "## Setup    \n",
    "You will work with the Keras example Jupyter notebook [**Multiclass semantic segmentation using DeepLabV3+**](https://keras.io/examples/vision/deeplabv3_plus/). This notebook uses a DeepLab3+ backbone network to perform multi-class semantic segmentation on the images in the dataset.   \n",
    "\n",
    "It is recommended that you run this notebook in [**Google Colab Pro+**](https://colab.research.google.com/signup) piad subscription. Colab Pro+ allows background execution so that model training is not lost when session ends. For fastest execution the **A100 GPU** is recommended, along with High-RAM. \n",
    "\n",
    "Before you execute the code in the notebook make the following changes. In the interest of limiting development time and complexity of this example we are taking a few short cuts, violating best practices in the process.   \n",
    "- Only one model evaluation metric is used, accuracy. Ideally, we should be using others like mAP.\n",
    "- No early stopping is used. The model simply runs for a fixed number of epochs, even if the result is not optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe937a28-285f-48b2-9e84-1a003c47ec6a",
   "metadata": {},
   "source": [
    "### 1. Initial visualization of dataset\n",
    "\n",
    "The dataset contains pairs of a $512 \\times 512 \\times 3$ RGB image and a $512 \\times 512 \\times 1$ ground truth segmentation mask. This segmentation mask contains the lables for training the multi-class segementation model. To visualize a few of these pairs, create a code cell following the cell that creates the TensorFlow data set and paste in the following code.     "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cd2f35f-bdc5-4ee1-b41c-a0fbca999f74",
   "metadata": {},
   "source": [
    "for i in range(10):\n",
    "  _, ax = plt.subplots(1,2, figsize=(8, 4))\n",
    "  example_image, example_mask = load_data(train_images[i], train_masks[i])\n",
    "  ax[0].imshow(example_image.numpy().astype(np.uint8))\n",
    "  ax[1].imshow(example_mask)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c5433-e8da-4495-977c-14fba5fda2d1",
   "metadata": {},
   "source": [
    "### 2. Training\n",
    "\n",
    "As provided, the model training code in the original Jupyter notebook could use some improvement. Replace the original code with the code shown below. The updated code addresses two issues.     \n",
    "1. **Model overfitting:** The original code resulted in erratic training and validation loss and accuracy curves, indicating that the model was overfit. The updated code addresses this problem in two ways:\n",
    "     - The original learning rate appears to be too high. To address this problem a exponentially decaying learning rate schudule with a lower initial rate is used.      \n",
    "     - The original optimizer specification did not include any weight decay. The updated code includes the `weight_decay` hyperparameter.    \n",
    "2. **Better charts for learning curves:** The updated code places the training and validation curves on the same plot axis so that the viewer gets a better sense of the trajectory of the model learning. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc370ea0-96d2-403f-b616-8af9115db837",
   "metadata": {},
   "source": [
    "epochs = 30\n",
    "learning_rate_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=0.0001, decay_steps=1000, decay_rate=0.9\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate_schedule, weight_decay=0.01),\n",
    "    loss=loss,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\n",
    "\n",
    "def plot_loss(history):\n",
    "    '''Function to plot the loss vs. epoch'''\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(test_loss) + 1))\n",
    "    plt.plot(x, test_loss, color = 'red', label = 'Test loss')\n",
    "    plt.plot(x, train_loss, label = 'train losss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    x = list(range(1, len(history.history['accuracy']) + 1))\n",
    "    plt.plot(x, history.history['accuracy'], label = 'Training accuracy')\n",
    "    plt.plot(x, history.history['val_accuracy'], color = 'red', label = 'Validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Epoch')\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(history)\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898074e-d61e-4dbc-a54b-0d75052dcdb5",
   "metadata": {},
   "source": [
    "### 3. Visualizing errors in the learned masks     \n",
    "One may well wonder about the error between the ground truth segmenation masks and the predicted segmentation masks. The code below does the following:   \n",
    "1. Displays the ground truth mask, the predicted mask and the absolute difference between the ground truth and predicted as images.\n",
    "2. Compute and print an average error between the ground truth mask and the predicted mask. This average error is scaled to a range of $[0,1]$, with 0 being no error and 1 being maximum disagreement.\n",
    "\n",
    "Following the code cell in the Inference on Train Images section, create a new code cell paste in the code shown below.   "
   ]
  },
  {
   "cell_type": "raw",
   "id": "55949749-90e3-4de2-b64b-ed34aa54068e",
   "metadata": {},
   "source": [
    "for i in range(5):\n",
    "  val_image, val_mask = load_data(val_images[i], val_masks[i])\n",
    "  predicted_mask = infer(model, val_image)\n",
    "\n",
    "  _ , ax = plt.subplots(1,3, figsize=(10, 3))\n",
    "  ax[0].imshow(val_mask)\n",
    "  ax[1].imshow(predicted_mask)\n",
    "\n",
    "  predicted_mask_gray = cv2.cvtColor(\n",
    "                                     decode_segmentation_masks(predicted_mask, colormap, 20),\n",
    "                                      cv2.COLOR_BGR2GRAY\n",
    "                                     )\n",
    "  mask_diff = np.abs(np.subtract(val_mask.numpy().reshape((512,512)),\n",
    "                           predicted_mask_gray.reshape((512,512))))\n",
    "  ax[2].imshow(mask_diff)\n",
    "  print('Average error = ', str(np.divide(np.sum(mask_diff), 512*512*255)))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd72e1fb-7149-47fa-bbbe-34024d17ef97",
   "metadata": {},
   "source": [
    "## Exercises   \n",
    "Now that you have made the required updates to the Jupyter notebook, execute the code. Expect the model training to take about 1 hour. \n",
    "Once execution has completed examine the results and answer the questions in the following exercises.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa2e06-c512-4b80-830c-16e13d8cd6ac",
   "metadata": {},
   "source": [
    "**Exercise 13-1:** Examine the images and ground truth masks displayed by the code added to the Creating the Tensor FLow section of the notebook and answer these questions:    \n",
    "1. The semmantic segmentation ground truth mask labels are coded as colors in these plots. Compare the mask labels to the body parts of the people shown in the associated image. Do these ground truth labels have semantic meaning with realtionship to the structure of these humans? Provide an example. \n",
    "2. Are the semantic segmentation categories consistent from image to image and why is this important in training a model?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62504c8-87b7-46df-acae-c08cfad95cdc",
   "metadata": {},
   "source": [
    "**Answers:** \n",
    "1.        \n",
    "2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4312a2-b79b-4503-8c7e-a25c1ff9bc08",
   "metadata": {},
   "source": [
    "> **NoteL** Before proceeding to the next exercise you might want to review the first two sections of [**Multi-Scale Context Aggregation by Dialated Convolutions**](https://arxiv.org/pdf/1511.07122v3) by Yu and  Koltun, 2016.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd94c95-2f93-4e9f-a403-477e1e2b6e1e",
   "metadata": {},
   "source": [
    "**Exercise 13-2:**     \n",
    "1. Examine the ground truth masks. In one or a few brief sentances describe the different scales that you see, stating a few examples.\n",
    "2. Keeping the different scales in the images in mind, explain how the encoder or backbone in the first cell specifing the DeepLab3+ encoder deals with scales. What is the range of scales the ecoder incorporates?\n",
    "3. DeepLab3+ has a fully convolutional architecture. What are the dimensions of the final (output) convolution layer and how do these dimensions correspond to the required output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae268cff-c053-4fcc-83a3-7d40423c7d0b",
   "metadata": {},
   "source": [
    "**Answers:**     \n",
    "1.       \n",
    "2.      \n",
    "3.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feef9dc-a6a7-43f7-b3b0-e7b4016d91b0",
   "metadata": {},
   "source": [
    "**Exercise 13-3:** Examine the fully convolutional architecture of the DeepLab3+ model. Notice the output spatial dimenstions and channel numbers of the DeepLab3+ model output layer. In one or a few short sentances, explian why these are the appropriate spatial dimensions and channel numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1208f8-9715-402f-a40b-db2a34dd60d0",
   "metadata": {},
   "source": [
    "**Answer:**      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463de14f-fc9c-4b9c-96c3-1eb4bfc0e8da",
   "metadata": {},
   "source": [
    "**Exercise 13-4:**  In the Training code block the loss funciton is defined that operatoes on the output of the convolutional model. Answer the following questions.    \n",
    "1. Examine the mask examples and consider the large fraction of total pixels that are background. Further consider the fraction of the total pixels that make up the different semmantic categories. What do these observations tell you about the class imbalnce for this problem? Is this imbalance situation likely to be common of other segmentation problems and why?     \\\n",
    "2. Given your observations about inherent class imbalance, why is the [`keras.losses.SparseCategoricalCrossentropy`](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class) function a good choice for a loss function? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f297c1d-4aa6-4b1d-96fb-5d20fb2b8c70",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "1.    \n",
    "2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8ee14-e47c-4424-a688-b38503828a88",
   "metadata": {},
   "source": [
    "**Exercise 13-5:** In the code cell of the Inference using Colormap Overlay section examine the infer funciton. Keeping in mind the output shape of the fully convolutional (no MLP, or softmax) DeepLab3+ network, in one or a few sentances explain how this function returns the category of each pixel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f932fd4-8e61-4a80-ab9f-03e008486943",
   "metadata": {},
   "source": [
    "**Answer:**      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09511f35-9667-48e2-a116-eb96a3d71e9a",
   "metadata": {},
   "source": [
    "**Exercise 13-6:** Examine the images displayed by the code you added to the Inference on Trained Images section of the notebook. In each row left to right are the ground truth segmentation mask, the predicted mask, and the difference between the ground truth and the prediction. The brighter the color in the righthand image the greater the error. Notice also the average relative error on a $[0,1]$ scale is printed. What can you notice about the size of the high error regions compared to the size of the overall image and what does this tell you about possible problems with class imbalance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52abd2df-9dde-481e-8900-20e3672a852e",
   "metadata": {},
   "source": [
    "**Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13991c82-15b5-4cb9-b62b-fe25c7a866eb",
   "metadata": {},
   "source": [
    "#### Copyright, 2025, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
