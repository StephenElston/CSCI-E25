{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bf68e8",
   "metadata": {},
   "source": [
    "# CSCI E-25      \n",
    "## Image Stitching \n",
    "### Steve Elston\n",
    "\n",
    "## Introduction   \n",
    "\n",
    "This lab demonstrates some basic principles of image stitching. The focus here is on transformation of images to a common image plane to form a mosaic. Only planar projective transforms are employed. No blending algorithms are applied.   \n",
    "\n",
    "This lab is intended to be short and simple. The exercises are all conceptual. No coding is required.  \n",
    "\n",
    "You can find an example which includes more sophisticated methods of blending images in this [example](https://github.com/scikit-image/skimage-tutorials/blob/main/lectures/solutions/adv3_panorama-stitching-solution.ipynb), or in the [Scikit Image User Guide](https://scikit-image.org/docs/stable/auto_examples/registration/plot_stitching.html?highlight=stitching).   \n",
    "\n",
    "As a first step, execute the code in the cell below to import the packages you will need.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062281e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage \n",
    "from skimage import data\n",
    "from skimage.filters.rank import equalize\n",
    "import skimage.filters as skfilters\n",
    "import skimage.morphology as morphology\n",
    "import skimage.transform as transform\n",
    "import skimage.feature as feature\n",
    "import skimage.measure as measure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import exposure\n",
    "from PIL import Image\n",
    "from skimage.feature import match_descriptors\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdd8cb-9082-4f58-9056-f09937325231",
   "metadata": {},
   "source": [
    "> **Note:** For a more comprehensive example of image stitching with Scikit Image see [this example](https://github.com/scikit-image/skimage-tutorials/blob/main/lectures/solutions/adv3_panorama-stitching-solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed65b747",
   "metadata": {},
   "source": [
    "## Load and Prepare the Images\n",
    "\n",
    "You will now load and prepare images used to create a mosaic. Execute the code in the cell below to load the image and convert it to gray-scale. \n",
    "\n",
    "> **Note:** You can reproduce the examples in the slide deck if you run this notebook with the `ButeScene3.JPG` image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf641b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grayscale(img, h=8):\n",
    "    plt.figure(figsize=(h, h))\n",
    "    _=plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    _=plt.axis('off');\n",
    "    \n",
    "bute_scene = Image.open('../datafiles/ButeScene.JPG')\n",
    "bute_scene = np.array(bute_scene)\n",
    "print(bute_scene.shape)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(bute_scene)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "bute_scene = rgb2gray(bute_scene)\n",
    "bute_scene = exposure.equalize_adapthist(bute_scene)\n",
    "plot_grayscale(bute_scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6dcb81",
   "metadata": {},
   "source": [
    "The code in the cell below splits the original image into two overlapping images. One of the images is then translated, rotated and scaled. In the remainder of this lab, these two images will be translated an a mosaic created which should resemble the original image.   \n",
    "\n",
    "Execute the code in the cell below to perform the following operations:  \n",
    "1. The images are cropped by opposite sift factors and then resized. The cropping retains an overlap area between the two images. \n",
    "2. A similarity transformation is applied to the right hand image. The transformation matrix is displayed.   \n",
    "3. The images are displayed side by side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86563bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pair(left, right):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(16, 24))\n",
    "    ax = ax.flatten()\n",
    "    ax[0].imshow(left, cmap=plt.get_cmap('gray'))\n",
    "    ax[0].axis('off');\n",
    "    ax[1].imshow(right, cmap=plt.get_cmap('gray'))\n",
    "    ax[1].axis('off');\n",
    "    plt.show\n",
    "    return ax\n",
    "\n",
    "shift = 1000\n",
    "img_dimension = 256\n",
    "dimension_tuple = (img_dimension,img_dimension)\n",
    "bute_left = transform.resize(exposure.equalize_adapthist(np.copy(bute_scene[:,:bute_scene.shape[1] - shift])), (dimension_tuple))\n",
    "bute_right = transform.resize(exposure.equalize_adapthist(np.copy(bute_scene[:,shift:])), (dimension_tuple))\n",
    "\n",
    "\n",
    "theta = math.pi/16\n",
    "#theta = 0.0\n",
    "scale = 1.1\n",
    "#scale = 1.0\n",
    "translation = [0,-64]\n",
    "#translation = [0,0]\n",
    "transform_matrix = transform.SimilarityTransform(rotation=theta, translation=translation, scale=scale)\n",
    "print(transform_matrix)\n",
    "bute_right = transform.warp(bute_right, transform_matrix)\n",
    "\n",
    "_=plot_pair(bute_left, bute_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a04bd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81fbcf",
   "metadata": {},
   "source": [
    "## Search for best interest point matches\n",
    "\n",
    "The images must be transformed to a common image plane in order to form a mosaic. The transformation matrix is computed by solving a least squares problem using interest points with descriptors matching between the two images.       \n",
    "\n",
    "There is, however, a complication with using planar transformations with images, such as the ones in our example. Strictly speaking, planar transformations are an approximation which is strictly valid only for distant objects. For objects near the camera center, planar transformations can introduce significant distortion.    \n",
    "\n",
    "The example images have vegetation in the foreground. The vegetation has many potential interest points. Having interest points in the foreground creates problems when fitting and applying planar transformations. Fortunately, like for most images, foreground objects are at the bottom of the image. Therefore we cut off the bottom of both images when computing interest points and descriptors. \n",
    "\n",
    "To find interest points and their descriptor we will use [skimage.features.ORB](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.ORB) method. ORB combines several important steps for finding matching descriptors between two images:     \n",
    "1. Finds interest points and descriptors at multiple scales using a Laplacian pyramid.   \n",
    "2. Finds interest points using the [FAST algorithm](https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test), an efficient and robust detector.    \n",
    "3. Creases rotationally invariant descriptors with the rotated [BREIF algorithm](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_brief.html).   \n",
    "4. Provides an efficient graph search algorithm for finding matching descriptors.      \n",
    "\n",
    "Execute the code in the cell below which performs the following operations for each image:    \n",
    "1. Creates an ORB object with the desired hyperparameters.  \n",
    "2. Applies the `detect_and_extract` function to the image. This function adds attributes of the the coordinates of the interest points (keypoints) along with their descriptors to the ORB object.\n",
    "3. Extract the descriptor and keypoint location attributes from the ORB object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10f730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 180\n",
    "downscale=2.0\n",
    "n_keypoints=1200\n",
    "fast_threshold=0.1\n",
    "harris_k=0.4\n",
    "\n",
    "orb_left = feature.ORB(downscale=downscale, n_keypoints=n_keypoints, \n",
    "                       fast_threshold=fast_threshold, harris_k=harris_k) \n",
    "temp = bute_left[:cutoff,:]\n",
    "orb_left.detect_and_extract(temp)\n",
    "orb_left_descriptiors = orb_left.descriptors\n",
    "orb_left_keypoints = orb_left.keypoints\n",
    "\n",
    "orb_right = feature.ORB(downscale=downscale, n_keypoints=n_keypoints, \n",
    "                       fast_threshold=fast_threshold, harris_k=harris_k) \n",
    "temp = bute_right[:cutoff,:]\n",
    "orb_right.detect_and_extract(temp)\n",
    "orb_right_descriptiors  = orb_right.descriptors\n",
    "orb_right_keypoints = orb_right.keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e99f0",
   "metadata": {},
   "source": [
    "To check on the effect of truncating the bottom of the right hand image, execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grayscale(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6854755",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to find the matching descriptors between the two images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5623da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = feature.match_descriptors(orb_left_descriptiors, orb_right_descriptiors, cross_check=True)\n",
    "print('Shape of the match array = ' + str(matches.shape))\n",
    "print('\\nHead of the match array')\n",
    "print(matches[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa458f",
   "metadata": {},
   "source": [
    "There are 195 matching descriptors between the two images. The matches are represented by a two column numpy array. The first column indicating the descriptor index of the left image and the second column indicating the descriptor index for the right image.   \n",
    "\n",
    "Now, plot lines between the matching keypoints by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdab727",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 8))\n",
    "\n",
    "feature.plot_matched_features(\n",
    "    bute_left,\n",
    "    bute_right,\n",
    "    keypoints0=orb_left_keypoints,\n",
    "    keypoints1=orb_right_keypoints,\n",
    "    matches=matches,\n",
    "    ax=ax,\n",
    "    only_matches=False\n",
    ")\n",
    "\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Left side image VS Right side image:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ac6ce",
   "metadata": {},
   "source": [
    "> **Exercise 10-1:** Answer the following questions in one or two sentences:\n",
    "> 1. Describe the evidence you see for outliers in these matches. \n",
    "> 2. Ignoring the outliers, examine some of the matching descriptor keypoints. Do you think these matches are reasonable given features on the two images and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead30a0",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.      \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95694e29",
   "metadata": {},
   "source": [
    "## Robust Transformation Fitting\n",
    "\n",
    "With the keypoints and matched it is now time to solve a robust least squares problem using the the [skimage.measure.ransac](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.ransac) function. To do so, execute the code in the cell below which performs the following operations: \n",
    "1. Creates arrays with the coordinates of the matching keypoints for the left and right hand images.    \n",
    "2. Creates a robust fit to a similarity transform model using the RANSAC algorithm. The function returns both a robustly computed transform model object and a list of logicals indicating if the match was considered an inlier.    \n",
    "3. Prints the results of the RANSAC fitting process.   \n",
    "4. Displays the images showing the inlier keypoint matches.   \n",
    "\n",
    "> **Note 1:** You will need to have at least 50 inliers to get a good fit for the transform matrix. While in principle, only 7 inliers are needed to solve the for the degrees of freedom of the transform, such a small number of inliers often leads to an unstatisfactory solution. Since the RANSAC algorithm is stochastic the number of inliers found can be quite variable. If the algorithm does not at first find sufficient inliers, you run the code below again.      \n",
    "> **Note 2:** The RANSAC algorithm is sensitve to the choices of the hyperparameters `min_samples` and `residual_threshold`. You may need or want to experiment with different values of these hyperparameters. See the [skimage.measure.ransac](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.ransac) documentation for details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef4d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_keypoints = orb_left_keypoints[matches[:, 0]][:, ::-1]\n",
    "right_keypoints = orb_right_keypoints[matches[:, 1]][:, ::-1]\n",
    "\n",
    "np.random.seed(22344)\n",
    "model_robust, inliers = measure.ransac((left_keypoints, right_keypoints), transform.SimilarityTransform,\n",
    "                                   min_samples=10, residual_threshold=10.0, max_trials=5000)\n",
    "\n",
    "print('Inlier logical array')\n",
    "print(inliers)\n",
    "print('\\nNumber of inliers = ' + str(np.sum(inliers)))\n",
    "print('\\nRobust transform model')\n",
    "print(model_robust)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "feature.plot_matches(ax, bute_left, bute_right, orb_left_keypoints, orb_right_keypoints, matches[inliers])\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8ec7f",
   "metadata": {},
   "source": [
    "> **Exercise 10-2:** Answer the following questions in one or two sentences:   \n",
    "> 1. Have a sufficient number of inliers been found to fit a similarity transform model and why? \n",
    "> 2. Consider the geometry of the location of the inlier keypoints with respect to the extent of the image. Do you think the spacing of the inlier matches is likely to create errors in accurately determining the transform matrix? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5b371",
   "metadata": {},
   "source": [
    "> **Answers:**  \n",
    "> 1.        \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15001f91",
   "metadata": {},
   "source": [
    "## Apply the Transform to Create the Mosaic\n",
    "\n",
    "With a robust transformation matrix computed you will now use this transformation to create a mosaic of the two images. As the first step, the corners of the warped images need to be determined. These warped corners define the corners of the common image plan onto which the image mosaic is constructed. The comments in the code below explain the steps used. Execute this code and examine the results.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the corners of the image in a Numpy array\n",
    "r, c = bute_left.shape[:2]\n",
    "corners = np.array([[0, 0],\n",
    "                    [0, r],\n",
    "                    [c, 0],\n",
    "                    [c, r]])\n",
    "\n",
    "print('Corners')\n",
    "print(corners)\n",
    "\n",
    "## Find the corner points in the warped coordinate system  \n",
    "warped_corners = model_robust(corners)\n",
    "\n",
    "print('\\nWarped corners')\n",
    "print(warped_corners)\n",
    "\n",
    "# Find the extents of both the reference image and the warped\n",
    "# target image\n",
    "corners_all = np.vstack((warped_corners, corners))\n",
    "\n",
    "# The overall output shape will be max - min\n",
    "corner_min = np.min(corners_all, axis=0)\n",
    "corner_max = np.max(corners_all, axis=0)\n",
    "output_shape = np.ceil((corner_max - corner_min)[::-1]).astype(int)\n",
    "print('\\noutput shape of mosaic = ' + str(output_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87739384",
   "metadata": {},
   "source": [
    "We have now established the corners of the warped images on the projective image plane. We can use these to compute the offset (translation) required to map the images onto the new image plane.     \n",
    "\n",
    "The right hand image only requires only translation to the image plane. Execute the code in the cell below to perform the following operations:      \n",
    "1. The offset between the two images is computed.   \n",
    "2. The image is warped (translated) to the position on the image plane.   \n",
    "3. A mask is created for the background around the image. \n",
    "4. The warped (translated) image has the background set to 0.    \n",
    "5. The result is plotted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135df644",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Be aware that warp takes the inverse mapping as an input1\n",
    "offset = transform.SimilarityTransform(translation = -corner_min)\n",
    "\n",
    "## Translate the image\n",
    "bute_right_warped = transform.warp(bute_right, offset.inverse, order=3, output_shape=output_shape, cval=-1)\n",
    "bute_right_mask = (bute_right_warped != -1)  # Mask == 1 inside image\n",
    "bute_right_warped[~bute_right_mask] = 0      # Return background values to 0\n",
    "\n",
    "plot_grayscale(bute_right_warped) #[~bute_right_mask] = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebc216",
   "metadata": {},
   "source": [
    "THe same steps are applied to the left hand image. The difference being that the offset is added as a transformation matrix and the inverse is applied. The inverse is used since we are mapping the target image.  \n",
    "\n",
    "Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_left = (model_robust + offset).inverse\n",
    "bute_left_warped = transform.warp(bute_left, transform_left, order=3, output_shape=output_shape, cval=-1)\n",
    "\n",
    "bute_left_mask = (bute_left_warped != -1)  # Mask == 1 inside image\n",
    "bute_left_warped[~bute_left_mask] = 0      # Return background values to 0\n",
    "\n",
    "plot_grayscale(bute_left_warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd480b91",
   "metadata": {},
   "source": [
    "It is now time to create a basic mosaic. Execute the code in the cell below to perform the following operations:    \n",
    "1. A mask for the mosaic of the images is created.  \n",
    "2. The warped images are added together and normalized.   \n",
    "3. Plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_mask = (bute_left_mask * 1.0 + bute_right_mask)  # Multiply by 1.0 for bool -> float conversion\n",
    "\n",
    "images_merged_normalized = (bute_left_warped + bute_right_warped)/ np.maximum(overlap_mask, 1)\n",
    "plot_grayscale(images_merged_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864343f",
   "metadata": {},
   "source": [
    "> **Exercise 10-3:** This mosaic has some artifacts. Answer the following questions in one or a few sentences.  \n",
    "> 1. What two obvious artifacts from the stitching of the iamges can you observe. and what does this tell you about error in estimating the transformation parameters?     \n",
    "> 2. Carefully examine the pixel level alignment between the images, particually in the foreground vs. background. How can the use of a planar transformation explaine this alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8d6f4",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.       \n",
    "> 2.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1ed0e",
   "metadata": {},
   "source": [
    "#### Copyright 2022, 2023, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310186c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
