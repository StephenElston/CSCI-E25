{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b382859",
   "metadata": {},
   "source": [
    "\n",
    "# CSCI E-25\n",
    "## Extracting Features From Images\n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73097e",
   "metadata": {},
   "source": [
    "Feature extraction from images is a fundamental aspect of computer vision. Image features must be extracted for many computer vision methods. Just a few of these are:    \n",
    "1. Stitching images together.   \n",
    "2. Locating and identifying objects in images.    \n",
    "3. Models of motion from a series of images.     \n",
    "4. Stereo vision and depth estimation.    \n",
    "\n",
    "In these exercises we focus on classical methods for extracting features from images. In state-of-the-art practice features are created using deep neural networks. Will address learning features with deep neural networks extensively latter. Rest assured that some background in classical methods will help you understand and appreciate the deep learning methods.     \n",
    "\n",
    "To get started, execute the code in the cell below to import the packages you will need for the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93747b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage \n",
    "from skimage import data\n",
    "from skimage.filters.rank import equalize, entropy\n",
    "import skimage.filters as skfilters\n",
    "import skimage.feature as feature\n",
    "import skimage.segmentation as segmentation\n",
    "import skimage.morphology as morphology\n",
    "import skimage.transform as transform\n",
    "import skimage.util as util\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfc9b9",
   "metadata": {},
   "source": [
    "## Preparing the Image     \n",
    "\n",
    "Regardless of the methods used an image must be prepared for feature extraction. Often many steps of correcting and filtering the image are required for proper feature extraction. An important step is the adjustment of the spectrum of the image. We need the spectrum to be as wide as possible. We call the transformation  **pre-whitening**. The broad spectrum is considered white light has all spectral components equally represented. We have already discussed the process of whitening the spectrum of images, contrast improvement. An image with high contrast has a broad spectrum.              \n",
    "\n",
    "The cat image requires little preprocessing, beyond pre-whitening. Execute the code in the cell below to load and prepare the cat image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fd359",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_grayscale(img, title, h=8):\n",
    "    plt.figure(figsize=(h, h))\n",
    "    _=plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    _=plt.title(title)\n",
    "\n",
    "cat_image = rgb2gray(data.cat())\n",
    "plot_grayscale(cat_image, 'Image size = ' + str(cat_image.shape))\n",
    "\n",
    "cat_grayscale_equalized = exposure.equalize_hist(cat_image.flatten()).reshape(cat_image.shape)\n",
    "plot_grayscale(cat_grayscale_equalized, 'Equlized image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02f47c",
   "metadata": {},
   "source": [
    "> **Important note!** Unless otherwise stated, the `cat_grayscale_equalized` image should be used for subsequent exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381dc6c2",
   "metadata": {},
   "source": [
    "## Edge Detection  \n",
    "\n",
    "**Edges** are a fundamental feature of am image. Edges are characterized by rapid changes in the intensity and are therefore associated with **high gradients**. The rapid change in intensity levels also means that the spectrum around an edge will have high-frequency components. \n",
    "\n",
    "We can take advantage of the broad spectrum at edge features to create an edge detector algorithm. Recall that a Gaussian filter acts is low-pass, reducing the high frequency components. If we take the difference of the Gaussian filters with different bandwidths we can detect edges.              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c210823",
   "metadata": {},
   "source": [
    "> **Exercise 3.1:** To gain a bit of understanding from a simple case you will create a simple one dimensional example of an edge detector. In this case, you will apply a convolutional gradient operator to a square wave function that is first positive and then negative. Aficionados of obscure functions will notice this is a Harr basis function (Harr, 1909). You will find and display the changes in gradient of this function by the following steps:   \n",
    "> 1. Create the 1-d convolution operator as a Numpy array: $[0.5,-1.0,0.5]$.\n",
    "> 2. Convolve the `series` with the operator using  the [numpy.convolve](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html) function. Make sure to set the `mode='same'` argument so the resulting series is the same length as the original.  \n",
    "> 3. Plot the result with the `plot_conv` function provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6130cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv(series, conv, span):\n",
    "    x = list(range(series.shape[0]))\n",
    "    plt.plot(x, series, label = 'Original series')\n",
    "    plt.plot(x, conv, color = 'red', label = 'Convolution result')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Series of raw values and convolution')\n",
    "\n",
    "series = np.concatenate((np.zeros((20,)), np.zeros((5,)) + 1.0, np.zeros((5,)) - 1.0, np.zeros((20,))))    \n",
    "    \n",
    "##  Your code goes here    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592895b1",
   "metadata": {},
   "source": [
    "> Examine the plot and answer the following questions:   \n",
    "> 1. Why are the 3-point convolution operator values correct in terms of sign, and normalization?   \n",
    "> 2. Does the magnitude of the resulting series correctly represent the gradient of the original series, and why?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f26e8",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1. \n",
    "> 2.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ee29e",
   "metadata": {},
   "source": [
    "> **Exercise 3-2:** You will now create and test an edge detector using the difference of Gaussian filtered images. Starting with the equalized gray-scale image, perform the following steps:\n",
    "> 1. Define a first vector of values of $\\sigma_1 = [1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]$ and a second value of $\\sigma_2 = 0.5$. \n",
    "> 2. For each value of the $\\sigma_1$ compute an image which is the difference between the Gaussian filtered images computed with $\\sigma_1$ and $\\sigma_2$.\n",
    "> 3. Display each resulting image, with the value of $\\sigma_1$ in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa52666",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigma1 = [1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]\n",
    "sigma2 = 0.5\n",
    "##  Your code goes here  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3577d",
   "metadata": {},
   "source": [
    "> Answer the following questions:\n",
    "> 1. How does the gradient measurement of the images change as the value of $\\sigma_1$ changes and what does this tell you about the scale of features in the image?    \n",
    "> 2. Given the bandwidths of these Gaussian filters compared to the filter with $\\sigma = 0.5$, do the changes of gradient measured make sense and why?     \n",
    "> 3. Does it appear that the difference of Gaussian filters has detected high gradients with different orientations on the image, and why?  \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77792b5",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1. \n",
    "> 2.  \n",
    "> 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd45407",
   "metadata": {},
   "source": [
    "> **Exercise 3-3:** One approach to finding edges is to look for areas of maximum gradient in an image. The difference of Gaussian filters has a strong response to large gradients in image intensity that define edges. In many cases a boolean indicator of an edge is required. To create such an edge indicator from the difference of Gaussian filtered images (equalized gray-scale) with $\\sigma_1 = 1.0$ and $\\sigma_2 = 0.5$ perform the following steps:    \n",
    "> 1 Use the [skimage.filters.threshold_otsu](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.threshold_otsu) function to compute a threshold value for the gradient.       \n",
    "> 2. Apply the threshold and create an image of boolean type.   \n",
    "> 3. Display the boolean image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77394e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s1=16.0\n",
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401f4be",
   "metadata": {},
   "source": [
    "> Examine the results and answer the following questions:     \n",
    "> 1. Has the binarization process captured the edges at all possible orientations, and why?     \n",
    "> 2. Based on your observation, do you think the edge features are sufficient to identify the animal in the image as a cat within reasonable certainty?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff248b",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.    \n",
    "> 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d6154",
   "metadata": {},
   "source": [
    "Now that you have a bit of experience with edge detection, we will explore a few of the many commonly used edge detection algorithms. As you proceed notice how different algorithms produce different results. \n",
    "\n",
    "> **Exercise 3-4:** The Sobel filter is another form of edge detector. You will now apply the [skimage.filters.sobel](https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.sobel) function to the equalized gray-scale cat image. In this case you will simply compute the norm and not the directions. Then, display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7947f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  Your code goes here  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4b5be",
   "metadata": {},
   "source": [
    "> 3. Next compute the binary image use the [skimage.filters.threshold_otsu](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.threshold_otsu) function to find a threshold, apply the threshold found and display the result. **Hint:** Notice that you can biniarize the image by taking pixels greater than or equal to or less than or equal to the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Your code goes here  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c69355",
   "metadata": {},
   "source": [
    "> Compare the edges found with the Sobel filter with those found by the difference of Gaussian filters. Answer the following questions:   \n",
    "> 1. Which aspects of the edges found by the two methods are substantially similar? \n",
    "> 2. How do the edge features detected by the two methods differ, in particular in terms of texture, continuity, compactness, etc? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7915a8",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.  \n",
    "> 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16868c",
   "metadata": {},
   "source": [
    "## Corner Detection    \n",
    "\n",
    "Along with edges, corners are another fundamental feature of images. Detection of corners is fundamentally more difficult than edges:   \n",
    "1. Corners are multi-dimensional so a require more than just a first order gradient for detection.     \n",
    "2. Corers are two-dimensional and have orientation.     \n",
    "3. The angel forming the corner is a fundamental characteristic and changing the angel changes the characteristic of the corner.     \n",
    "As a result of these fundamental characteristics, corner detectors must be based on metrics of multi-dimensional changes in intensity. As an example, the **Sobel** edge detector is a $2 \\times 2$ array of all possible second partial derivatives. This formulation allows us to determine both the presence and the orientation (direction) of corners.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077cb014",
   "metadata": {},
   "source": [
    "> **Exercise 3-5:** The Harris corner detector is one of the many widely use algorithms to detect corner features in images. The detector finds a set of x-y coordinates for each corner that meets a threshold. Apply the [skimage.feature.corner_harris](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.corner_harris) function to the equalized gray scale cat image by the following steps:\n",
    "> 1. Apply the Harris corner detector to the equalized gray-scale image.\n",
    "> 2. Use a for loop to iterate over values of the `min_distance` argument, $[1,3,5,9,12,20]$ and then perform tee steps below.       \n",
    "> 3. Use the [skimage.features.corner_peaks](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.corner_peaks) to filter the corner features using the `min_distance` value. \n",
    "> 4. Print the minimum distance and the number of corner features the filtering.\n",
    "> 5. Side by side, plot the gray-scale and the gradient image computed with the difference of Gaussian filtered images with $\\sigma_1 = 2.0$ and $\\sigma_2 = 0.5$. Superimpose on these images corners detected using a '+' marker for the corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cfdd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777a657",
   "metadata": {},
   "source": [
    "> Examine the results and images and answer these questions:    \n",
    "> 1. How can you describe the change in the number of corner features as the minimum distance is increased, and why?  \n",
    "> 2. Do the features that remain as the minimum distance increases appear more robust (or obviously corners to the eye) given the gradient computed with the difference of Gaussians?   \n",
    "> 3. Consider a trade-off between using a large set of features which may better represent a specific image vs. a sparser but more robust set of features. How might you decide how to optimally filter the features?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7a976",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.    \n",
    "> 2. \n",
    "> 3.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14c9d8",
   "metadata": {},
   "source": [
    "### Eigenvalues and Corner Detection     \n",
    "\n",
    "Now that you have worked with a corner detection algorithm, you will create your own using eigenvalues. This algorithm is a simplified version of the well-known SIFT algorithm. The algorithm uses the ratio of the eigenvalues to find corners with changes in gradient. The steps of this algorithm are:    \n",
    "1. The Hessian for each pixel in the image is computed using the operator span specified, $\\sigma$.   \n",
    "2. The eigenvalues of the Hessians are computed.  \n",
    "3. The ratio of the largest to the second eigenvalue is computed.   \n",
    "4. The eigenvalue ratio is thresholded to reduce the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183aa088",
   "metadata": {},
   "source": [
    "> **Exercise 3-6:** You will now implement the corner detection and examine the results by the following steps:   \n",
    "> 1. Compute the Hessian of the equalized gray-scale cat image using [skimage.feature.hessian_matrix](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.hessian_matrix), iterated over values of $\\sigma = [0.2,0.5,1.0,2.0,4.0]$.    \n",
    "> 2. For each Hessian compute the eigenvalues using [skimage.feature.hessian_matrix_eigvals](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.hessian_matrix_eigvals).    \n",
    "> 3. Compute the ratio of the first and second eigenvalues. These values can be found in the first (largest) and second elements of the list returned by the `hessina_matrix_eigvals` function.   \n",
    "> 4. Apply a threshold of $0.01$ to the largest largest eigenvalues. If the larges eigenvalue is less than the threshold set the eigenvalue ration to $10^{-6}$.   \n",
    "> 5. Display images for the components of the Hessian, $[HRR, HRC, HCC]$, along with the image of the thresholded eigenvalue ratio. Make sure to label all images with the operator span and the Hessian components.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2846bca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigma_list = [0.2,0.5,1.0,2.0,4.0]\n",
    "threshold = 0.01\n",
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01569589",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> 1. How can you describe the how the components of the Hessian change as the span of the Hessian operator, $\\sigma$, changes and why?\n",
    "> 2. What differences can you see between the components of the Hessian and why?   \n",
    "> 3. What differences do you see in the feature image as the span of the Hessian operator, $\\sigma$, changes and why?\n",
    "> **End of exercise**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245d640",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.    \n",
    "> 2.     \n",
    "> 3.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffabe9",
   "metadata": {},
   "source": [
    "## Interest Point Descriptors   \n",
    "\n",
    "In many computer vision applications, including image stitching, stereo vision, and flow (motion tracking), require the unique identification of interest points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9abe38",
   "metadata": {},
   "source": [
    "### The HOG Algorithm\n",
    "\n",
    "The orientation of corners can be used as part of a representation or feature map of an image. As with corner detection, a great many algorithms for determining corner orientation have been developed. Here, we will only work with the **HOG** or **histograms of oriented gradients** algorithm. The algorithm finds the directional bin with maximum value for a histogram of the gradients over patches of the image.  \n",
    "\n",
    "The [skimage.feature.hog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.hog) function returns a list. The first element of the list is a vector of the magnitude of the gradients. The second element of the list is a $2 \\times 2$ array of orientation vectors. When a visualization is performed, the long axis of the markers shows the orientation and the width of the marker indicates the magnitude of the gradient.      \n",
    "\n",
    "> **Exercise 3-7:** You will now apply the HOG algorithm to the equalized gray-scale cat image. You will compare the results of using a different number of pixels per cell to estimate the histogram of 9 orientations by the following steps:\n",
    "> 1. Iterate over tuples for the `pixels_per_cell` argument with dimensions, $[(4,4), (8,8), (16,16), (32,32)]$ for the convolutional operator, and for each tuple do the rest of these steps.\n",
    "> 2. Compute the gradient orientations using the `pixels_per_cell` argument and with `visualize=True`.\n",
    "> 3. Print the pixels per cell tuple, with the dimensions of the operator and the number of HOGs returned by the function in the title of each image. The number of HOGs is in the first element of the list returned by the `hog` function.    \n",
    "> 4. Display the gray-scale image of the corner orientations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d78ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71e71b",
   "metadata": {},
   "source": [
    "> Examine the results and answer the following questions:       \n",
    "> 1. How has the density of the corner direction features changes as the pixels per cell dimensionality increases and why?      \n",
    "> 2. How does the magnitude of the gradient change as the pixels per cell and scale increases and does this make sense given the algorithm and why?     \n",
    "> 3. How does the smoothness and consistency of the gradient directions as the pixels per cell dimensionality or scale increases and does this make sense given the algorithm and why?   \n",
    "> 4. As the size of the operator increases, you can see that the representation of the cat image changes, becoming more abstracted. If you goal is to robustly represent the object what do you think the trade-off will be and why?    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84266ab",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.     \n",
    "> 2.  \n",
    "> 3.   \n",
    "> 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b00b63",
   "metadata": {},
   "source": [
    "### The BRIEF Algorithm   \n",
    "\n",
    "As with edge and corner detectors, a great many **interest point detection** algorithms have been developed. Here we will only work with one, the BRIEF algorithm. The BRIEF algorithm creates a hash of the gradient directions over patches of the image. These hashes can be matched between images to find common interest points.                   \n",
    "\n",
    "> **Exercise 3-8:** You will now apply the [sskimage.feature.BRIEF](https://scikit-image.org/docs/0.18.x/api/skimage.feature.html?highlight=brief#skimage.feature.BRIEF) function to the equalized gray-scale cat image by the following steps:     \n",
    "> 1. Apply the Harris corner detection algorithm to the equalized gray-scale cat image. \n",
    "> 2. Find the Harris corner peaks with `min_distance=5`, and print the number of peaks found. \n",
    "> 3. Initialize the BRIEF feature extractor with `patch_size=5`.\n",
    "> 4. Extract the BRIEF descriptor hashes from the cat gray-scale image and the corner peaks found. The function returns an object with many attributes. The descriptor hash is the `.descriptor` attribute. \n",
    "> 5. Print the first 4 hashes, being careful to label them.      \n",
    "> 6. Compute the similarity [**Hamming distance**](https://en.wikipedia.org/wiki/Hamming_distance) between the first hash and all subsequent hashes.\n",
    "> 7. Display a histogram of the Hamming distances using 30 bins and the range limited to the possible similarity values. Make sure you label the axes and provide a meaningful title for your chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2a01e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb9e5d",
   "metadata": {},
   "source": [
    "> Examine the images noticing how the pattern of interest point rings changes as the radius of the outer ring increases and answer the following questions.\n",
    "> 1. How does the distribution of the interest points change as the radius of the outer ring increases.   \n",
    "> 2. Paying careful attention to the markers at the center of the rings, notice how the centers converge at large out ring radii. What conclusion can you draw about the importance of these interest points?         \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824cb16a",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1. \n",
    "> 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dac47d",
   "metadata": {},
   "source": [
    "## Texture       \n",
    "\n",
    "Texture is a ubiquitous feature of nearly all images. However, stating exactly what texture is, and how to measure it, is not simple or strait forward. In general, we can say that texture is a function of local variation of the image. Typical measures used to quantify texture are:     \n",
    "- Covariance over local patches of the image where regions with higher covariance have rougher textures.       \n",
    "- Entropy computed over local patches of the image where regions with higher entropy have rougher textures.   \n",
    "\n",
    "Here we will focus on local entropy to measure texture using the [skimage.filters.rank.entropy](https://scikit-image.org/docs/dev/api/skimage.filters.rank.html#skimage.filters.rank.entropy) function. \n",
    "ids\n",
    "\n",
    "> **Exercise 3-9:** You will now use local entropy  as a measure of texture on the equalized gray-scale cat image by doing the following.    \n",
    "> 1. Iterate over the disk patch diameters, $[3,6,12,24,48,96]$, and for each value do each of the following steps. \n",
    "> 2. Compute the local entropy of the color (3-channel) cat image using the [skimage.morphology.disk](https://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.disk) for the patch argument, and using the diameter value. Make sure you convert the image to unsigned 8-bit integer with [skimage.util.img_as_ubyte](https://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.img_as_ubyte).    \n",
    "> 3. Side by side, display the image and a histogram of the entropy values with 50 bins. Include a title with the disk diameter and axes labels for the histogram.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ce858",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab7dbe",
   "metadata": {},
   "source": [
    "> Examine histogram of the hash similarity values. Keeping in mind the maximum and minimum possible values, what statement can you make about how strong the similarity is between these interest points. Try to explain your reasoning.          \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10881394",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43811b3",
   "metadata": {},
   "source": [
    "#### Copyright 2021, Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
