{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b382859",
   "metadata": {},
   "source": [
    "\n",
    "# CSCI E-25\n",
    "## Extracting Features From Images\n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73097e",
   "metadata": {},
   "source": [
    "Feature extraction from images is a fundamental aspect of computer vision. Image features must be extracted for many computer vision methods. Just a few of these are:    \n",
    "1. Object recognition. \n",
    "2. Stitching images together.   \n",
    "3. Locating and identifying objects in images.    \n",
    "4. Models of motion from a series of images.     \n",
    "5. Stereo vision and depth estimation.    \n",
    "\n",
    "In these exercises we focus on classical methods for extracting features from images. In state-of-the-art practice features are created using deep neural networks. Will address learning features with deep neural networks extensively latter. Rest assured that some background in classical methods will help you understand and appreciate the deep learning methods.     \n",
    "\n",
    "To get started, execute the code in the cell below to import the packages you will need for the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93747b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage \n",
    "from skimage import data\n",
    "from skimage.filters.rank import equalize, entropy\n",
    "import skimage.filters as skfilters\n",
    "import skimage.feature as feature\n",
    "import skimage.segmentation as segmentation\n",
    "import skimage.morphology as morphology\n",
    "import skimage.transform as transform\n",
    "from skimage.transform import probabilistic_hough_line\n",
    "import skimage.util as util\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import exposure\n",
    "from skimage.draw import line as draw_line\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfc9b9",
   "metadata": {},
   "source": [
    "## Preparing the Image     \n",
    "\n",
    "Regardless of the methods used an image must be prepared for feature extraction. Often many steps of correcting and filtering the image are required for proper feature extraction. An important step is the adjustment of the spectrum of the image. We need the spectrum to be as wide as possible. We call the transformation  **pre-whitening**. The broad spectrum is considered white light has all spectral components equally represented. We have already discussed the process of whitening the spectrum of images, contrast improvement. An image with high contrast has a broad spectrum.              \n",
    "\n",
    "The cat image requires little preprocessing, beyond pre-whitening. Execute the code in the cell below to load and prepare the cat image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grayscale(img, title, h=8):\n",
    "    plt.figure(figsize=(h, h))\n",
    "    _=plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    _=plt.title(title)\n",
    "\n",
    "cat_image = rgb2gray(data.cat())\n",
    "plot_grayscale(cat_image, 'Image size = ' + str(cat_image.shape))\n",
    "\n",
    "cat_grayscale_equalized = exposure.equalize_adapthist(cat_image, clip_limit=0.02, kernel_size=8)\n",
    "plot_grayscale(cat_grayscale_equalized, 'Equlized image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2905e",
   "metadata": {},
   "source": [
    "> **Important note!** Unless otherwise stated, the `cat_grayscale_equalized` image should be used for subsequent exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381dc6c2",
   "metadata": {},
   "source": [
    "## Edge Detection  \n",
    "\n",
    "**Edges** are a fundamental feature of am image. Edges are characterized by rapid changes in the intensity and are therefore associated with **high gradients**. The rapid change in intensity levels also means that the spectrum around an edge will have high-frequency components. \n",
    "\n",
    "We can take advantage of the broad spectrum at edge features to create an edge detector algorithm. Recall that a Gaussian filter acts is low-pass, reducing the high frequency components. If we take the difference of the Gaussian filters with different bandwidths we can detect edges.              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c210823",
   "metadata": {},
   "source": [
    "> **Exercise 3.1:** To gain a bit of understanding you will create a simple one dimensional example of an edge detector. In this case, you will apply a Laplacian operator to a square wave function that is first positive and then negative. Aficionados of obscure functions will notice the square wave is a member of the Harr basis function (Harr, 1909) series. You will find and display the changes in gradient of this function by the following steps:   \n",
    "> 1. Create the 1-d convolution operator as a Numpy array: $[1.0,-2.0,1.0]$.\n",
    "> 2. Convolve the `series` with the Laplacian operator using  the [numpy.convolve](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html) function. Make sure to set the `mode='same'` argument so the resulting series is the same length as the original.  \n",
    "> 3. Plot the result with the `plot_conv` function provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6130cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv(series, conv):\n",
    "    x = list(range(series.shape[0]))\n",
    "    plt.plot(x, series, label = 'Original series')\n",
    "    plt.plot(x, conv, color = 'red', label = 'Convolution result')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Series of raw values and convolution')\n",
    "\n",
    "series = np.concatenate((np.zeros((20,)), np.zeros((5,)) + 1.0, np.zeros((5,)) - 1.0, np.zeros((20,))))    \n",
    "    \n",
    "##  Your code goes here    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592895b1",
   "metadata": {},
   "source": [
    "> Examine the plot and answer the following questions:   \n",
    "> 1. Why are the 3-point convolution operator values correct in terms of sign, and normalization?   \n",
    "> 2. Does the magnitude of the resulting series correctly represent the gradient of the original series, and why?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f26e8",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.          \n",
    "> 2.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ee29e",
   "metadata": {},
   "source": [
    "> **Exercise 3-2:** You will now create and test an edge detector using the difference of Gaussian filtered images. Starting with the equalized gray-scale cat image, perform the following steps:\n",
    "> 1. Define a first vector of values of $\\sigma_1 = [1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]$ and a second value of $\\sigma_2 = 0.5$. \n",
    "> 2. For each value of the $\\sigma_1$ compute an image which is the difference between the Gaussian filtered images computed with $\\sigma_1$ and $\\sigma_2$.\n",
    "> 3. Display each resulting image, with the value of $\\sigma_1$ in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa52666",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma1 = [1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]\n",
    "sigma2 = 0.5\n",
    "##  Your code goes here  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3577d",
   "metadata": {},
   "source": [
    "> Answer the following questions:\n",
    "> 1. How does the gradient measurement of the images change as the value of $\\sigma_1$ changes and what does this tell you about the scale of features in the image?    \n",
    "> 2. Given the bandwidths of these Gaussian filters compared to the filter with $\\sigma = 0.5$, do the changes of gradient measured make sense and why?     \n",
    "> 3. Does it appear that the difference of Gaussian filters has detected high gradients with different orientations on the image, and why?  \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77792b5",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.            \n",
    "> 2.               \n",
    "> 3.              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd45407",
   "metadata": {},
   "source": [
    "> **Exercise 3-3:** One approach to finding edges is to look for areas of maximum gradient in an image. The difference of Gaussian filters has a strong response to large gradients in image intensity that define edges. In many cases a boolean indicator of an edge is required. To create such an edge indicator from the difference of Gaussian filtered images (equalized gray-scale) with $\\sigma_1 = 1.0$ and $\\sigma_2 = 0.5$ perform the following steps:    \n",
    "> 1 Use the [skimage.filters.threshold_otsu](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.threshold_otsu) function to compute a threshold value for the gradient.       \n",
    "> 2. Apply the threshold and create an image of boolean type.   \n",
    "> 3. Display the boolean image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77394e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=1.0\n",
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401f4be",
   "metadata": {},
   "source": [
    "> Examine the results and answer the following questions:     \n",
    "> 1. Has the binarization process captured the edges at all possible orientations, and why?     \n",
    "> 2. Based on your observation, do you think the edge features are sufficient to identify the animal in the image as a cat within reasonable certainty?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff248b",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.       \n",
    "> 2.              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d6154",
   "metadata": {},
   "source": [
    "Now that you have a bit of experience with edge detection, we will explore a few of the many commonly used edge detection algorithms. As you proceed notice how different algorithms produce different results. \n",
    "\n",
    "### Sobel edge detector   \n",
    "\n",
    "The Sobel edge detector uses the gradients of the pixel values to find edges. Edges are characterized by high gradient magnitude.  \n",
    "\n",
    "Like many feature detectors, a thresholding process must be applied to the gradients found with the Sobel filter. The result is a binary image showing the detected edges. The higher the threshold value, the fewer edge features that will be displayed. This is an example of **nonmaximal suppression**, a widely used method to filter features in computer vision.   \n",
    "\n",
    "> **Exercise 3-4:** The Sobel filter is another form of edge detector. You will now apply the [skimage.filters.sobel](https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.sobel) function to the equalized gray-scale cat image. In this case you will simply compute the norm and not the directions. Then, display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4b5be",
   "metadata": {},
   "source": [
    "> 3. To examine the density of the Sobel filter output, execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23351abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gray_scale_distribution(img):\n",
    "    '''Function plots histograms a gray scale image along \n",
    "    with the cumulative distribution'''\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12, 5))\n",
    "    ax[0].hist(img.flatten(), bins=50, density=True, alpha=0.3)\n",
    "    ax[0].set_title('Histogram of image')\n",
    "    ax[0].set_xlabel('Pixel value')\n",
    "    ax[0].set_ylabel('Density')\n",
    "    ax[1].hist(img.flatten(), bins=50, density=True, cumulative=True, histtype='step')\n",
    "    ax[1].set_title('Cumulative distribution of image')  \n",
    "    ax[1].set_xlabel('Pixel value')\n",
    "    ax[1].set_ylabel('Cumulative density') \n",
    "    plt.show()\n",
    "\n",
    "plot_gray_scale_distribution(cat_sobel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d88bc2",
   "metadata": {},
   "source": [
    "> 4. Next compute the binary image use the [skimage.filters.threshold_otsu](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.threshold_otsu) function to find a threshold. In a loop itterate over multipliers $[1.0,2.0,3.0]$. For each multiplier, apply the product of the multiplier times the threshold found and display the result. Makes sure you display the threshold value for each image.  **Hint:** Notice that you can biniarize the image by taking pixels greater than or equal to or less than or equal to the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c69355",
   "metadata": {},
   "source": [
    "> Compare the edges found with the Sobel filter with those found by the difference of Gaussian filters. Answer the following questions:   \n",
    "> 1. Which aspects of the edges found by the two methods are substantially similar? \n",
    "> 2. How do the edge features detected by the two methods differ, in particular in terms of texture, continuity, compactness, etc? \n",
    "> 3. How do the edges found with the Sobel filter change with the threshold value, and why does this result make sense?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7915a8",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.          \n",
    "> 2.           \n",
    "> 3.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765e1e5",
   "metadata": {},
   "source": [
    "### Canny edge detector\n",
    "\n",
    "As a point of comparison, run the code in the cell below to see the results of the Canny filter edge detector for various Gaussian smoothing parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9806b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sigma in [1.0,2.0,5.0]:\n",
    "    cat_canny = feature.canny(cat_grayscale_equalized, sigma=sigma)\n",
    "    plot_grayscale(cat_canny, 'Canny filter result, sigma = ' + str(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0df068",
   "metadata": {},
   "source": [
    "Compare the edges found with the Canney operator to those found with the Sobel algorithm. Notice that the edges found with Canney are thinner. Particularly, with larger smoothing parameters. Many parts of the cat's face, like the facial hair patterns form closed or nearly closed shapes, outlining the edge of each pattern element.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72878a",
   "metadata": {},
   "source": [
    "## Hough Transform    \n",
    "\n",
    "The family of [Hough transforms](https://en.wikipedia.org/wiki/Hough_transform) are operators that enhance lines in images that are poorly defined. Commonly used versions of the Hough transform enhance straight lines and curved lines, including circles. Three-dimensional versions of the Hough transforms have been been developed for surfaces.     \n",
    "\n",
    "Hough transforms are often used to enhance the features of images used for other computer vision algorithms. For example, enhancement of lines can improve the performance of deep learning algorithms.                  \n",
    "\n",
    "We will focus on the Hough transform for straight lines. The Hough algorithm works by searching a space of possible lines. A simple search for all possible lines using the standard formulation of a straight line, $y = m\\ x + b$, is computationally infeasible. Further for vertical lines the slop is not defined, $m \\rightarrow \\infty$. An alternative formulation for a straight line is the Hesse normal form.      \n",
    "\n",
    "$$r = x\\ cos(\\theta) + y\\ sin(\\theta)$$     \n",
    "\n",
    "Where:      \n",
    "$r =$ the length of a vector from the origin closes point of the line.      \n",
    "$\\theta = $ the angle between the $x$ axis and the vector from the origin closest point of the line.  \n",
    "\n",
    "The Hesse normal form has several advantages for this problem:      \n",
    "- The search for lines can be performed by searching a grid of possible $r$ and $\\theta$ values.    \n",
    "- The Hesse normal form is valid for any line orintation.  \n",
    "\n",
    "The grid of possible $r$ and $\\theta$ values can be regarded as generating a series of proposals for the presence of lines. The values of the proposals are stored in an accumulator. Points where there are peaks in the accumulator are the most likely parameters of a line, $r$ and $\\theta$.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4a2cc",
   "metadata": {},
   "source": [
    "To better understand the Hough transform, we will start with a synthic example. The code in the cell below creates a simple image with three straight lines on a black background. Execute the code and example the image.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_lines = np.zeros((256,256))\n",
    "three_lines[draw_line(132, 64, 196, 196)] = 1.0\n",
    "three_lines[draw_line(96, 64, 64, 180)] = 1.0\n",
    "three_lines[draw_line(64, 64, 175, 155)] = 1.0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.imshow(three_lines, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209580f",
   "metadata": {},
   "source": [
    "We will now apply the basic strait line Hough transform to the simple image. The code in the cell below does the following:      \n",
    "  1. Uses [skimage.transform.hough_line](https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.hough_line) to search a range of test angles. The function returns the values in the accumultor, $h$, the angle, $\\theta$ and the distance to the closest point on the line, $r$.      \n",
    "  2. The log of the accumlotor values is displayed over a range of angles and distances.       \n",
    "  3. The lines found are superimposed on the original image by iterating over all angles distriances tested to find the maximum accumulator values using the [transform.hough_line_peaks](https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.hough_line_peaks) function. This function returns the accumulator value, angle and distance of peaks detected. These peaks are estimates of the detected lines. Each line detected is displayed on the image.   \n",
    "  \n",
    "Execute the code in the cell below of visualize the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6440a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\n",
    "h, theta, r = transform.hough_line(three_lines, theta=test_angles)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "angle_rage = 180\n",
    "r_step = 0.5 * np.diff(r).mean()\n",
    "bounds = [-angle_rage,+angle_rage,\n",
    "          r[-1] + r_step, r[0] - r_step]\n",
    "ax[0].imshow(np.log(1 + h), extent=bounds, cmap=plt.get_cmap('gray'));\n",
    "ax[0].set_title('Hough transform')\n",
    "ax[0].set_xlabel('Angle in degrees')\n",
    "ax[0].set_ylabel('Distance in pixels')\n",
    "\n",
    "ax[1].imshow(three_lines, cmap=plt.get_cmap('gray'))\n",
    "ax[1].set_ylim((three_lines.shape[0], 0))\n",
    "\n",
    "ax[1].set_title('Detected lines')\n",
    "ax[1].set_xlim(0,256)\n",
    "ax[1].set_ylim(0,256)\n",
    "\n",
    "for _, angle, dist in zip(*transform.hough_line_peaks(h, theta, r)):\n",
    "    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n",
    "    ax[1].axline((x0, y0), slope=np.tan(angle + np.pi/2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd96118",
   "metadata": {},
   "source": [
    "The image on the right shows the accumulator values displayed by distance and angle. Notice there are three distinct points in the $r,\\ \\theta$ space where the sinusoidal lines converge. These points represent peaks in the accumulator, and are therefore represent the parameters in Hesse normal form of the detected lines.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a1204",
   "metadata": {},
   "source": [
    "Let's explore some properties of the Hough transform through an real image example. We will use Hough transforms to enhanec the lines on the retina image. To start, execute the code in the cell below to load and display the image.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_equilized = exposure.equalize_adapthist(rgb2gray(data.retina()))\n",
    "plot_grayscale(retina_equilized, 'Retina image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d14dd",
   "metadata": {},
   "source": [
    "> **Exercise 3-5:** You will now apply the Hough transform to the euqalized retina image.   \n",
    "> 1. Compute and display the Canny filtered image with the sigma argument set to 1.5.   \n",
    "> 2. Display the gray scaled filtered image.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16292419",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a946dd",
   "metadata": {},
   "source": [
    "> 3. Complte the code in the cell below using the [skimage.transform.probabilistic_hough_line](https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.probabilistic_hough_line) function. A probabilistic Hough transform differs from the standard Hough transform by using maximum likelihood estimations, rather than finding peaks in an accumulator. Use the `threshold` and `line_length` arguments from the tupple created by the iterator. Set the `line_gap` argument to 3.             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73051c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(lines, ax):\n",
    "    for line in lines:\n",
    "        p0, p1 = line\n",
    "        ax.plot((p0[0], p1[0]), (p0[1], p1[1]), color='blue')\n",
    "\n",
    "_,ax = plt.subplots(3,2,figsize=(10,15))\n",
    "ax=ax.flatten()\n",
    "\n",
    "thresholds = [5,20,5,20,5,20]\n",
    "lengths = [5,5,10,10,20,20]\n",
    "for i, parms in enumerate(zip(thresholds, lengths)):\n",
    "    ## Put your code below  \n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61423a2",
   "metadata": {},
   "source": [
    "> Examine the plots and answer these questions:    \n",
    "> 1. How do the detected lines in the image change with the `line_length` argument?    \n",
    "> 2. How do the detected lines change with the `threshold` argument. and how does this effect compare to changes in `line_length`?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4e1f6",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.         \n",
    "> 2.              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16868c",
   "metadata": {},
   "source": [
    "## Corner Detection    \n",
    "\n",
    "Along with edges, corners are another fundamental feature of images. Detection of corners is fundamentally more difficult than edges:   \n",
    "1. Corners are 2-dimensional features, and require more than just a first order gradient for detection.     \n",
    "2. Corners orientation based on the directions of edges forming them.     \n",
    "3. The angel forming the corner is a fundamental characteristic and changing the angel changes the characteristic of the corner.     \n",
    "As a result of these fundamental characteristics, corner detectors must be based on metrics of multi-dimensional changes in intensity. As an example, the **Sobel** edge detector is a $2 \\times 2$ array of all possible second partial derivatives. This formulation allows us to determine both the presence and the orientation (direction) of corners.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077cb014",
   "metadata": {},
   "source": [
    "> **Exercise 3-6:** The Harris corner detector is one of the many widely use algorithms to detect corner features in images. The detector finds a set of x-y coordinates for each corner that meets a threshold of minimum distance. The threshold is used to perform nonmaximal suppression. Apply the [skimage.feature.corner_harris](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.corner_harris) function to the equalized gray scale cat image by the following steps:\n",
    "> 1. Apply the Harris corner detector to the equalized cat gray-scale image.\n",
    "> 2. Iterate over values of the `min_distance` argument, $[1,3,5,9,12,20]$ and then perform the steps below.       \n",
    "> 3. Use the [skimage.features.corner_peaks](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.corner_peaks) to filter the corner features using the `min_distance` value. \n",
    "> 4. Print the minimum distance and the number of corner features the filtering.\n",
    "> 5. Side by side, plot the gray-scale and the gradient image computed with the difference of Gaussian filtered images with $\\sigma_1 = 2.0$ and $\\sigma_2 = 0.5$. Superimpose on both images the locations of the corners detected using a '+' marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777a657",
   "metadata": {},
   "source": [
    "> Examine the results and images and answer these questions:    \n",
    "> 1. How can you describe the change in the number of corner features as the minimum distance is increased, and why?  \n",
    "> 2. Do the features that remain as the minimum distance increases appear more robust (or obviously corners to the eye) given the gradient computed with the difference of Gaussians?   \n",
    "> 3. Consider a trade-off between using a large set of features which may better represent a specific image vs. a sparser but more robust set of features. How might you decide how to optimally filter the features?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7a976",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.         \n",
    "> 2.           \n",
    "> 3.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14c9d8",
   "metadata": {},
   "source": [
    "### Eigenvalues and Corner Detection     \n",
    "\n",
    "Now that you have worked with a corner detection algorithm, you will create your own using eigenvalues. This algorithm is a simplified version of the well-known SIFT algorithm. The algorithm uses the ratio of the eigenvalues to find corners with changes in gradient. The steps of this algorithm are:    \n",
    "1. The Hessian for each pixel in the image is computed using the operator span specified, $\\sigma$.   \n",
    "2. The eigenvalues of the Hessians are computed.  \n",
    "3. The ratio of the largest to the second eigenvalue is computed.   \n",
    "4. The eigenvalue ratio is thresholded to reduce the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183aa088",
   "metadata": {},
   "source": [
    "> **Exercise 3-7:** You will now implement the corner detection and examine the results by the following steps:   \n",
    "> 1. Compute the Hessian of the equalized gray-scale cat image using [skimage.feature.hessian_matrix](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.hessian_matrix), iterated over values of $\\sigma = [0.2,0.5,1.0,2.0,4.0]$.    \n",
    "> 2. For each Hessian compute the eigenvalues using [skimage.feature.hessian_matrix_eigvals](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.hessian_matrix_eigvals).    \n",
    "> 3. Compute the ratio of the magnitudes or absolute values of the first and second eigenvalues. These values can be found in the first (largest) and second elements of the list returned by the `hessina_matrix_eigvals` function.   \n",
    "> 4. Apply a threshold of $0.01$ to the largest largest eigenvalues. If the larges eigenvalue is less than the threshold set the eigenvalue ratio to $10^{-6}$.   \n",
    "> 5. Display images for the each of the 3 components of the Hessian, $[HRR, HRC, HCC]$, along with the image of the log of the thresholded eigenvalue ratio. Make sure to label all images with the operator span, each of the Hessian components, or log eigenvalue ratio.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2846bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_list = [0.2,0.5,1.0,2.0,4.0]\n",
    "threshold = 0.01\n",
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01569589",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> 1. How can you describe the how the components of the Hessian change as the span of the operator, $\\sigma$, changes and why?\n",
    "> 2. What differences can you see between the components of the Hessian and why? *Hint*, pay careful attention to the orientations of the detected features.    \n",
    "> 3. What differences do you see in the feature image as the span of the Hessian operator, $\\sigma$, changes and why?\n",
    "> **End of exercise**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245d640",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.           \n",
    "> 2.     \n",
    "> 3.          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4b0f6",
   "metadata": {},
   "source": [
    "## Interest Point Descriptors   \n",
    "\n",
    "In many computer vision applications, including image stitching, stereo vision, and flow (motion tracking), require the unique identification of **interest points** or **key points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9abe38",
   "metadata": {},
   "source": [
    "### The HOG Algorithm\n",
    "\n",
    "The orientation of corners can be used as part of a representation or feature map of an image. As with corner detection, a great many algorithms for determining corner orientation have been developed. Here, we will only work with the **HOG** or **histograms of oriented gradients** algorithm. The algorithm finds the directional bin with maximum value for a histogram of the gradients over patches of the image.  \n",
    "\n",
    "The [skimage.feature.hog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.hog) function returns a list. The first element of the list is a vector of the magnitude of the gradients. The second element of the list is a $2 \\times 2$ array of orientation vectors. When a visualization is performed, the long axis of the markers shows the orientation and the width of the marker indicates the magnitude of the gradient.      \n",
    "\n",
    "> **Exercise 3-8:** You will now apply the HOG algorithm to the equalized gray-scale cat image. You will compare the results of using a different number of pixels per cell to estimate the histogram of 9 orientations by the following steps:\n",
    "> 1. Iterate over tuples for the `pixels_per_cell` argument with dimensions, $[(4,4), (8,8), (16,16), (32,32)]$ for the convolutional operator, and for each tuple do the rest of these steps.\n",
    "> 2. Compute the gradient orientations using the `pixels_per_cell=pixels`and with `visualize=True` arguments.\n",
    "> 3. Print the pixels per cell tuple, with the dimensions of the operator and the number of HOGs returned by the function in the title of each image. The number of HOGs is in the first element of the list returned by the `hog` function.    \n",
    "> 4. Display the gray-scale image of the corner orientations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71e71b",
   "metadata": {},
   "source": [
    "> Examine the results and answer the following questions:       \n",
    "> 1. How has the density of the corner direction features changes as the pixels per cell dimensionality increases and why?      \n",
    "> 2. How does the magnitude of the gradient change as the pixels per cell and scale increases and does this make sense given the algorithm and why?     \n",
    "> 3. How does the smoothness and consistency of the gradient directions as the pixels per cell dimensionality or scale increases and does this make sense given the algorithm and why?   \n",
    "> 4. As the size of the operator increases, you can see that the representation of the cat image changes, becoming more abstracted. If you goal is to robustly represent the object what do you think the trade-off will be and why?    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84266ab",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.        \n",
    "> 2.           \n",
    "> 3.          \n",
    "> 4.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b00b63",
   "metadata": {},
   "source": [
    "### The BRIEF Algorithm   \n",
    "\n",
    "As with edge and corner detectors, a great many **interest point** or **key point** detection algorithms have been developed. Here we will only work with one, the BRIEF algorithm. The BRIEF algorithm creates a hash of the gradient directions over patches of the image. These hashes can be matched between images to find common interest points.                   \n",
    "\n",
    "> **Exercise 3-9:** You will now apply the [sskimage.feature.BRIEF](https://scikit-image.org/docs/0.18.x/api/skimage.feature.html?highlight=brief#skimage.feature.BRIEF) function to the equalized gray-scale cat image by the following steps:     \n",
    "> 1. Apply the Harris corner detection algorithm to the equalized gray-scale cat image. \n",
    "> 2. Find the Harris corner peaks with `min_distance=5`, and print the number of peaks found. \n",
    "> 3. Initialize the BRIEF feature extractor with `patch_size=5`.\n",
    "> 4. Extract the BRIEF descriptor hashes from the cat gray-scale image and the corner peaks found. The function returns an object with many attributes. The descriptor hash is the `.descriptor` attribute. \n",
    "> 5. Print the first 4 hashes, being careful to label them.      \n",
    "> 6.  Compute the [**Hamming similarity**](https://en.wikipedia.org/wiki/Hamming_distance) between the first hash and all subsequent hashes using [numpy.logical_xor](https://numpy.org/doc/stable/reference/generated/numpy.logical_xor.html).  \n",
    "> 7. Display a histogram of the Hamming similrity using 30 bins and the range limited to the possible similarity values. Make sure you label the axes and provide a meaningful title for your chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "plt.hist(similarity, bins=30)\n",
    "plt.xlim((0,255))\n",
    "_=plt.xlabel('Similarity')\n",
    "_=plt.ylabel('Number')\n",
    "_=plt.title('Similrity of BRIEF descriptors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb9e5d",
   "metadata": {},
   "source": [
    "> Examine histogram of the hash similarity values. Keeping in mind the maximum and minimum possible values, what statement can you make about how strong the similarity is between these interest points. Try to explain your reasoning.         \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772c9f2",
   "metadata": {},
   "source": [
    "> **Answer:**        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dac47d",
   "metadata": {},
   "source": [
    "## Texture       \n",
    "\n",
    "Texture is a ubiquitous feature of nearly all images. However, stating exactly what texture is, and how to measure it, is not simple or strait forward. In general, we can say that texture is a function of local variation of the image. Typical measures used to quantify texture are:     \n",
    "- Covariance over local patches of the image where regions with higher covariance have rougher textures.       \n",
    "- Entropy computed over local patches of the image where regions with higher entropy have rougher textures.   \n",
    "\n",
    "Here we will focus on local entropy to measure texture using the [skimage.filters.rank.entropy](https://scikit-image.org/docs/dev/api/skimage.filters.rank.html#skimage.filters.rank.entropy) function. \n",
    "ids\n",
    "\n",
    "> **Exercise 3-10:** You will now use local entropy  as a measure of texture on the equalized gray-scale cat image by doing the following.    \n",
    "> 1. Iterate over the disk patch radii, $[3,6,12,24,48,96]$, and for each value do each of the following steps. \n",
    "> 2. Compute the local entropy of the equalized gray-scale cat image using the [skimage.morphology.disk](https://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.disk) for the patch argument, and using the diameter value. Make sure you convert the image to unsigned 8-bit integer with [skimage.util.img_as_ubyte](https://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.img_as_ubyte).    \n",
    "> 3. Side by side, display the image and a histogram of the entropy values with 50 bins. Include a title with the disk diameter and axes labels for the histogram.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ce858",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Your code goes here  \n",
    "for radii in [3,6,12,24,48,96]: \n",
    "    cat_entropy = entropy(util.img_as_ubyte(cat_grayscale_equalized), morphology.disk(radii))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16,5))\n",
    "    _=ax[0].imshow(cat_entropy)\n",
    "    _=ax[0].set_title('For disk radii = ' + str(radii))\n",
    "    _=ax[1].hist(cat_entropy.flatten(), density=True, bins=50, histtype='step')\n",
    "    _=ax[1].set_title('For disk radii = ' + str(radii))\n",
    "    _=ax[1].set_ylabel('Density')\n",
    "    _=ax[1].set_xlabel('Entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab7dbe",
   "metadata": {},
   "source": [
    "> Examine these results and answer the following questions:     \n",
    "> 1. How can you describe the change in the image with operator diameter, and what does this tell you about the scale of the features?    \n",
    "> 2. At what scale does the histogram exhibit distinctly multi=modal behavior, and what does this mean in terms of possibly segmenting the image into regions with common properties?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10881394",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1. As operator size increases, the features defined by entropy be come larger and more continuous looking. This behavior corresponds to increasing feature scale by the octaves of the operator diameter.     \n",
    "> 2. At an operator diameter of 48 the histogram has distinctly bi-modal behavior. Partition by these modes allow the image to be divided into several domains or regions.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10161a",
   "metadata": {},
   "source": [
    "Local entropy shows texture properties of an image. There are several uses for these properties in CV algorithm. Segmentation of an image into regions by texture is one such algorithm. \n",
    "\n",
    "The code in the cell below segments the image into 3 regions by value of the entropy. Execute the code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce25a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = 24.0 \n",
    "threshold1 = 7.8\n",
    "threshold2 = 7.5\n",
    "cat_entropy_96 = entropy(util.img_as_ubyte(cat_grayscale_equalized), morphology.disk(radii))\n",
    "cat_entropy_96 = np.where(cat_entropy_96 > threshold1, 1.0, np.where(cat_entropy_96 > threshold2, 0.5, 0.0))\n",
    "#cat_entropy_96 = np.where(cat_entropy_96 < threshold2, 0.0, cat_entropy_96)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,5))\n",
    "ax = ax.flatten()\n",
    "_=ax[0].imshow(cat_entropy_96)\n",
    "_=ax[0].set_title('Segmentationn for disk radii = ' + str(radii))\n",
    "_=ax[1].imshow(cat_grayscale_equalized, cmap=plt.get_cmap('gray'))\n",
    "_=ax[1].set_title('Original cat image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d87ab",
   "metadata": {},
   "source": [
    "Compare the regions found by the segmentation to the original gray-scale image. Notice how the segmentation of the image by texture reasonably matches the large-scale features of the cat image. This segmentation could be used to mask the areas of image of interest, for example.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43811b3",
   "metadata": {},
   "source": [
    "#### Copyright 2021, 2022, 2023, 2024, 2025 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae2048-4445-42d8-9815-53c98bcbd5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
