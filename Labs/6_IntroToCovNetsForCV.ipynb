{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CSCI E-25\n",
    "## Introduction to Convolutional Neural Networks\n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Introduction to convolutional neural networks\n",
    "\n",
    "This lesson introduces you to a powerful neural network architecture, known as **convolutional neural networks**. Convolutional neural networks operate by **learning a set of filters**  or **convolution kernels**. Using a process, known as **convolution**, these filters extract a **feature map** from the raw data. The feature map is a **lower dimensional** map of the raw input features. This map is learned in a supervised machine learning process employing back propagation. You can think of convolutional neural networks as a powerful and flexible **dimensionality reduction** method. \n",
    "\n",
    "Convolutional neural networks are used to build feature maps from any type of data with coherency in one or more dimensions. This includes time series data, and text data as one dimensional cases, and image data as a two dimensional case. \n",
    "\n",
    "Convolutional neural networks have a resonably long history. The first known commerical application was for automated processing of check images by LeCun et. al. (1998). For unclear reasons, convolutional neural networks were religated to specialized applications such as hand writing recognition until Krizhevsky et. al. (2012) used them to deciscively win an ImageNet object recognition competition. Curiously, other teams had previously used convolutional neural networks to win competitons, but somehow this was not widely recognized. \n",
    "\n",
    "In this lesson you will learn:\n",
    "1. The basics of convolutional neural network architecture.\n",
    "2. How convolutional neural networks learn the filters to create the feature map. \n",
    "3. Feature extraction for creating the feature map. \n",
    "4. Visualization of the feature map and the filters.\n",
    "5. Commonly used convolutional opertor design. \n",
    "6. How to use the feature map in suppervised learning. \n",
    "7. Application of various regularization methods for improving the behavior of learning. \n",
    "\n",
    "****\n",
    "**Note:** The formulation of convolution filters used in deep learning is somewhat idiosycratic. If you are familar with the standard approach as applied in say electrical engineering or control engineering you may find the approach used here a bit different. \n",
    "****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to import the packages required to execute the code in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "#import keras\n",
    "from keras import callbacks\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import keras.utils.np_utils as ku\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop \n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Applying 2-d convolution\n",
    "\n",
    "Let's review how we can apply convolution operations in two dimensions. Figure 1.1 illustrates a simple example of applying a convolution operator to a 2-d input array. The 2-d input array can be a gray-scale image, for example. \n",
    "\n",
    "A 3X3 convolutional operator is illustrated in Figure 1.1. This operator is applied to a small image of dimension of 4X4 pixels. Within the 4X4 image the operator is only applied four times, where the complete operator lies on the image. Convolution where the operator is completely contained within the input sample is sometimes called call **valid convolution**. \n",
    "\n",
    "In this case the 4X4 pixel input results in a 2X2 pixel output. You can think of the convolutional operator mapping from the center of the operator input to pixel in the output. Similar to the 1-d case, for a convolution operator with span $s$ in both dimensions the output array will be reduced in each dimension by $\\frac{s + 1}{2}$\n",
    "\n",
    "![](img/2D-Conv.JPG)\n",
    "<center>Figure 1.1. Simple 2-d convolution operation</center>\n",
    "\n",
    "For a convolution with the identical span $s$ in both dimensions we can write the convolution operations as: \n",
    "\n",
    "$$S(i,j) = (I * K)(i,j) = \\sum_{m = {i - a}}^{i + a} \\sum_{n = j - a}^{j + a} I(i, j) K(i-m, j-n)$$\n",
    "\n",
    "Notice that $S$, $I$ and $K$ are all tensors. Given this fact, the above formula is easily extended to higher dimensional problems. \n",
    "\n",
    "The convoluational kernel $K$ and the image $I$ are commutative so there is an alternative representation by applying an operation known as **kernel flipping**. Flipped kernel convolution can be represented as follows:\n",
    "\n",
    "$$s(i,j) = (I * K)(i,j) = \\sum_{m = {i - a}}^{i + a} \\sum_{n = j - a}^{j + a} I(i-m, j-n) K(i, j)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convolution in higher dimensions\n",
    "\n",
    "In deep convolutional neural networks the input typically have several **input channels** with the convolution operation resulting in multiple **output channels**. \n",
    "\n",
    "So far, we have only looked at single channel convolution. Here are some examples of input tensors with different numbers of input channels:\n",
    "- A gray-scale image has a single input channel of pixels. \n",
    "- A color image typically has 3 input channels, {Red, Green, Blue}.\n",
    "- A color movie has a large number of input channels. For $n$ images in the video there are $3 * n$ images counting the Red, Green, Blue channels for each image.  \n",
    "\n",
    "The purpose of the a convolutional neural network is to create a feature map. To create the most general feature map convolution operators need not be restricted to a single input channel. \n",
    "\n",
    "Further, the feature map can can many channels, resulting in multiple output channels. Each channel is a set of features extracted from the input tensor. This concept is illustrated in Figure 1.2 below. In this figure a set of convolution operators transform the input tensors to multiple channels of the feature map. Each convolution operator maps to a specific output channel in the feature map. \n",
    "\n",
    "![](img/MultiChannel.JPG)\n",
    "<center>Figure 1.2. Example of multichannel convolution</center>\n",
    "\n",
    "Let's investigate the case of a 3-d input tensor $V$, a 3-d output tensor $Z$ and 4-d convolution tensor $K$. In this case the convolution equation becomes:\n",
    "\n",
    "$$Z_{i,j,k} = (V * Z)(i,j,k,l) = \\sum_{l} \\sum_{m = -a}^{a}\n",
    "\\sum_{n = -a}^{a}\n",
    "V_{i,j,l} \\cdot K_{i-m,j-n,k,l}$$\n",
    "where,  \n",
    "$i,j$ are the spatial dimensions,  \n",
    "$k$ is the index of the output channel,  \n",
    "$l$ is the index of the input channel,  \n",
    "$K_{i,j,k,l}$ is the kernel connecting the $l$th channel of the input to the $k$th channel of the output for pixel offsets $i$ and $j$    \n",
    "$V_{i,j,l}$ is the $i,j$ input pixel offsets from channel $l$ of the input,  \n",
    "$Z_{i,j,k}$ is the $ i,j$ pixel offsets from channel $k$ of the output,  \n",
    "$a = \\frac{1}{2}(kernel\\_span + 1)$, for an odd length kernel.  \n",
    "\n",
    "Notice that the summation in the equation above is over input channels $l$ as well as the spatial dimensions $m,n$ of the convolutional operator. This operation is applied for each spatial pixel coordinate $i,j$. This generalization allows the convolutional kernel to operate over multiple input channels. This is property is highly desirable since features should be extracted from the entire input tensor. \n",
    "\n",
    "The entire forgoing operation is performed for each output channel $k$. Therefore, you can think of the $k$th dimension of the 4-d kernel tensor as indexing multiple 3-d convolution kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parameter sharing and sparse interactions\n",
    "\n",
    "**Parameter sharing** and **sparse interactions** are two powerful aspects of convolutional neural networks. To understand what these terms mean and why they are important, consider the fully connected neural network in Figure 1.3 below.  \n",
    "![](img/FullyConnected.JPG)\n",
    "<center>Figure 1.3. A fully connected network</center>\n",
    "\n",
    "The fully connected neural network has a large number of weights, $5^2 = 25$ in this case. Compare this network to the convolutional layer with $2 \\times 2$ operator will have only $2*2 = 4$ weights. For a more realistic case, the fully connected network might have millions of weights. The convolutional network using **weight sharing** across the entire layer. For a $5 \\times 5$ CovNet layer there are only $5 * 5 = 25$ weights. Parameter sharing is also referred to as **tied weights**. This is an example of a **sparse interaction**.\n",
    "\n",
    "The shared nature of the parameters in convolutional neural networks is a significant reduction in complexity. Statistically, such a sparse model is said to **share statistical strength**. This statistical strength results in model weights with less uncertainty and a model less likely to be over-fit.  \n",
    "\n",
    "The computational efficiency of convolutional neural networks is not just in training. Convolutional neural networks are also efficient for creating **rich feature maps** which lead to high levels of accuracy in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Pooling and invariance\n",
    "\n",
    "To further reduce dimensionality and to make the feature map robust to small shifts in the input, **pooling** is applied following the nonlinear detection stage. Pooling involves applying an aggregation function to patches of the output from the nonlinear detector of a convolution layer. \n",
    "\n",
    "Researchers have tried a number of pooling schemes. In practice **max pooling** (Zhou and Chellappa, 1988) has proved to be both robust and versatile. In max pooling the output value is just the maximum of the input values in each patch. \n",
    "\n",
    "Unlike convolutional layers, there are no parameters to learn in pooling layers. \n",
    "\n",
    "Max pooling has the nice property of being robust to small shifts in the input values. For example, Figure 1.4 shows the response of max pooling to a right sift of a single pixel for an operator with a span of 3. Notice that the response itself is simply shifted one pixel to the right as well. The pattern of maximum values in the feature map (the output) remains the same. \n",
    "\n",
    "![](img/ShiftOne.JPG)\n",
    "<center>Figure 1.4.   \n",
    "    Example of max pooling responses to one pixel shift right</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Stride and tiling\n",
    "\n",
    "Up until now, we have only moved the convolution operator one pixel at a time. There is no restriction that requires this type of step, however. In fact, a convolutional operator or pooling opertor can be moved by several pixels in any direction. This step size is known as the **stride** of the operator. \n",
    "\n",
    "Operators with stride greater than one **decimate** the number of pixels in the output. This can be useful when resizing images of different dimensions so they have constant input dimensions for a neural network. \n",
    "\n",
    "Using an operator with $stride = span$ is a special case known as **tiling**. Tiled operators are layed out without any overlap. Figure 1.6 shows an example of tiling. In this case, an input of dimensions $6X6$ is tiled by operators with dimension $3X3$ with stride of 3. Only 4 tiles fit on the input in this case, resulting in a $4X4$ output array.\n",
    "\n",
    "![](img/Tiled.JPG)\n",
    "<center>**Figure 1.6. Example of a tiled convolution operator**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 6-1:** You will now undertake an exercise to better understand the effects of max pooling. To simplify this exercise you will work with 1-dimensional data and **edge detection** operators. Do the following:   \n",
    "> 1. Create a function named `max_pool` that returns maximum of the absolute value kernel operator with a stride of 1 over the input series. The kernel span should be an argument to the function, with default value of 3. For simplicity start the operator at the beginning of the series.     \n",
    "> 2. Create two edge detection convolutional kernels, $kernel1 = [-0.5, 1.0, -0.5]$ and $kernel2 = [0.5, -1.0, 0.5]$.   \n",
    "> 3. Apply both of the convolution kernels to the `series1` provided using the [numpy.convolve](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html) function. Assign these two results to separate variables.    \n",
    "> 4. Apply your `max_pool` function to the convolution result series. Assign these two results to different variables.    \n",
    "> 5. Display the results comparing the convolution series to the max-pooled series using the `plot_pool` function provided.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(series, pool, x_pool, label, ax):    \n",
    "    x = list(range(series.shape[0]))\n",
    "    ax.plot(x, series, label = 'Detected series')\n",
    "    ax.plot(x_pool, pool, color = 'red', label = label)\n",
    "    ax.set_title('Series of raw values and max pool values')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Value') \n",
    "    \n",
    "def plot_pool(series1, series2, pool1, pool2):\n",
    "    x = list(range(series1.shape[0]))\n",
    "    end = series1.shape[0]\n",
    "    end_pool = pool1.shape[0]\n",
    "    end_remain = end % end_pool\n",
    "    pool_stride = int((end_pool - end_remain)/end_pool) + 1\n",
    "    x_pool = list(range(end_remain, end_pool + end_remain, pool_stride))\n",
    "\n",
    "    ## Display the results\n",
    "    fig,ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "    ## First channel\n",
    "    plot_series(series1, pool1, x_pool, 'First channel max pool', ax[0])\n",
    "    ## Second channel\n",
    "    plot_series(series2, pool2, x_pool, 'Second channel max pool', ax[1])\n",
    "    \n",
    "nr.seed(12233)\n",
    "series1 = np.concatenate((np.zeros((20,)), np.zeros((5,)) + 1.0, np.zeros((5,)) - 1.0, np.zeros((20,))))\n",
    "\n",
    "def max_pool(series, span=3):\n",
    "    '''Performs simple 1d max pooling with\n",
    "    an operator over span and stride 1'''\n",
    "    ## Your code goes below\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. Define and apply new 5-point convolution kernels for the edge detection, $[-0.25, - 0.25, 1.0, -0.25, -0.25]$ and $[0.25, 0.25, -1.0, 0.25, 0.25]$. \n",
    "> 7. Apply the max pooling with kernel span of 5 to both of the edge detection series.  \n",
    "> 8. Display the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions in one or a few sentances:   \n",
    "> 1. How does the max-pooling operator with span of  preserve edges in the convolution output series?     \n",
    "> 2. Haw can you explain the invariance of the max-pooling results to the sign of the input series.  \n",
    "> 3. How can you explain the difference in the edge detection series and the max-pooled values between the two kernel spans in terms of the receptive field of the operator?    \n",
    "> **End of exercise**.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1. The pooled series shows the maximum absolute values within the span of the pooling operator which preserves maximums at the edges, or rapid transitions of the input signal. \n",
    "> 2. Max pooling uses absolute values, and is invariant to the sign of the convolution result.     \n",
    "> 3. The operations with kernel span of 5 have a larger receptive field that for kernel span of 3. The larger receptive field has the effect of elongating the edge regions detected and producing wider maximum regions from the max pooling.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Convolutional neural networks with Keras\n",
    "\n",
    "With the foregoing theory and simple examples, let's try constructing and testing a convolutional neural network using Keras. In this case, we will test classification of of the MNIST dataset. The neural network will have the following layers:\n",
    "1. An input layer for the 28X28 images.\n",
    "2. A multi-layer convolutional neural network to create a feature map.\n",
    "3. A fully-connected hidden layer to perform the classification.\n",
    "4. A output layer to indicate which digit is most likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing the dataset\n",
    "\n",
    "The preparation of this dataset is follows nearly identical steps to the process followed in an earlier lesson. In summary, three preparation steps are performed:\n",
    "1. Load the training and test image data and labels.\n",
    "2. Reshape the image tensors so the neural network views them as a 4-d tensor. For the training images the tensor has a shape of (60000,28,28,1) corresponding to 60000 images of dimension $28 \\times 28$ with a single channel (these are gray-scale images). At the same time the pixel values are converted to `float` type in the range $\\{ 0,1 \\}$.\n",
    "3. The labels are recoded as dummy variables using the Keras [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float')/255\n",
    "print(train_images.shape)\n",
    "print(train_images.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_images.shape, test_labels.shape)\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float')/255\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = ku.to_categorical(train_labels)\n",
    "print(train_labels[5:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = ku.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the one-hot encoding of the labels. There is one dummy variable (column) for each label value, $\\{0,...,9\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining the model. \n",
    "\n",
    "It is now time to define the model. \n",
    "\n",
    "> Early stopping regularization is an old idea in machine learning. The concept is simple, stop learning at the point the model starts to over-fit. Early stopping measures learning with a single selected validation metric. The early stopping algorithm can be describe as follows, using a validation metric like validation error rate or validation loss rate.    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Proceedure Early_Stoping\n",
    "    patience = initialPatience\n",
    "    patienceCount = 0\n",
    "    maxSteps = initialMaxSteps\n",
    "    steps = 0\n",
    "    bestMetric = 1.0\n",
    "    model = Instantiate_Model(trainData, validationData, hyperparameters)    \n",
    "    while patience >= patienceCount and steps <= maxSteps:     \n",
    "        model = Train(model)  \n",
    "        metric = Validate(model)  \n",
    "        if metric > bestMetic: \n",
    "            patienceCount =+\n",
    "        else  \n",
    "            bestMetric = metric   \n",
    "            bestModel = model\n",
    "        steps =+    \n",
    "    return bestModel        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, the callbacks required to implement early stopping regularization are defined. The steps to define the [early stopping call backs with Kera](https://keras.io/api/callbacks/early_stopping/) are:    \n",
    "1. Define a path to an HDF5 file used to save the training history. \n",
    "2. The first callback stops the training when the validation loss does not improve within 4 epochs, the patience.  \n",
    "3. The second callback saves the model weights resulting from the epoch with the best validation loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up and call-backs for early stopping\n",
    "filepath = 'my_model_file.hdf5' # define where the model is saved\n",
    "callbacks_list = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = 5 # Stop after patience steps with lower accuracy\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True # Only save model if it is the best\n",
    "        )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 6-2:** You will now use Keras to define the CovNet model named `nn`. This model uses both convolutional layers and max pooling layers. You will define this model by completing the function in the code cell below. You can find [documentation on the Keras convolutional layers here](https://keras.io/layers/convolutional/). You can find [documentaton on Keras pooling layers here](https://keras.io/layers/pooling/).\n",
    "> The architecture of the model you will create is:\n",
    "> 1. A 3X3 convolutional layer with 32 output channels using the [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2D/) function, with `activation='relu'`. Recall that this operation creates a feature map with 32 channels from the single gray-scale input channel. This is the input layer of the model, so make sure you define the input shape. \n",
    "> 2. A 2X2 max pooling layer using the [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) function.\n",
    "> 3. A dropout layer is used to improve regularization of the model using the [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) function with argument 0.2. \n",
    "> 3. A 3X3 convolution layer with 64 output channels, with `activation='relu'`. The 32 channels of the input layer are mapped to a 64 channel feature map. \n",
    "> 4. Another 2X2 max pooling layer. \n",
    "> 5. A dropout layer to improve regularization of the model with argument 0.2. \n",
    "> 6. A final 3X3 convolution layer with 64 output channels, with `activation='relu'`. \n",
    "> 7. A dropout layer to improve regularization of the model with argument 0.2. \n",
    "> 8. Flatten the 64 channel feature map to a vector.\n",
    "> 9. A Dense (fully-connected) hidden layer with 64 units is the first classifier layer, with `activation='relu'`. A limited amount of regularization is applied using Keras [layer regularizer l2](https://keras.io/api/layers/regularizers/) as a ` kernel_regularizer`, with hyperparameter 0.1. This layer operates on the flattened feature map. \n",
    "> 10. A dropout layer to improve regularization of the model with argument 0.5. \n",
    "> 11. The output Dense layer has 10 units with `activation='softmax'` to indicate the classification of the digits.\n",
    "> 12. A summary of the model is printed.\n",
    "> 13. The model object is returned. \n",
    "> Execute your code and examine the summary of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(): \n",
    "    ## Your code goes below\n",
    "    \n",
    "\n",
    "nn = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is quite bit of interesting information in the summary of this model. In one or a few sentances answer the following questions:\n",
    "> 1. How does the dimensionality of the feature map change at each convolution and max pooling layer? \n",
    "> 2. Compare the number of parameters of your CovNet model to the fully connected model you created for exercise 5-4. How different is the overall number of parameters and layers? \n",
    "> 3. What does the difference in the numbers of parameters between the CovNet and the fully connected model tell you about the ability to train these models with a finite amount of data? \n",
    "> 4. How many learnable parameters do the max-pooling layers have, and why?    \n",
    "> **End of exercise**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.     \n",
    "> 2.     \n",
    "> 3.      \n",
    "> 4.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train and test the model\n",
    "\n",
    "Now, it is time to compile and fit your model. This particular model appears to require only limited regularization. The table below shows the limited heuristic search performed on the hyperparamters.\n",
    "\n",
    "| Final Accuracy  | Dropout  | weight decay | Learning Rate |   \n",
    "| --------------- | -------- | ------------ | ------------- |\n",
    "| 0.9923          | 0.2      | 0.00         | 0.001        |\n",
    "| 0.9923          | 0.2      | 0.001         | 0.001        |\n",
    "| 0.9890          | 0.5      | 0.001         | 0.001        |\n",
    "\n",
    "\n",
    "The code in the cell below compiles the model using the chosen optimizer with learning rate and weight decay hyperparameters selected. The model is then fit and the history is captured. Run the code in the cell below, which may take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "nn.compile(optimizer=RMSprop(learning_rate=0.001, decay=0.001), \n",
    "           loss = 'categorical_crossentropy', metrics = ['accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "## Now fit the model\n",
    "history = nn.fit(train_images, train_labels, \n",
    "                  epochs = 80, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code in the cells below to plot the loss and accuracy history vs. epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    '''Function to plot the loss vs. epoch'''\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(test_loss) + 1))\n",
    "    plt.plot(x, test_loss, color = 'red', label = 'Test loss')\n",
    "    plt.plot(x, train_loss, label = 'Training losss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracy(history):\n",
    "    x = list(range(1, len(history.history['accuracy']) + 1))\n",
    "    plt.plot(x, history.history['accuracy'], color = 'red', label = 'Test accuracy')\n",
    "    plt.plot(x, history.history['val_accuracy'], label = 'Training accuracy')\n",
    "    plt.plot(x, history.history['top_k_categorical_accuracy'], label = 'Top 2 train accuracy')\n",
    "    plt.plot(x, history.history['val_top_k_categorical_accuracy'], label = 'Top 2 validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Epoch')      \n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(history)    \n",
    "plot_accuracy(history)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 6-3:** Examine these results and answer the following questions:    \n",
    "> 1. How does the accuracy and error rate of your CovNet model compare to the regularized fully connected model of Exercise 5-10 of the previous assignment?\n",
    "> 2. How can you characterize the convergence or learning rate of the CovNet model after the first 10 epochs.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.      \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will continue with the evaluation of the model. One can compute several possible accuracy metrics.    \n",
    "\n",
    "First, you will use conventaional or **top-1** accuracy along with **top-k** accuracy, in this case top-2. Top-1 accuracy measures the fraction of cases where the correct category is predicted. Top-k accuracy measures the fraction of cases where the correct category is in the top k probabilities of categories. Since there are only 10 categories for this problem we are limiting to $k=2$. For cases with many categories it is common to use top-5 accuracy.    \n",
    "\n",
    "You will also compute and display the mean average precision and mean average recall.  \n",
    "\n",
    "Execute this code in the cell below to compute and display these accuracy metrics along with average precision and average recall.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_imges, test_labels):   \n",
    "    ## Compute class predictions\n",
    "    class_probabilities = model.predict(test_images)\n",
    "    class_predictions = class_probabilities.argmax(axis=1)\n",
    "    \n",
    "    ## Compute the top-1 categorical accuacy.  \n",
    "    categorical_accuracy = CategoricalAccuracy()\n",
    "    categorical_accuracy.update_state(test_labels, class_probabilities)\n",
    "\n",
    "    # Compute the top-2 accuracy.  \n",
    "    top_2_accuracy = TopKCategoricalAccuracy(k=2, name='top_2_categorical_accuracy')\n",
    "    top_2_accuracy.update_state(test_labels, class_probabilities)\n",
    "\n",
    "    ## Summarize and print the result.   \n",
    "    print('Categorical accuracy = ' + str(categorical_accuracy.result().numpy()))\n",
    "    print('Top 2 accuracy = ' + str(top_2_accuracy.result().numpy()))\n",
    "\n",
    "    ## Get the class labels, counts and compute the class-specific recall and precision.  \n",
    "    test_labels_vector = test_labels.argmax(axis=1)\n",
    "    unique_labels, label_counts = np.unique(test_labels_vector, return_counts=True)\n",
    "    class_precision = metrics.precision_score(test_labels_vector, class_predictions, labels=unique_labels, average=None)\n",
    "    class_recall = metrics.recall_score(test_labels_vector, class_predictions, labels=unique_labels, average=None)\n",
    "\n",
    "    ## Comptue and print the mean average precision and recall accounting for class frequency weights\n",
    "    sum_label_counts = np.sum(label_counts)\n",
    "    weighted_average = lambda x: round(np.sum(np.divide(x * label_counts, sum_label_counts)), 4)\n",
    "    print('Average precision = ' + str(weighted_average(class_precision)))\n",
    "    print('Average recall = ' + str(weighted_average(class_recall)))\n",
    "\n",
    "    ## Compute, display and plot the confusion matrix.  \n",
    "    confusion_matrix = metrics.confusion_matrix(test_labels_vector, class_predictions)   \n",
    "    print('\\nConfusion matrix')\n",
    "    print(pd.DataFrame(confusion_matrix))\n",
    "    p = plt.imshow(np.log(np.divide(confusion_matrix + 1.0, np.sum(confusion_matrix, axis=1))))\n",
    "    cb = plt.colorbar(p)\n",
    "    _=cb.set_label('Log count')\n",
    "\n",
    "evaluate_model(nn, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 6-4:** Examine the evaluation metrics of the model. Notice that the accuarcy, mean average precision and mean average recall are all high. Now, answer these questions:    \n",
    "> 1. Consider the differences between the top-1 (categoracal) accuraccy and top-2 accuracy. Is the difference expected for this model and why?   \n",
    "> 2. Why are average precision and average recall values computed expected for this model?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.         \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 What features does the convolutional neural network learn?\n",
    "\n",
    "You might well ask, what is in the various layers of the feature map? It turns out, we can actually visualize the output from each layer of the convolutional neural network to get a feel for the learned features. This can be done for both convolutional and max pooling layers.\n",
    "\n",
    "As a first step we must pick and image used as the test case. For this example, we arbitrarily choose the 13th image in the training tensor. Execute the code in the cell below to visualize this image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_images[12,:,:,:]\n",
    "print(img.shape)\n",
    "plt.imshow(img.reshape((28,28)), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample image turns out to be a number 3. \n",
    "\n",
    "The code in cell below does the following:\n",
    "1. Extracts the layers from the model object into a list. \n",
    "2. Creates an activation map for each layer using the original model object and the list of layer outputs. \n",
    "3. Computes the activations for each layer by applying the predict method to the activation map. \n",
    "\n",
    "Execute this code to create a list of activations for the convolutional neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in nn.layers[:7]]\n",
    "activation_model = models.Model(inputs = nn.input, outputs = layer_outputs)\n",
    "activations = activation_model.predict(img.reshape(1,28,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below does the following:\n",
    "1. Iterates through the activation maps of the first 5 model layers.\n",
    "2. Iterates over the activation map channels in the layer displaying the image. There is one image for each channel in the feature map. \n",
    "\n",
    "Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    fig_shape = activations[j].shape\n",
    "    s = fig_shape[3]/32\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    for i in range(fig_shape[3]):\n",
    "        ax = fig.add_subplot(int(s*4),8,(i+1))\n",
    "        plt.imshow(activations[j].reshape((fig_shape[1],fig_shape[2],fig_shape[3]))[:,:,i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 6-5:** Examine the above sets of images. There are 5 sets of images, one for each of the output of the first 5 layers of the model. For each layer, there is an image showing each channel of the feature map. Keeping in mind that CNNs are known to be effective as finding find-grain features, in one or a few sentences, answer the following questions: \n",
    "> 1. The initial single channel $28 \\times 28 \\times 1$ input image tensor becomes a $32 \\times 26 \\times 26 \\times 1$, 32 channel feature map after the first convolution. The second $32 \\times 13 \\times 13 \\times 1$ output is from the max-pooling layer. In general terms, describe how much these feature maps resemble the original image, and why do you think this might be the case given the receptive field of the kernels?    \n",
    "> 2. Examine the feature maps created by the forth (convolution), fith (max-pooling), and seventh (convolution) layers. How can you best describe the level of abstraction of these layers and the the resemblance to the original image?      \n",
    "> 3. What effect can you observe of using the max-pooling layer other than dimensionality reduction, and why?  \n",
    "> 4. WHat relationship can you identify between the spatial dimensionality of the feature map and the level of abstraction and localization of the features? \n",
    "> **End of exercise**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.    \n",
    "> 2.     \n",
    "> 3.     \n",
    "> 4.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Padding convolution\n",
    "\n",
    "As you have observed, each convolution layer in a neural network reduces the spatial dimensions by $\\frac{span+1}{2}$ for odd span kernels. The coverage of the convolution opertor can be expanded by **zero padding** in the spatial dimenstion. With the zero padding added, the convolution operator covers the entire spatial dimension of the input tensor. This type of convolution is sometime referred to as **same convolution**, since the output tensor has the same spatial dimensions. \n",
    "\n",
    "In some situations, adding zero padding produces a better feature map. Zero padding allows each pixel to be visited the same number of times by the convolution operator. Without padding pixels at the edge, are not full represented in the feature map. More importantly, the larger feature map will have bigger model capacity. \n",
    "\n",
    "2-d convolution with zero padding is illustrated in Figure 4.1 below. The original input 2-d input tensor is shown in gray. The span of the kernel is 3X3 so one additional column of zeros is added to each sided along with a row of zeros top and bottom. As illustrated, the kernel can operate on the edge pixels. The result is an output tensor with the same spatial dimension as the input. \n",
    "\n",
    "![](img/Padding.JPG)\n",
    "<center>**Figure 4.1. 2-d convolution with zero padding**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 6-6:** With Keras, adding zero padding to convolutional layers is easy. The `padding = same` argument adds zero padding. The default is `padding = valid` adds no padding. Create a new model, with a new name 'nns', with the same layers as the foregoing model, and a dropout rate of 0.2, but with the `padding = same` argument for the convolutional layers.  Then print the summary of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nns.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the summary of your model and answer these questions:   \n",
    "> 1. The final feature map is $64 \\times 7 \\times 7$, rather than $64 \\times 3 \\times 3$ for the model without padding. This feature map has 49 pixels per channel, compared to 9 pixels per channel previously. Do you expect that this feature map will contain more information and why? \n",
    "> 2. How has the number of parameters changed for the convolutional layer, and is this expected? \n",
    "> 3. Compare the number of learnable parameters for the 64 unit fully connected layer between the models with and without padding. How can you account for this difference? \n",
    "> **End of exercise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**   \n",
    "> 1.     \n",
    "> 2.     \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next execute the code in the cell below to compile and fit the model. This process will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "nns.compile(optimizer=RMSprop(learning_rate=0.001, decay=0.001), \n",
    "            loss = 'categorical_crossentropy', metrics = ['accuracy', 'top_k_categorical_accuracy'])\n",
    "    \n",
    "## Now fit the model\n",
    "history_s = nns.fit(train_images, train_labels, \n",
    "                  epochs = 80, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cells below to plot the loss vs. epoch and accuracy vs. epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_s)    \n",
    "plot_accuracy(history_s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display accuracy netrics\n",
    "evaluate_model(nns, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 6-7:** Keeping in mind that small (3rd digit) differences in metrics are often simply a result of random sampling do these results appear significantly difference than for the unpadded model? It may well be the case that richer feature map may not be important for this relatively simple problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Learning Rate     \n",
    "\n",
    "The two foregoing models have been trained using a fixed learning rate. In many cases a fixed learning rate is far from optimal. The problem arrises since initial learning can take large steps, but small steps are required toward convergence to ensure a stabble termination of the algorithm. A fixed low learing rate will learn too slowing a the begining of training but converge at the end. On the other hard a high learning rate is effcive at the beginign of traning, but will lead to poor or unstable convergence.   \n",
    "\n",
    "The solution is to use a variable learning rate. The variable learning rate decreases at the training proceeds. In this way, the learning rate can be optimized to ensure the fastest posslble convergence of the learning. The variable learning rate is determined by a schedule. Common schedules include lenear decreases and exponential decreases. Keras supports a number of [learning rate schedulers](https://keras.io/api/optimizers/learning_rate_schedules/).      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 6-8:** You will now investigate using a variable learning rate for training the CNN model. The code in the cell bellow does the following:  \n",
    "> 1. An exponentially decay learning rate schedule is created using [tensorflow.keras.optimizers.schedules.ExponentialDecay](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/). \n",
    "> 2. The `nnv` model is instantiated, using the function you created earlier, and compiled. A learning rate schedule can be incorporated into most Keras optimizers. In this case, the [RMSprop optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/RMSprop) has an `learning_rate` argument set to the learning rate just created, and with `decay=0.001`.    \n",
    "> 3. The model is fit using 80 epochs, or untill the stopping criteria set in the call-back function is reached. The larger number of epochs allow the training to run closer to completion.    \n",
    "> \n",
    "> Execute this code to traing the model, which will take some time to run.\n",
    "\n",
    "> **Note:** You may be wondering why a new model needs to be instantiated to run this experiement. The reason is that Keras (nd Tensorflow) model objects will retain any weight values from previous learning. This property can be an advantage if one wishes to do additional training. For example, additional training can be performed when additional data is avaialable, without starting over again. Hoeever, if one wishes to run an independent experiment, as we do here, it is best to instantiate a new model object.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the learning rate schedule \n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, \n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "## Instantiate and compile the model\n",
    "nnv = create_model()\n",
    "nnv.compile(optimizer = RMSprop(learning_rate=lr_schedule, decay=0.001),\n",
    "                loss = 'categorical_crossentropy', \n",
    "                metrics = ['accuracy', 'top_k_categorical_accuracy'])\n",
    "    \n",
    "## Now fit the model\n",
    "history_v = nnv.fit(train_images, train_labels, \n",
    "                  epochs = 80, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next execute the code in the cell below to evalue the model just trained.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_v)    \n",
    "plot_accuracy(history_v) \n",
    "\n",
    "## Display accuracy netrics\n",
    "evaluate_model(nnv, test_images, test_labels)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the results and compare them to those created for Exercise 6-3.      \n",
    "> 1. Comare the number of epochs that occur before reaching the early stopping criteria. What does this differnece tell you about the effect of using a decaying learning rate?    \n",
    "> 2. How can you explain the improved model performance given the decaying learning rate?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.    \n",
    "> 2.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In these exercises you have applied convolutional neural networks (CNNs) to an image calassification problem. Some key points of this lesson are:     \n",
    "1. CNNs learn powerful features from images.\n",
    "2. Convolutional feature maps have multiple channels with a spatial dimensions.      \n",
    "3. Multi-layer CNNs create highly absract features at deeper layers. Deeper architectures can learn more powerful features.\n",
    "4. The convolutional feature map is used as an input to a classifier.\n",
    "5. Evaluation of multi-class classifiers can be evaluated by both top-1 and top-k accuacy metrics.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright  2018, 2019, 2020, 2021, 2022, 2023, 2024, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
