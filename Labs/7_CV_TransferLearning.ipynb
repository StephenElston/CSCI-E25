{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95325261",
   "metadata": {},
   "source": [
    "# CSCI E-25      \n",
    "## Transfer Learning and Data Augmentation  \n",
    "### Steve Elston\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Transfer learning** and **data augmentation** are important approaches to paractical use of deep neural network models for computer vision. You will work with examples of each of these methods.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21297feb",
   "metadata": {},
   "source": [
    "### Transfer learning   \n",
    "Transfer learning employes deep neural network models which have been previously trained on large datasets. This pretraining can require enourmous computing resources, as well as massive datasets. These resources are often not available in practice. The dataset available for a particular prblem may be too small, or the computing resources may not be available for the project.  \n",
    "\n",
    "Most deep learning platforms have a module of pre-trained models. You can find an [extensive list of optipns for Keras](https://keras.io/api/applications/). \n",
    "\n",
    "Instead, we can use a model with weights trained previously on other datasets. We stay that we **transfer** the learning from one task **learned** with some traning data to another task with different image characteristics. This process is known as **transfer learning**. \n",
    "\n",
    "In most cases of transfer learning there are two major components of the model, a **backbone network** and a **head**. The pre-trained **backbone** network produces a feature map. A head, placed on the backbone, performs the task-specific learning. Examples of task-specific learning include:   \n",
    "- **Object classification:** Our goal for the exercises in this lesson.  \n",
    "- **Object detection:** Fine the objects in an image.  \n",
    "- **Semantic segmentation:** Detect and label the types of 'things' in an image.  \n",
    "- **Object tracking:** Track how the bjects in an series of images (a video) are moving.   \n",
    "\n",
    "There are several approaches to task-specific traning with transfer learning:     \n",
    "\n",
    "**Frozen backbone network:** The weights of the backbone network are frozen and the resulting feature map is used directly. The weights of the task-specific head are learned using case-specific data. This approach minimizes the amound of traning data requried, since only the weights of the head need be trained. This approach leads to methods known as **few shot training** for a specific case of a task. Accuracy may be sacrificed since the feature map is not optimized for the task.         \n",
    "\n",
    "**Fine tune traning of the backbone network:** Weights of the task specific head are learned using the case-speific data. At the same time, the weights of the backbone network are **fine-tuned** for the task. In many cases, only a few epochs are required for fine tuning. This second approaceh often provides better performance, since the feature map produced by the backbone is more likely to have features specific for the task. However, fine tuning of the backbone network may fail if there is insufficient data to effectively train the additional weights.     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a5007",
   "metadata": {},
   "source": [
    "### Data augmentation     \n",
    "\n",
    "Even with good pre-training, sufficient task-specific data may not be available even to learn the required weights using transfer learning. In such cases one can apply **data augmentation**.  The process of data augmenation creats new traning samples from existing training images. The new image samples are created by **randomly** appling one or more **transformations** to the original image. The label of the transformed image is the same as the original image. Yet, given the randomly chosen transformations applied, the new image will have different characteristics. Further, since the transformations are random in nature, several new samples can be created from the same original image. Thus, augmented data will help tranin models to have better generalization. \n",
    "\n",
    "Deep learning platforms include packages for standard random data augmentation. For example, in the Keras documentation you can find [examples of applying random transformations](https://www.tensorflow.org/tutorials/images/data_augmentation) to augment image data. \n",
    "\n",
    "Examples of random transformations which can be used to augment image traning data include:   \n",
    "1. Random rotation.     \n",
    "2. Random translation along either or both axes.  \n",
    "3. Random cropping of the image followed by resizing to the original size. \n",
    "4. Flipping the transformed image to create a mirror image.   \n",
    "5. Randomly adjusting the contrast of the image.   \n",
    "6. Adding Gaussian or other noise to the image.  \n",
    "7. Independently applying random brightness adjsutments to the histograms of the color channels. \n",
    "8. Randomly down sampling followed by resizing.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704553d2",
   "metadata": {},
   "source": [
    "## Example Notebook   \n",
    "\n",
    "For these exercises you will use the [Image classification via fine-tuning with Efficientnet](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/) Keras example notebook. This notebook uses the pre-trained **EfficientnetB0** model, the smallest model in the EfficientNet family introduced by [Tan and Le, 2019](https://arxiv.org/abs/1905.11946). This version of the model is faster to train and faster at inference, but at a sacrifice in accuracy.   \n",
    "\n",
    "To run the notebook you will need a [Google Colabratory account](https://colab.research.google.com/) if you do not already have one. Log into your google account. Start on the [Image classification via fine-tuning with Efficientnet](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/) page. Then, click the `View in Colab` tab to start the notebook in Colab (but do not execute yet!). Alternatively, you may want to run this notebook locally, if you have the resources.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b146af",
   "metadata": {},
   "source": [
    "### The Dataset   \n",
    "\n",
    "The [Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html) dataset contains over 20,000 images of 120 dog breeds. The goal for a classifier is to learn to classify dog breeds correctly.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b5f90-38aa-439e-85e5-61f30c3591d7",
   "metadata": {},
   "source": [
    "### Modifications to the Notebook   \n",
    "\n",
    "Before you attempt to run this notebook in Colab you must make several changes. These changes will help with model training and evaluation.     \n",
    "\n",
    "1. **Verify the version of TensorFlow.** The note on the Jupyter notebook indicates it should be run on TensorFlow 3. Yet, this does not seems to be necessary. To verify you are using a suitable version of TensorFlow execute the code shown below just after executing the imports. The version of TensorFlow version 2.15 or higher.        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb01dff1",
   "metadata": {},
   "source": [
    "print('TensorFlow version = ' + str(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff811dce",
   "metadata": {},
   "source": [
    "2. **Improving traing plots.** To obtain a better understanding of the learning of the model substitute the code shown below for the code in the cell following the `Training the Model from Scratch` code cell. This code will display both the loss and accuracy for training and evaluation as the model training evolves.         "
   ]
  },
  {
   "cell_type": "raw",
   "id": "59e47c5e",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist(hist):\n",
    "    _,ax = plt.subplots(1,2, figsize = (12,6))\n",
    "    ax[0].plot(hist.history[\"accuracy\"])\n",
    "    ax[0].plot(hist.history[\"val_accuracy\"])\n",
    "    ax[0].set_title(\"model accuracy\")\n",
    "    ax[0].set_ylabel(\"accuracy\")\n",
    "    ax[0].set_xlabel(\"epoch\")\n",
    "    ax[0].legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    ax[1].plot(hist.history[\"loss\"])\n",
    "    ax[1].plot(hist.history[\"val_loss\"])\n",
    "    ax[1].set_title(\"model loss\")\n",
    "    ax[1].set_ylabel(\"loss\")\n",
    "    ax[1].set_xlabel(\"epoch\")\n",
    "    ax[1].legend([\"train\", \"validation\"], loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7c67e-196b-4c9f-b81f-0c09884c5139",
   "metadata": {},
   "source": [
    "3. **Split cell of of training model from scratch.** You may find it convenient to split the code cell in the Training Model from Scratch section so that the following lines are in a seperate executable cell. By doing this split, you can restart the notebook and instantiate the model without having to wait for the long training time.       "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a25ba6c-c541-4cd7-918e-14c7ff63548d",
   "metadata": {},
   "source": [
    "epochs = 30  # @param {type: \"slider\", min:10, max:100}\n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7766df-807a-4beb-a917-b6b5b46561d9",
   "metadata": {},
   "source": [
    "The execution time for training this model is rather long. Before executing the above code, reduce the number of epochs to 30 to reduce this training time.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec3fcd-dc6e-4da9-b64e-de6ed48d31ee",
   "metadata": {},
   "source": [
    "4. **Update to Transfer learning from pre-trained weights code.** Dispite the claim in the notebook that the training of the model using transfer learning can be trained with an agressive training rate, this does not seem to be the case. Experimentation shows that performance of the model can be iomproved by a slower learning rate and use of weight decay for regularization. The experiments and results shown in the table below were run only on the first 10 epochs to limit experimentation time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb3b5f-48fd-4a31-9792-2fd1b21ae2fd",
   "metadata": {},
   "source": [
    "| Accuracy  | Learning Rate | Weight Decay |    \n",
    "| --------  | ------------- | ------------ |     \n",
    "| .7898     | $10^{-2}$       | None       |      \n",
    "| .7853     | $10^{-2}$      | 0.001       |    \n",
    "| .7931     | $10^{-2}$      | 0.01        |\n",
    "| .8180     | $10^{-3}$      | 0.01        |\n",
    "| .8108     | $10^{-4}$      | 0.01        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de7698-aa14-4f91-a9d0-7ff9a576f88e",
   "metadata": {},
   "source": [
    "`To improve the model training, update the line of code defining the optimizer as follows.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "faa331e5-c132-4f03-8b0c-60e43f933b9b",
   "metadata": {},
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc465b84-f99f-4fa4-ac46-b8f86398295b",
   "metadata": {},
   "source": [
    "6. **Evaluate the trained model.** You will add several blocks of code following the training of the frozen weight model to help you evaluate the results more fully.\n",
    "\n",
    "As a first step create a code cell with the following code which finds the labels of the test image dataset.        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f261352-c517-423c-b915-2ef034273550",
   "metadata": {},
   "source": [
    "one_hot_labels = [x for _,x in  ds_test.unbatch().as_numpy_iterator()]\n",
    "test_labels = np.argmax(one_hot_labels, axis=1)\n",
    "test_labels[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfebcd90-f406-46d8-9b15-bdd7d9e14526",
   "metadata": {},
   "source": [
    "The next code block does the following:      \n",
    "- Predict the probabiliteis of the 120 classes from the test images.    \n",
    "- Assign the highest probability class as the prediction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d503d-abbb-4b9a-9f91-aa964da9cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(ds_test, batch_size=1)\n",
    "predicted = predictions.argmax(axis=1)\n",
    "predicted[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad370103-79fa-4222-9869-3be2cf960fdc",
   "metadata": {},
   "source": [
    "Add a code cell with the following code which preformes these functions:     \n",
    "- Compute and print the accuracy.     \n",
    "- Compute the class-specific precision and recall measure.        \n",
    "- Compute and print the weighted mean precision and recall measures.\n",
    "\n",
    "> **Note:** For this code the metrics package from Scikit Learn is used since it is well documented and stable. Keras has only full metric support for binary classifiers. TensorFlow has some support, but the availability and detials are highly version dependent.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca001674-a927-4b26-b1ac-28df2dad1d3b",
   "metadata": {},
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print('Overall accuracy = ' + str(round(metrics.accuracy_score(test_labels, predicted), 4)))\n",
    "\n",
    "unique_labels, label_counts = np.unique(test_labels, return_counts=True)\n",
    "class_precision = metrics.precision_score(test_labels, predicted, labels=unique_labels, average=None)\n",
    "class_recall = metrics.recall_score(test_labels, predicted, labels=unique_labels, average=None)\n",
    "\n",
    "sum_label_counts = np.sum(label_counts)\n",
    "weighted_average = lambda x: round(np.sum(np.divide(x * label_counts, sum_label_counts)), 4)\n",
    "print('Average precision = ' + str(weighted_average(class_precision)))\n",
    "print('Average recall = ' + str(weighted_average(class_recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542f6ce-7369-4a28-b7c4-9f24344e061c",
   "metadata": {},
   "source": [
    "Finally, you will add a cell with the following code to create and display the confusion matrix. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b46d05bf-7081-4bc5-b8bb-8ea932e58608",
   "metadata": {},
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_labels, predicted)\n",
    "\n",
    "plt.figure(figsize = (12,9))\n",
    "p = plt.imshow(np.log(np.divide(confusion_matrix + 1.0, np.sum(confusion_matrix, axis=1))))\n",
    "cb = plt.colorbar(p)\n",
    "_=cb.set_label('Log count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9ad36-b1ef-429d-aeb1-9fbf2279f7a0",
   "metadata": {},
   "source": [
    "6. **Update for the unfrozen weight model training.** It will make only minimal difference in the model training but a limited improvement can be had by updating the line of code used to define the optimizer for this model.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c8499-219e-4bfe-b496-df3291ccf296",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68757a34",
   "metadata": {},
   "source": [
    "### Steps to submit notebook   \n",
    "\n",
    "Once you have successfully exectued your notebook in colab you must use `Save a copy in Drive` to save any of your work in your [GoogleDrive account](https://support.google.com/drive/answer/2424384?hl=en&co=GENIE.Platform%3DDesktop). You can then download the notebook from your GoogleDrive account and upload it into Canvas for submission.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962f8e7",
   "metadata": {},
   "source": [
    "## Understabing the Code   \n",
    "\n",
    "Before proceeding, it is worthwhile to understand some key aspects of the code.\n",
    "\n",
    "### Importing the pre-trained model  \n",
    "\n",
    "Under the heading, *Keras implementation of EfficientNet*, code to import and configure [EfficientNetB0](https://keras.io/api/applications/efficientnet/) is described. Examine the code:  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a973446f",
   "metadata": {},
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "model = EfficientNetB0(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9ad4d",
   "metadata": {},
   "source": [
    "In transfer learning the feature map generating convolutional layers are pre-trained. These pre-trained layers constitued a **convolutional backbone**. A new head is added on top of the backbone and trained for the specific task. The argument `include_top=False` creates a model object without the head.     \n",
    "\n",
    "The `weights='imagenet'` argument creates a model object with weights learned by supervised learning with the large [ImageNet dataset](https://www.image-net.org/). These weights for the convolutional backbone layers for the creation of a rich feature maps without additional traning.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f482f3",
   "metadata": {},
   "source": [
    "### Load the dataset and resize the images. \n",
    "\n",
    "The next several code cells import the Stanford Dogs dataset, and resizes the images to $253 \\times 253$ pixels, as required by EfficientNetB0. The Stanford Dogs dataset comes with independently sampled train and test sets.\n",
    "\n",
    "The code in the cell of the *Visualizing Data* subsection displays a sample of the dog images. Notice about the following about these images:   \n",
    "1. The are different crops and angles for the dog in each image.    \n",
    "2. The dog in the images have a variety of scales.  \n",
    "3. Some images have other objects.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77efbb5a",
   "metadata": {},
   "source": [
    "### Data augmentation    \n",
    "\n",
    "Very often only limited task-specific training data is available. In such cases, **augmentting** the training data can be an effictive preprocessing step.  The code in the *Data augmentation* section performs a series of random data augmenation steps. The code in the second cell of this section displays a sample of agmentated verisons of one image.Examine these results.   \n",
    "\n",
    "> **Exercise 7-1:** Answer these questions:   \n",
    "> 1. Which augmentation methods are applied to the images?    \n",
    "> 3. Describe some of the ways you can observed the multiple-transformation agumentation manifest in the displayed samples?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fe5be",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.      \n",
    "> 2.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a20841",
   "metadata": {},
   "source": [
    "### Training the model     \n",
    "\n",
    "The remainder of the notebook contains code for traning the model by three different approaches. Examine the code and notice the three different traning approaches:    \n",
    "1. **Training from scratch:** In this case no pre-trained weights are used. The model weights are learned from scratch using the training data.     \n",
    "2. **Trainng the head with frozen backbone weight:** In this case, the weights of the classification head of the model are trained, with the backbone weights frozen. In other words, the feature map is created from the pretrained weights. The feature map is used by the classifier head. The classifier uses weights learned from the training data. Notice in the code for the *Transfer learning from pre-trained weights* section of the notebook* the model object is created with the  following line of code, excluding the head.            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "173ffe61",
   "metadata": {},
   "source": [
    "model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04547bd0",
   "metadata": {},
   "source": [
    "3. **Fine tuning weights:** In some caes, it is possible to improve model performance by **fine tuning** the weights of the backbone. The fine tuning can result in a feature map better suited to the feature specific to images used for the particular case. Once the model in the notebook is trained with the fronzen backbone weights, the weights of the trainable layers are made `trainable`, or unfrozen.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "37cb8169",
   "metadata": {},
   "source": [
    "for layer in model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a3cac",
   "metadata": {},
   "source": [
    "## Executing the Code and Examining the Results      \n",
    "\n",
    "You are now ready to execute the code in the notebook. Do so, and allow the code to run to completion.       \n",
    "\n",
    "**Warning!! Expect slow execution!** It appears that the model is intended to be run using **Tensor Processing Units (TPUs)** rather than GPUs. However, configuring the environment to work correctly with TPUs is quite challenging. Further, a dedicated Google cloud storeage account (not GoogleDrive) appears to be required. The notebook will execute with the above modifications, but slowly. Over 2 minute per epoch is required for the untrained model. Models using transfer learning require about a minute or less per epoch.    \n",
    "\n",
    "The entire notebook can be executed from the menu at the top from `Runtime -> Run all`. As noted above, execution will take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11cf30",
   "metadata": {},
   "source": [
    "> **Exercise 7-2:** Examine the results of traning from scratch and answer these quesitons:   \n",
    "> 1. Based on the training loss and error rate, is the model continuing to learn over the epochs.     \n",
    "> 2. Based on the training and test losses and error rates, does the model show generalizatation as the the training epochs progress.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df11871",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.       \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14e869",
   "metadata": {},
   "source": [
    "> **Exercise 7-3:** Next, examine the results from the pretraned backbone (fronzen weight) model trained in the *Transfer learning from pre-trained weights* section of the notebook, then answer the following questions.     \n",
    "> 1. In terms of traning and test loss and accuracy, describe the progress of the training of the head?    \n",
    "> 2. What evidence is there that the model will generalize?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969f4e0",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.       \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46566204-f15b-4656-9f7a-01bbd19600b8",
   "metadata": {},
   "source": [
    "> **Exercise 7-4:** Examine the accuracy, average precision and average recall computed from the model. What do this figures tell you about the consistncy of the classification performance of the classifier model across the different classes?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd845d30-d8d1-4dbd-b984-c5f41e03a8b5",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cdbc8-6c4d-4304-b73b-d0bef099b67f",
   "metadata": {},
   "source": [
    "> **Exercise 7-5:** Examine the heat map of the log confusion matrix. What does the small number of off-diagonal weakly bright spots on this plot tell you about the errors for this classifier model?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a68f3-f521-4b23-8b3d-4825150238f9",
   "metadata": {},
   "source": [
    "> **Answer:**      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b001da51",
   "metadata": {},
   "source": [
    "> **Exercise 7-6:** Examine the results of the fine tuning withe the unfrozen backbone weights and answer these questions:     \n",
    "> 1. Comparing the validation loss and accuracy of the last epochs of the model with frozen backbone weights and the fine tuning with the unfrozen weights. Has the fine tuning improved the overall model performance and why?          \n",
    "> 2. Is there any additional fine-tune learning after the first epoch and why?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443d793",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.      \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a1261",
   "metadata": {},
   "source": [
    "#### Copyright 2023, 2024, Stephen F Elston. All rights reserved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea4cba-1d2c-467b-a227-1864acacc6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
