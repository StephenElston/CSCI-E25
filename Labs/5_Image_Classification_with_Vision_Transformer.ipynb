{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ37Bcyvpe2H"
   },
   "source": [
    "\n",
    "# CSCI E-25   \n",
    "## Image classification with Vision Transformer\n",
    "## Steve Elston\n",
    "\n",
    "> **Attributiion:** This notebook is a modification of the original [Keras example notebook](https://keras.io/examples/vision/image_classification_with_vision_transformer/) by [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/), 2021/01/18.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVwZ-A20pe2J"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The Vision Transformer (ViT), [Dosovistskiy, et. al., 2020](https://arxiv.org/abs/2010.11929), model was as early application of a transformer architecture to images. While this model is not state of the art in terms of performance or computational efficiency one can learn a lot about how transformer architectures are applied to images. Thus, our goal is to develop an understanding of image transformer models rater than attempting to achieve state of the art performance.      \n",
    "\n",
    "Vision transformers apply the concept of **attention** to generating feature maps, or image embeddings. Convolutional neural networks (CNNs) are effective at creating feature maps for images, exhibiting advantageous **inductive bias**. However, CNNs have a generally small or local **receptive field**. In contrast, attention is a computed globally over the image. While CNNs are effective at mapping small scale features, attention provides mappings for features covering larger and specific parts of the image. For example, a CNN will effectively capture the edges and local textures. Whereas, attends to entire parts of an image important to the task being learned. For example, if the task is to identify objects, attention will capture features of entire objects such as vehicles, road surfaces, people, animals, etc.    \n",
    "\n",
    "### The ViT model\n",
    "\n",
    "The ViT model is a pure transformer architecture, with no convolutional layers. There are three major steps in the ViT algorithm.     \n",
    "1. The image is tokenized. Tokenization divides the image into small patches that can be efficiently processed. **Positional encoding** is used to encode the positions of the patches in the original image. The concepts of tokenization and positional encoding are inherited from transformer models used for natural language processing (NLP). As the processing of the tokens proceeds to deeper layers the tokens become more abstracted.  \n",
    "2. An activation tensor is computed using multiple multi-headed layers of **scaled dot product attention (SDPA)** transformers. Multiple attention heads in each layer create attention-based feature maps or embeddings. Each head learns a different layer of the feature map. In each of the heads of a layer SDPA is computed as the product of the value (V) with the softmax activation of the dot product of the key (K) and query (Q), scaled by the square root of the dimension of the key, $d_k$.\n",
    "$$SDPA = softmax \\Bigg( \\frac{Q K^T}{\\sqrt{d_k}}\\Bigg) V$$\n",
    "3. Once the multiple heads have computed the activation tensor the layers are mixed using a **multi-layer perceptron (MLP)**. The purpose of this so call **token mixing** is two fold. First, the dimensionallity of the tensor is reduced to the embedding dimension. Second the information in the tensor is linearly reweighted by the learned weights of the MLP.     \n",
    "\n",
    "### Understanding attention\n",
    "\n",
    "The arguments to foregoing equation are vectors, V, K and Q. These vectors are embeddings of tokens. The embedding is computed by matrix multiplication of a **learned weight matrix**, $W_c\\v, W_k, W_q$, with the token vectors, $T_v, T_k, T_qq$.   \n",
    "\n",
    "\\begin{align}\n",
    "    V = T_v \\cdot W_v^T \\\\\n",
    "    K = T_k \\cdot W_k^T \\\\\n",
    "    Q = T_q \\cdot W_q^T \\\\\n",
    "\\end{align}\n",
    "\n",
    "We can interpret the product of $Q$ and $K$ as the **dot product similarity** between the query and the key. $Q$ and $K$ are normalized before the dot product is computed. As a result, the dot product similarity is the same as the cosine similarity. This similarity is then scaled by the square root of the embedding dimension and a softmax activation is applied. The result is an activation tensor which is then multiplied by the value vector $V$ to give attention.  \n",
    "\n",
    "### Mulitheaded attention\n",
    "\n",
    "A given attention layer attends to a particular type of feature, such as color, shape or texture. To create feature maps we use **multiheaded attention**, where each head learns different features. Each head learns different weight matricies, $W_v, W_k, W_q$, giving different vector embeddings, $V, K, Q$. The output of the multiple heads is combined by **mixing the attention layers**. The result of the mixing operation is a vector with the length of the embedding dimension. Intuitively, we can think of the concept of using multiple attention heads as analogous to using multiple channels in convolutional layers of a CNN.\n",
    "\n",
    "### Self attention\n",
    "\n",
    "For the classification task example in this notebook we use just one vector, $V = K = Q$. Applying the learned weights, $W_v, W_k, W_q$, to this token vector and with the softmax activation gives **self attention**. Conceptually, self attention creates feature maps attending to features used by the classifier head.    \n",
    "\n",
    "### Dataset\n",
    "\n",
    "The example in the notebook implements a ViT model classification of the images, using the **[CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)**. The CIFAR-100 has low resolution $32 \\times 32$ images with 100 classes of objects with 500 training and 100 testing images for each class. While 500 training examples per category may see like a lot of data, given the small size of the images, the complexity (number of trainable parameters), and the visual similarity between some of the classes, this is a challenging classification task!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Transformer Models  \n",
    "\n",
    "As has already been mentioned we are using the now obsolete ViT model as a baseline for learning the basic principles of transformer models. Given the limitations of this model, the small training dataset, and the limited computing power used, we do not expect anything approaching state-of-the-art results. \n",
    "\n",
    "You can find an extensive library of pre-trained Keras image transformation model in the [*Keras_cv_attention_models*](https://github.com/leondgarse/keras_cv_attention_models) repository and package.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7-49hXwpe2K"
   },
   "source": [
    "### Setup to Run this Notebook\n",
    "\n",
    "This notebook was created and tested using a Google Colab Pro+ account. While not considered large by current standards, training the models in this notebook is computationally intensive.  Expect long run-times for model training in any environment. You are free to run this notebook in any environment of your choosing that has sufficient resources.\n",
    "\n",
    "To run the notebook in Colab you will need a [Google Colabratory account](https://colab.research.google.com/) if you do not already have one. Log into your google account. You can then *Upload* this notebook into your work Colab space. Make sure you configure the Runtime to use an appropriate GPU, such as A100. Large memory should not be required. Further, a dedicated Google cloud storage account (not GoogleDrive) is required. It appears that conflicts arise in the stack when using an H100 GPU with the JAX backend.  \n",
    "\n",
    "To import the packages required to run this notebook execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-SVhBQ5pe2K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "import keras\n",
    "from keras import layers, models, ops\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5GE00Erpe2K"
   },
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "The widely used CIFAR 100 dataset is provided in the `keras.datasets` package. The code in the cell below loads the train and test images and labels. Execute this code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJyTNl_Gpe2L",
    "outputId": "be3b9394-ca86-4fb8-ccdc-cbf350638a92"
   },
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "print(f\"The number of unique class: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTLgK3yhpe2L"
   },
   "source": [
    "## Configure the Hyperparameters\n",
    "\n",
    "The code in this notebook has quite a few hyperparamters. To set this hyperparameters execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BytLYBNIpe2L"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "weight_decay = 0.001\n",
    "batch_size = 256\n",
    "num_epochs = 25\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoZjkEQ4pe2L"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "As has already been mentioned, there are only 500 training classes per category in CIFAR 100. We can improve on this situation by defining **data augmentation** layers for our ViT model. Execute the code in the cell below to instantiate the data augmentation object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqcjIf4-pe2L"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        layers.RandomContrast(factor=0.1),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aqQbsA--RbL"
   },
   "source": [
    "> **Exercise 5-1:** Two of the layers defined above perform standard preprocessing of the images rather than augmentation. Answer these questions.\n",
    "> 1. What are these layers and what is there purpose?\n",
    "> 2. How are the pixel values of the images represented after applying these layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu1-aXns-RbL"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.            \n",
    "> 2.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lAIVkdKpe2M"
   },
   "source": [
    "## Implement a Multilayer Perceptron (MLP) Layer       \n",
    "\n",
    "The ViT model requires an MLP layer to mix the output of the heads in the attention layers. An MLP layer is also required for the classification head. The code in the layer below defines the layers of the MLP. Execute this code to instantiate this function.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQnk9WTqpe2M"
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRhPqTndpe2M"
   },
   "source": [
    "## Patch Creation Layer     \n",
    "\n",
    "It is now time to explore the code to tokenize the images, by creating patches. Execute the code in the cell below to create the Patches class.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f69fEhCPpe2M"
   },
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = ops.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = ops.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdflvTgPpe2M"
   },
   "source": [
    "Execute the code below to display patches for a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "id": "vsD5AgcGpe2M",
    "outputId": "b08d1f96-a117-483a-d41d-96327bccad3c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = ops.image.resize(\n",
    "    ops.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "\n",
    "# Convert image to float32 before passing to Patches\n",
    "patches = Patches(patch_size)(resized_image.astype(\"float32\"))\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiQWk7dh-RbL"
   },
   "source": [
    "> **Exercise 5-2:** Examine the code for the Patches object and the example images and resulting patches and answer these questions. *Note:* if the object in the randomly select image is not clear, execute the code in the cell above until you have a clear image.   \n",
    "> 1. Given the dimensions of the input image and of the patches, how many horizontal and vertical patches are created?\n",
    "> 2. What is the upper limit on the number of non-overlapping tokens one can create from the image? What do this small tokens represent?   \n",
    "> 3. The `keras.ops.reshape` function is applied to the patches tensor. Consider the dimensions of the resulting output tensor. Explain what the number of rows of this tensor represents? Explain what the dimension of the row vectors represents?\n",
    "> 4. Examine the image of the patches. Are do some patches contain more of the object to be classified as opposed to other items or background in the image and what does this mean for the tokens that should be attended to optimize performance of the task-specific head?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LagV39k-RbL"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.            \n",
    "> 2.             \n",
    "> 3.              \n",
    "> 4.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcnSK_z6pe2M"
   },
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "Once patches are found, the token embedding must be computed. The embedding vector is created by two steps.     \n",
    "1. Linear projection of the image tokens into the embedding space.\n",
    "2. Adding positional encoding to the embedded token vectors.   \n",
    "\n",
    "Execute the code in the cell below to create the Patches class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUVxxis9pe2N"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXGllVeb-RbM"
   },
   "source": [
    "> **Exercise 5-3:** Examine the code in the `__init__` and `call` methods above and answer these questions in one or a few sentences.   \n",
    "> 1. Explain how the code computes and applies the projection weight matrix to compute the token embedding.\n",
    "> 2. Explain how the positional embedding is created and added to the token embedding. It may help you to read the documentation for the [Keras embedding layer](https://keras.io/api/layers/core_layers/embedding/).      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v670Wwem-RbM"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.             \n",
    "> 2.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEF9uZaspe2N"
   },
   "source": [
    "## Build the ViT model\n",
    "\n",
    "We now have all the pieces required to build the complete ViT model. This model executes the following steps.     \n",
    "1. Data augmentation is applied to the batch of images.     \n",
    "2. The patches of the augmented images in the batch are embedded and positionally encoded.   \n",
    "3. Multiple transformer layers compute an attention tensor of dimension $[batch\\_size,\\ num\\_patches,\\ projection\\_dim]$. The transformer uses the [Keras multiheaded attention layer](https://keras.io/api/layers/attention_layers/multi_head_attention/). Reading the documentation for this layer will help your understanding of the hyperparameters and the arguments.\n",
    "4. The classifier head computes the most probable category.     \n",
    "\n",
    "Execute this code to instantiate the function.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K9Em0oVpe2N"
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # Apply the MLP to the x3 tensor\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    # Flatten the tensor and apply dropout regularization\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP layer for the classifier\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u17IZ8xvZ_5T"
   },
   "source": [
    "> **Exercise 5-4:** Examine the code in the cell above and a provide short answers to the following questions in one or a few sentences. Notice that the loop constructs the stack of transformer layers.        \n",
    "> 1. Explain how normalization layer applied before `MultiHeadAttention affects the interpretation of the dot product computed.\n",
    "> 2. Explain how the arguments to the `MultiHeadAttention` result in computing self-attention.\n",
    "> 3. The normalized tensor from the `.MultiHeadAttention` is passed to a MLP. What is purpose of this MLP and what are the input and output tensor dimensions?\n",
    "> 4. What does the last layer in the loop do and why is this function important?\n",
    "> 5. The scaleability limitations of transformer models are widely known. Compute how the relative computational demands of the model will change if the patch size is changed from $6 \\times 6$ pixels to:\n",
    ">    - a) $3 \\times 3$ pixels\n",
    ">    - b) $12 \\times 12$ pixels?    \n",
    ">      Perform the simple algebraic calculation to find the ratio of complexity for these cases.    \n",
    "> 6. What does the result of your scaling calculation tell you about the trade-off between spatial resolution of a tokenized image transformer model and computational complexity?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBhP1Z4JS2ig"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.           \n",
    "> 2.                \n",
    "> 3.           \n",
    "> 4.             \n",
    "> 5. \n",
    "> 6.           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTfvuGTTpe2N"
   },
   "source": [
    "## Compile, Train, and Evaluate the Model         \n",
    "\n",
    "With the model constructed it is time to compile, train and evaluate the model. Execute the code in the cell below and examine the results. Expect the training to take some time. On Colab Pro+ running an A100 GPU the training took over one hour.    \n",
    "\n",
    "As a first step, execute the model in the cell below to instantiate the model and print a summary.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4-6f6O06zF1K",
    "outputId": "f8278151-8b76-4dc7-c7df-77b48bbd8c70"
   },
   "outputs": [],
   "source": [
    "## Instantiate the model\n",
    "vit_classifier = create_vit_classifier()\n",
    "\n",
    "## Print the model summary\n",
    "vit_classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 5-5:** Examine the model summary and answer this question.    \\\n",
    "> 1. To get feel for how many trainable parameters there are in a ViT block sum the learnable parametersdifferen for the last block from the `layer_normalization`` to the second `add` following `multi-head attention`.     \n",
    "> 2. Now, compare the total trainable parameters of the ViT model and the model you created for Exercise 4-10. How do you think this difference in number of free parameters affects how difficult it is to train the ViT model?    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.           \n",
    "> 2.             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below trains the ViT model. The function returns the trained model along with the training history.    \n",
    "\n",
    "Hyperparameters for the model are set in the *Configure the Hyperparameters* section of this notebook, above. An exhaustive search has not been conducted. A limited search was conducted by fitting the model for 15 epochs only and observing the results, which are summarized here.    \n",
    "\n",
    "| Weight Decay | Learning Rate | Val Accuracy/top-5 | Comments | \n",
    "| :----: | :----: | :----: | :----: |\n",
    "| 0.0001 | 0.001 | 0.15/0.54| Erratic train and val curves, learning low after 10 epochs |\n",
    "| 0.0001 | 0.0001 | 0.14/0.40 | Less erratic train curves, learning low after 10 epochs |\n",
    "| 0.001 | 0.0001 | 0.13/0.35 | Less erratic train and val curves, learning continues for 15 epochs |\n",
    "\n",
    "Using an A100 GPU on Colab each epoch took approximately 6 minutes to run. If you find the training time excessive, you can reduce the `num_epochs` hyperparameter to 15. Execute the code to train the model.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwF2fxxFpe2N",
    "outputId": "063710d2-a0fc-4b75-dc30-7dc964faa8da"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "## Train the model\n",
    "vit_classifier, history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the training of this model, execute the code in the cell below to display the learning curve.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "9MHR9hHakf8Q",
    "outputId": "5c1b2152-36da-4fc8-c913-c3b13b160f5c"
   },
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    _,ax = plt.subplots(1,2, figsize = (12,6))\n",
    "    ax[0].plot(hist.history[\"accuracy\"], label=\"train\")\n",
    "    ax[0].plot(hist.history[\"val_accuracy\"], label=\"validation\")\n",
    "    ax[0].plot(hist.history['top-5-accuracy'], label=\"Top 5 train accuracy\")\n",
    "    ax[0].plot(hist.history['val_top-5-accuracy'], label=\"Top 5 validation accuracy\")\n",
    "    ax[0].set_title(\"model accuracy\")\n",
    "    ax[0].set_ylabel(\"accuracy\")\n",
    "    ax[0].set_xlabel(\"epoch\")\n",
    "    ax[0].legend(loc=\"upper left\")\n",
    "    ax[1].plot(hist.history[\"loss\"], label=\"train\")\n",
    "    ax[1].plot(hist.history[\"val_loss\"], label=\"validation\")\n",
    "    ax[1].set_title(\"model loss\")\n",
    "    ax[1].set_ylabel(\"loss\")\n",
    "    ax[1].set_xlabel(\"epoch\")\n",
    "    ax[1].legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MoqQv3PS2ih"
   },
   "source": [
    "As stated at the beginning of this notebook, we are not expecting state of the art results from this model. To provide some perspective, the results reported by [Dosovistskiy, et. al., 2020](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model on the massive JFT-300M dataset. The pre-trained model is then fine-tuning it on the target dataset.\n",
    "\n",
    "> **Exercise 5-6:** Examine the results of the model training and provide brief answers to these questions.\n",
    "> 1. Examine the accuracy and loss curves from the model training. What evidence is there that the model has learned over the limited number of epochs, and has the learning completed and why?\n",
    "> 2. Is there evidence that with the chosen hyperparameters the model is exhihibiting significant over-fitting and why?\n",
    "> 3. A higher capacity transformer model might give better performance, but at a cost in scaleability. Describe two options for expanding the capacity of the transformer model.\n",
    "> 4. We are training this model using a conventional supervised machine learning, employing gradient descent algorithm, AdamW, to minimize categorical cross entropy. What other more sophisticated and complex approach could be used to effectively train this model using the limited labeled data available? Why do you expect this approach to produce better results and what are the costs of applying this approach.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiYBD1TZS2ih"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.             \n",
    "> 2.               \n",
    "> 3.          \n",
    "> 4.               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below uses the validation dataset to compute summary model performance statistics and to display a confusion matrix. This code will require considerable computing time to perform the required inferences. Execute the code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 896
    },
    "id": "jg_JpEL0RdOJ",
    "outputId": "2656f6e6-e3ef-432c-e16f-b1ca409621b2"
   },
   "outputs": [],
   "source": [
    "def print_model_performance(test_labels, ds_test, test_model):\n",
    "    ## Compute predicted labels\n",
    "    predictions = test_model.predict(ds_test, batch_size=1)\n",
    "    predicted = predictions.argmax(axis=1)\n",
    "\n",
    "    k = 5\n",
    "    print('Overall accuracy = ' + str(round(metrics.accuracy_score(test_labels, predicted), 4)))\n",
    "    print('Top 5 accuracy = ' + str(round(metrics.top_k_accuracy_score(test_labels, predictions, k=k),4)))\n",
    "\n",
    "    unique_labels, label_counts = np.unique(test_labels, return_counts=True)\n",
    "    class_precision = metrics.precision_score(test_labels, predicted, labels=unique_labels, average=None)\n",
    "    class_recall = metrics.recall_score(test_labels, predicted, labels=unique_labels, average=None)\n",
    "\n",
    "    sum_label_counts = np.sum(label_counts)\n",
    "    weighted_average = lambda x: round(np.sum(np.divide(x * label_counts, sum_label_counts)), 4)\n",
    "    print('Average precision = ' + str(weighted_average(class_precision)))\n",
    "    print('Average recall = ' + str(weighted_average(class_recall)))\n",
    "    return predicted\n",
    "\n",
    "def plot_confusion_matrix(test_labels, predicted):\n",
    "    confusion_matrix = metrics.confusion_matrix(test_labels, predicted)\n",
    "\n",
    "    plt.figure(figsize = (12,9))\n",
    "    p = plt.imshow(np.log(np.divide(confusion_matrix + 1.0, np.sum(confusion_matrix, axis=1))))\n",
    "    cb = plt.colorbar(p)\n",
    "    _=cb.set_label('Log count')\n",
    "\n",
    "test_labels = y_test.flatten()\n",
    "## Compute predictions and display performance metrics\n",
    "predicted = print_model_performance(test_labels, x_test, vit_classifier)\n",
    "\n",
    "## Display the confusion matrix\n",
    "plot_confusion_matrix(test_labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALU45Fc_Emv6"
   },
   "source": [
    "As was noted previously, this is a challenging classification problem with many similar categories. Further, the training of the ViT model is incomplete.    \n",
    "\n",
    "> **Exercise 5-7:** Examine the confusion matrix noting the pattern of errors. What does this pattern of errors tell you about the difficulty of identifying certain categories of object in the images.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13xVg4tLEmv6"
   },
   "source": [
    "> **Answer:**               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPF1yK2lS2ih"
   },
   "source": [
    "## Exploring the Attention Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRAuQ9IFEmv6"
   },
   "source": [
    "It will be interesting to explore the attention activations from different layers in this network.    \n",
    "\n",
    "As a first step, execute the code in the cell below to create and display a list of the layer names for the network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EoIyDl11OD2",
    "outputId": "1f094241-3794-417e-a96d-b53fe0089408"
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "  print(pd.Series([layer.name for layer in vit_classifier.layers]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyFGmNB3Emv-"
   },
   "source": [
    "We will now view activations from three of the MSA layers. To do so, we display the results from the following normalization layer, which will limit issues with scale of the activations.  \n",
    "\n",
    "The code in the cell below extracts the normalized activation patches for the specified layers for a single image and displays them. Execute this code and examine the results.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W37CeIhGgzXB",
    "outputId": "bf1cb0c4-bb6e-43c3-c275-ccd67286c039"
   },
   "outputs": [],
   "source": [
    "def plot_patches(patches, patch_display_size=8):\n",
    "  n = int(np.sqrt(patches.shape[1]))\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.tight_layout()\n",
    "  for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    # Reshape the 64-dimensional feature vector into an 8x8 grayscale image\n",
    "    patch_img = ops.reshape(patch, (patch_display_size, patch_display_size))\n",
    "    plt.imshow(ops.convert_to_numpy(patch_img), cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "layer_names = [layer.name for layer in vit_classifier.layers] # create list of layer names\n",
    "layer_indx = [13, 22, 76] # indices of layers we want to visualize\n",
    "layer_outputs = [vit_classifier.get_layer(layer_names[i]).output for i in layer_indx]\n",
    "\n",
    "activation_model = models.Model(inputs=vit_classifier.inputs[0], outputs=layer_outputs)\n",
    "\n",
    "img_tensor = np.expand_dims(image, axis=0)\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "layer_label=[layer_names[i] for i in layer_indx]\n",
    "for i in range(len(layer_label)):\n",
    "  print(layer_label[i])\n",
    "  plot_patches(activations[i], patch_display_size=int(np.sqrt(projection_dim)))\n",
    "  plt.show()\n",
    "  print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX4hhKjkEmv_"
   },
   "source": [
    "The activations are organized by the image patches with lighter colors indicating higher activation. Notice that some patches have higher activations, indicating high attention in that patch. Further, the patterns within each patch changes from layer to layer, showing that attention is on different features of the image.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6uA_WUT1CSP"
   },
   "source": [
    "#### Portions of this document are copyright 2026, Stephen F Elston. All rights reserved.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
