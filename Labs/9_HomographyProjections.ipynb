{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dc767f",
   "metadata": {},
   "source": [
    "# CSCI E-25      \n",
    "## Homography and Projection \n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a7ef0",
   "metadata": {},
   "source": [
    "## Introduction   \n",
    "\n",
    "Transformation or projection of images is a fundamental and essential method in computer vision. Many CV applications, such as image stitching and stereo vision, require projection methods. \n",
    "\n",
    "In these exercises our primary focus is on projection using the extrinsic matrix. You will apply three types of commonly used extrinsic and one intrinsic transformation to an image:    \n",
    "1. **Euclidean**, rotation and translation. \n",
    "2. **Similarity**, rotation, translation and scale.\n",
    "3. **Affine**,  rotation, translation, scale and shear.    \n",
    "4. **Projective**, a general transformation for camera pose.   \n",
    "5. **Intrinsic camera parameters**, focal length. \n",
    "\n",
    "Before starting the exercises execute the code in the cell below to import the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage \n",
    "from skimage import data\n",
    "from skimage.filters.rank import equalize\n",
    "import skimage.filters as skfilters\n",
    "import skimage.morphology as morphology\n",
    "import skimage.transform as transform\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import exposure\n",
    "from skimage.draw import circle_perimeter, polygon_perimeter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa371554",
   "metadata": {},
   "source": [
    "## Load and Prepare the Image \n",
    "\n",
    "For these exercises you will work with a gray scale image. Execute the code in the cell below to load the image and display it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ea697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grayscale(img, h=8, ax=None):\n",
    "    if ax is None: \n",
    "        plt.figure(figsize=(h, h))\n",
    "        _=plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    else:\n",
    "         _=ax_i.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "camera_image = data.camera() \n",
    "print('Image size = ' + str(camera_image.shape))\n",
    "plot_grayscale(camera_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72df5282",
   "metadata": {},
   "source": [
    "To make the process of visualizing the transformations of image easier a dark margin will be added to the image. Execute the code in the cell below to place the image on the background and to display the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdf7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_margin = 256\n",
    "background = np.zeros((camera_image.shape[0] + 2*half_margin, camera_image.shape[1] + 2*half_margin)).astype('int')\n",
    "print('Shape of the background = ' + str(background.shape))\n",
    "\n",
    "camera_image_background = background\n",
    "#camera_image_background[2*half_margin:camera_image_background.shape[0], 0:camera_image_background.shape[1]-2*half_margin] = camera_image\n",
    "camera_image_background[half_margin:camera_image_background.shape[0] - half_margin, half_margin:camera_image_background.shape[1]-half_margin] = camera_image\n",
    "plot_grayscale(camera_image_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fcc25c",
   "metadata": {},
   "source": [
    "> **Note:** Unless otherwise specified, use this gray scale image of the photographer where specified in the following exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d985a39-792c-4951-902a-55c415e44c12",
   "metadata": {},
   "source": [
    "To demonstrate these principles execute the code in the cell below to create an image with some simple geometric shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91763c-6a19-406d-a0e0-88d6341c7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a blank image\n",
    "synthetic_image = np.zeros((1024, 1024), dtype=np.uint8)\\\n",
    "## Add a circle\n",
    "rr, cc = circle_perimeter(512, 512, 196)\n",
    "synthetic_image[rr, cc] = 1\n",
    "rr, cc = circle_perimeter(512, 512, 195)\n",
    "synthetic_image[rr, cc] = 1\n",
    "## Add a square\n",
    "rr, cc = polygon_perimeter([256, 256, 768, 768], [256, 768, 768, 256])\n",
    "synthetic_image[rr, cc] = 1\n",
    "rr, cc = polygon_perimeter([255, 255, 767, 767], [255, 767, 767, 255])\n",
    "synthetic_image[rr, cc] = 1\n",
    "## Display the results\n",
    "plot_grayscale(synthetic_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6269a23-ee65-4738-8ec6-aba02a421ea1",
   "metadata": {},
   "source": [
    "## Euclidean Transformation\n",
    "\n",
    "The Euclidean transformation involves only rotation and translation. The shape of objects is preserved by the Euclidean transformation.    \n",
    "\n",
    "The code in the cell below applies both translation and rotation to the geometric shape image. The Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af03921-5234-4d0d-b6df-77cbfbdbd38d",
   "metadata": {},
   "source": [
    "> **Exercise 9-1:** You will now apply the Euclidean transformation to the synthetic image and then the real image. This operation involves applying a rotation and translation to the image plane. Follow the numbered steps.\n",
    "> 1. To start, you will complete the function in the cell below. The function will return a $3 x 3$ Numpy array for the Euclidean transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504651f-da8d-4683-a6bf-5edf5d173d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean(theta, translation):\n",
    "    ## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af5205-cf58-40a7-9f95-f4d9e8fb7c4a",
   "metadata": {},
   "source": [
    "> 2. Execute the code in the cell below.\n",
    ">\n",
    "> **Important Note:** Notice that the second argument of the warp functuion is an *inverse map*. This convention makes sense for many CV tasks, such as image stitching and stereo vision, where one must apply the inverse transform to an image to recify it with the coordinates of a reference image. This means that some values of transform matrix will need to be modified to account for the way the transformaiton in applied by this function. Which transformation variables using an inverse affects depends on which side of the image plane the observer is viewing. For these exercises, we assome the observer is looking at the image plane from the side with the images in the scene. We maintain the right-handed coordinate system convention from this point of view.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7027a5-8e73-4b72-931c-6a4be81265a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pix = 512.0\n",
    "_, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "ax = ax.flatten()\n",
    "theta = [-math.pi/16.0, math.pi/8.0]\n",
    "for i,ax_i in enumerate(ax): \n",
    "    translation = np.array([n_pix * math.sin(theta[i]), -n_pix * math.sin(theta[i])])\n",
    "    \n",
    "    transform_matrix = euclidean(theta[i], translation)\n",
    "    print(np.round(transform_matrix, 3))\n",
    "    _=plot_grayscale(transform.warp(synthetic_image, transform_matrix), ax=ax_i)\n",
    "    ax_i.set_title('Theta = ' + str(np.round(theta[i], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b2652-a303-478e-96e6-a723306e5a73",
   "metadata": {},
   "source": [
    "> Answer the following questions:    \n",
    "> Q1. Does the roation angle appear correct for both case and why given the right-handed coordinate system?     \n",
    "> Q2. Which aspect? of the rectangle shape are preseved by this transform? In other words, why is the Euclidean transformation consider ridgid body motion.             \n",
    "> Q3. Explain why the x and y translations computed above keep shapes at the center of the images in the right-handed coordinate system. Pay attention to the sign of the translation compoenents, keeping in mind the origin of the coordiate system is at the upper left of the image.        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6cb5b-91af-4e3c-ab2d-ce97ce41f55e",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> Q1.     \n",
    "> Q2.       \n",
    "> Q3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004509e1-0a1e-4093-b714-422cd86b3d54",
   "metadata": {},
   "source": [
    "> 3. Apply the second transformation matrix to the image of the photographer and display the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b5ac0-2b5e-49f6-b2e8-481bf19de85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_grayscale(transform.warp(camera_image_background, transform_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d4fdb-1595-4601-81a2-7d7e79582ea3",
   "metadata": {},
   "source": [
    "> Q4. Is the transformation of the photographer image consistent with the transformation of the geometric shape image and is this expected?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab650ee-4640-4dbb-9af5-1b13a13bac0a",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40cc74",
   "metadata": {},
   "source": [
    "> 4. Finally, you can check your transformation matrix by using the [skimage.transform.EuclideanTransform](https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.EuclideanTransform) function, using the rotation angle and translation vector for the second example. Compute and display the transformation matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9e07c",
   "metadata": {},
   "source": [
    "> Q5. Compare the transformation matrix comptued with the function you created with the one created with the Scikit image functuion. Are they identical as is this expected?    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88dab0",
   "metadata": {},
   "source": [
    "> **Answer:**      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30f66b-036d-42b6-9f82-4755461645e0",
   "metadata": {},
   "source": [
    "## Similarity Transform\n",
    "\n",
    "You will now extend the generality of the transformation by adding a change of scale. A similarity transformation can perform rotation, translation as well as scale. Like the Euclidean transform, the similarity transform parallel lines and angles.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9f4e2-92a3-43b3-9ce1-e512fca93c61",
   "metadata": {},
   "source": [
    "> **Exercise 9-2:**  You will now do the following to explore the properties of the similarity transform: \n",
    "> 1. Complete the `similarity`function in the cell below to compute and return the following.      \n",
    ">    a. Use the `euclidean` function you created in Exercise 1 to compute the $3 x 3$ Euclidiean transformation matrix.       \n",
    ">    b. Create a $3 x 3$ scale transform as a Numpy array with the **inverse** x axis and y axis scale factors as the first two diagonal elements, 1.0 as the final diagonal element, and 0s everywhere else. The inverse of the scale is required since the [skimage.transform.warp](https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.warp) function expects the inverse.\n",
    ">    c. Use [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) function to multiply the Euclidean matrix (second argument) by the scale matrix (first argument). The order of the multiplication will mater.\n",
    "> 2. Execute the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd921202-c722-4d00-a65a-50dc939c7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "def similarity(theta, translation, scale):\n",
    "    ## Complete the function  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "ax = ax.flatten()\n",
    "theta = math.pi/8.0\n",
    "scale = np.array([[1.0,0.5],\n",
    "                 [0.5, 1.0]]) \n",
    "for i,ax_i in enumerate(ax): \n",
    "    transform_matrix = similarity(theta, [0, -256], scale[i,:])\n",
    "    print(np.round(transform_matrix,3))\n",
    "    _=plot_grayscale(transform.warp(synthetic_image, transform_matrix), ax=ax_i)\n",
    "    ax_i.set_title('Scale = ' + str(np.round(scale[i,:], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1a144-b24b-44ef-a065-964d5e9ea075",
   "metadata": {},
   "source": [
    "> Now, answer the following questions:     \n",
    "> Q1: Notice the x and y axis scaling of the geometric shapes. Are the images as expected given the two scalings and why?    \n",
    "> Q2: What properties of the rectangle are preseved by these transformations?      \n",
    "> Q3: Noice that y translation has changed for the first case. How can you expalain this change, keeping in mind that we are computing the inverse transform?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb8a9f-3bcc-4f07-9264-ea212649b038",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> Q1:      \n",
    "> Q2:      \n",
    "> Q3:     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fc483-3524-4b71-90b5-01f05f20365d",
   "metadata": {},
   "source": [
    "> 4. Apply the second transformation matrix to the image of the photographer and display the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69baa9d-baa5-4bb2-b2f8-0c6aa4be373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_grayscale(transform.warp(camera_image_background, transform_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f761509-508b-464c-b615-3a263e5d0e8b",
   "metadata": {},
   "source": [
    "> Q4. Is the transformation of the photograper image consistent with the geometric image and is this expected?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511429a8-28ab-48b4-858e-270025aee98b",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a08c85",
   "metadata": {},
   "source": [
    "> 4. You can check your transformation matrix by using the [skimage.transform.SimilarityTransform](https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.SimilarityTransform) function, using the rotation angle, translation vector and scale (using the inverse). Compute and display the transformation matrix, in homogeneous coordinates, using the arguments specified for the second transformation.  Given the inverse convention of the `warp` function you will need to use the inverse scaling which you can compute with [numpy.divide](https://numpy.org/doc/stable/reference/generated/numpy.divide.html).     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1ae2e",
   "metadata": {},
   "source": [
    ">  Q5. Is the homogeneous transformation matrix you computed identical to the one computed with your similarity transform function, with the permutation of some off-diagonal values becasuse of the use of the inverse convention?      \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf86ce",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38330f9-2fe1-4bb1-a934-506406845d09",
   "metadata": {},
   "source": [
    "> **Exercise 9-3:** One can use negative scaling with a similarity transform to create flipped images about an axis, e.g. mirror images. This appraoch can be quite useful in creating augmented datasets, and can be combined with rotaion, scaling, etc. to create additonal training examples. You will now complete the code in the cell below to display three cases:\n",
    "> 1. Flip around the vertical axis with no rotation and maintaining scale.\n",
    "> 2. Flip around the horizontal axis with no rotation and maintaining scale.\n",
    "> 3. Flip around both axes with no rotation and maintaining scale.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba22f2e-c289-454c-8e78-40e428977b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = np.array([[-1024, 0],[0, -1024],[-1024, -1024]])\n",
    "## Complete the code required to create mirror image \n",
    "\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(2,2, figsize=(10, 10))\n",
    "ax = ax.flatten()\n",
    "ax[-1].set_visible(False)\n",
    "for i, ax_i in enumerate(ax[:-1]): \n",
    "    transform_matrix = similarity(0.0, translate[i,:], scale[i,:])\n",
    "    print(np.round(transform_matrix,3))\n",
    "    _=plot_grayscale(transform.warp(camera_image_background, transform_matrix), ax=ax_i)\n",
    "    ax[i].set_title('Scale = ' + str(np.round(scale[i,:], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c241a2-3cf1-404e-9fe6-aff611135ffa",
   "metadata": {},
   "source": [
    "> **End of Exercise** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2eebfd",
   "metadata": {},
   "source": [
    "## Affine Transform\n",
    "\n",
    "Continuing to generalize the transformation you will now add a shear factor to the transformation. An affine transformation can perform rotation, translation, scaling and shear. The affine transformation preserves parallel lines.   \n",
    "\n",
    "> **Exercise 9-4:**  You will now do the following to explore the properties of the affine transform following these steps\n",
    "> 1. Complete the `affine`function in the cell below to compute and return the following.      \n",
    ">    a. Use the `similarity` function you created in Exercise 2 to compute the $3 x 3$ similarity transformation matrix.       \n",
    ">    b. Create a $3 x 3$ shear transform as a Numpy array with $tan(\\gamma)$ on the first two off-diagonal elements.\n",
    ">    c. Use [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) function to multiply the similarity matrix (second argument) by the shear matrix (first argument). The order of the multiplication will mater.\n",
    "> 2. Execute the code in the cell below.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be375be-8fc2-4abe-9ff1-697fb4794341",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "def affine(theta, translation, scale, shear):\n",
    "    ## Complete the function \n",
    "\n",
    "\n",
    "theta = 0.0 \n",
    "scale = [0.5,0.5]\n",
    "translation = [-400,-400]\n",
    "\n",
    "_, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "ax = ax.flatten()\n",
    "shear = np.array([[0.0, math.pi/4], [math.pi/4, 0.0]])\n",
    "for i,ax_i in enumerate(ax): \n",
    "    transform_matrix = affine(theta, translation, scale, shear[i,:])\n",
    "    print(np.round(transform_matrix,3))\n",
    "    _=plot_grayscale(transform.warp(synthetic_image, transform_matrix), ax=ax_i)\n",
    "    ax_i.set_title('Shear = ' + str(np.round(shear[i,:], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94272d-af49-4bb0-84ed-89c88a51d5c0",
   "metadata": {},
   "source": [
    "> Examine these results and anser the following questions.     \n",
    "> Q1. Examine the shear applied to rectangle. Are the shapes as expected from the shear applied and why?     \n",
    "> Q2. What properties of the rectangle have been preserved by these transformations?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b74e05-2748-4b51-96f5-6f118352d3e3",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> Q1.     \n",
    "> Q2.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e444d8-36f6-4aee-a749-650ec5dffa6a",
   "metadata": {},
   "source": [
    "> 3. Apply the second affine transform matrix to the photograph of the photographer and display the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8ccd0-39b6-4bf8-9eae-bdff5c866585",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_grayscale(transform.warp(camera_image_background, transform_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273d1b7",
   "metadata": {},
   "source": [
    "> **End of Exercise** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781a681",
   "metadata": {},
   "source": [
    "## Working with the Intrinsic Matrix\n",
    "\n",
    "Up until now, you have been working only with the **extrinsic transformation matrix**, which defines the projection from one plan onto another plane, with the same camera center. The extrinsic properties of are also known as **camera pose**. These transformations do not account for inernal camera parameters.   \n",
    "\n",
    "The **intrinsic matrix** is used to model camera specific characteristics. Here we will only deal with one camera parameter, the focal length. Focal length is typically denoted $[f_x, f_y]$, for the x and y components, which can be independent. The differences in x and y can arrise for a number of reasons, such as asymmetry of the camera sensor.     \n",
    "\n",
    "It is often easier to work with the inverse of focal length:     \n",
    "\n",
    "$$\\Bigg[\\frac{1}{f_x},\\frac{1}{f_x} \\Bigg] = [\\phi_x, \\phi_y]$$    \n",
    "\n",
    "In Cartesian coordinates, for a basic pinhole camera the object location, $[x,y,w]$, maps to the $[x,y]$ location on the image plane by the following relationships:   \n",
    "\n",
    "$$[x,y] = \\Bigg[\\frac{u}{f_x\\ w},\\frac{v}{f_x\\ w} \\Bigg]= \\Bigg[\\frac{\\phi_x\\ u}{w},\\frac{\\phi_y\\ v}{w} \\Bigg]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59c309",
   "metadata": {},
   "source": [
    "> **Exercise 9-5:** You will now apply an intrinsic matrix for two different camera focal lengths by the following steps:  \n",
    "> 1. Define a Numpy intrinsic matrix in homogeneous coordinates with focal length $[f_x, f_y]=[0.5, 0.5]$, array offset of 0, and skew correction of 0.      \n",
    "> 2. Define a Numpy extrinsic transform matrix in homogeneous coordinates using the `affine` function you created in Exercise 3 with rotation $\\theta= 0$, translation $= [-512,-512]$, scaling $=[1.0,1.0]$, and shear $[0.0,0.0]$.  \n",
    "> 3. Perform matrix multiplication between the extrinsic matrix (first argument) by the intrinsic matrix (second argument) using [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html).\n",
    "> 4. Print the resulting product of the transformation matrices. \n",
    "> 5. Apply the resulting transformation product to the image of the photographer and display the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd043f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb633dc",
   "metadata": {},
   "source": [
    "> 6. Define a Numpy intrinsic matrix in homogeneous coordinates with focal length $[f_x, f_y]=[2.0, 2.0]$, array offset of 0, and skew correction of 0.     \n",
    "> 7. Define a Numpy extrinsic transform matrix in homogeneous coordinates using the `affine` function you created in Exercise 3 with rotation $\\theta= 0$, translation $= [256,256]$, scaling $=[1.0,1.0]$, and shear $[0.0,0.0]$.  \n",
    "> 8. Perform matrix multiplication between the extrinsic matrix (first argument) by the intrinsic matrix (second argument) using [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html).\n",
    "> 9. Print the resulting product of the transformation matrices. \n",
    "> 10. Apply the resulting transformation product to the image of the photographer and display the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fad554",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d915755",
   "metadata": {},
   "source": [
    "> In one or two sentences expain the relationship you observe between focal length and field of view on the image plane and how do you think this effect relates to wide angle and telephoto camera lenses?       \n",
    "> **End of exercise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bda4a7",
   "metadata": {},
   "source": [
    "> **Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9637d",
   "metadata": {},
   "source": [
    "## Projective transform\n",
    "\n",
    "All of the transforms we have worked on so far require the optical center to be at the same location. In other words, the perspective point is unchanged. Projective transforms are the most general planar transformations. Here we will only deal with some simple examples, which are equivalent to changing the camera position or perspective point. This transformation or homography, $mathcal{H}$, can be expressed:    \n",
    "\n",
    "$$\\mathcal{H} = H_P H_A H_S H_E$$    \n",
    "Where,        \n",
    "$H_E =$ the Euclidean transform matrix.      \n",
    "$H_S =$ the similarity transform matrix.    \n",
    "$H_A =$ the affine transform matrix.    \n",
    "\n",
    "And,     \n",
    "$H_P = \\begin{bmatrix}\n",
    "   1  & 0 & 0 \\\\\n",
    "    0  & 1 & 0 \\\\\n",
    "    \\phi_{31}  & \\phi_{32} & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We will call the last row, $[\\phi_{31}, \\phi_{32}, 1]$, the perspective vector \n",
    "\n",
    "For the following exercise you will work with a picture of a piece of furniture in the interior of a house. To load and prepare this image execute the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "building = Image.open('../datafiles/Building4.JPG')\n",
    "building = np.array(building)\n",
    "print('Iinital image shape = ' + str(building.shape))\n",
    "\n",
    "building = rgb2gray(building)\n",
    "building = transform.resize(building, (300,400))\n",
    "print('Final image shape = ' + str(building.shape))\n",
    "\n",
    "plot_grayscale(building)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcab465-3f29-411f-9dae-94cac3a5edbb",
   "metadata": {},
   "source": [
    "> **Exercise 9-6:**  You will now do the following to explore the properties of the projective transform or homography following these steps\n",
    "> 1. Complete the `projective`function in the cell below to compute and return the following.      \n",
    ">    a. Use the `affine` function you created in Exercise 4 to compute the $3 x 3$ affine transformation matrix.       \n",
    ">    b. Create a $3 x 3$ projective transform Numpy array with 1s on the diagonal and zeros elsewhere for the first two rows. The third row should be the `perspective ` argument of the function, the perspective vector.\n",
    ">    c. Use [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) function to multiply the affine matrix (second argument) by the perspective matrix (first argument). The order of the multiplication will mater.\n",
    "> 2. Execute the code in the cell below.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345cec2a-96a6-4e3d-9840-c9592f0ff369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projective(theta, translation, scale, shear, perspective):\n",
    "    ## Complete the code in this function \n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "shear = [0.0, 0.0]\n",
    "theta = 0.0 \n",
    "scale = [.75, .75]\n",
    "translation = [0.0,0.0] \n",
    "\n",
    "_, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "ax = ax.flatten()\n",
    "perspective = np.array([[0.0005, 0, 1.0],\n",
    "                      [-0.0005, 0, 1.0],\n",
    "                      [0, 0.0005, 1.0],\n",
    "                      [0, -0.0005, 1.0]])\n",
    "for i,ax_i in enumerate(ax): \n",
    "    transform_matrix = projective(theta, translation, scale, shear, perspective[i,:])\n",
    "    _=plot_grayscale(transform.warp(synthetic_image, transform_matrix), ax=ax)\n",
    "    ax_i.set_title('Prespective = ' + str(perspective[i,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9a62d-d616-4f7b-8836-8d1e7b17274c",
   "metadata": {},
   "source": [
    "> Examine the transformed images of the geometric shapes. The original geometric image has the perspective point on a line perpendicular to the center point. Describe the perspective point for each image in terms of change of perspective point. An answer in simple relative terms with respect to the original perspective point, right, left, up, down, and distance (nearer, further) are all that is required. Hint: The keystone shape tapers away from the perspective point.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d984554-310b-4b62-aaed-cfea4a7f1802",
   "metadata": {},
   "source": [
    "> **Answers by perspective vector:**      \n",
    ">        \n",
    ">          \n",
    ">        \n",
    ">        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b57f9f-59ab-4d5e-9f5e-82d03aeb396f",
   "metadata": {},
   "source": [
    "> 3. Next execute the code in the cell below to apply the perspective transform to the building image.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0fa5f-f7da-4eb7-b999-0ea2474e5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "shear = [0.0, 0.0]\n",
    "theta = 0.0 \n",
    "scale = [.6, .6]\n",
    "translation = [0.0,0.0] \n",
    "\n",
    "_, ax = plt.subplots(2,2, figsize=(10,8))\n",
    "ax = ax.flatten()\n",
    "perspective = np.array([[0.0001, 0, 1.0],\n",
    "                        [0.001, 0, 1.0],\n",
    "                        [-0.0001, 0, 1.0],\n",
    "                      [-0.001, 0.0, 1.0]])\n",
    "for i,ax_i in enumerate(ax): \n",
    "    transform_matrix = projective(theta, translation, scale, shear, perspective[i,:])\n",
    "    _=plot_grayscale(transform.warp(building, transform_matrix), ax=ax)\n",
    "    ax_i.set_title('Prespective = ' + str(perspective[i,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515fc706",
   "metadata": {},
   "source": [
    "> Examine the transformed images of the building image. Describe the change of perspective point for each transformed image in terms of change of perspective point. An answer in simple relative terms with respect to the original perspective point, right, left, up, down, and distance (nearer, further) are all that is required.    \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec4af9-94a8-49f1-95ff-07499a73e59d",
   "metadata": {},
   "source": [
    "> **Answers by perspective vector:**      \n",
    ">                \n",
    ">         \n",
    ">               \n",
    ">        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3f55a",
   "metadata": {},
   "source": [
    "####  Copyright 2022, 2023, 2024 Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9e8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
